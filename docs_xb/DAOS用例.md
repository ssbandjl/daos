# 用例[¶](https://docs.daos.io/v2.0/overview/use_cases/#use-cases)

本节提供了一个非详尽的用例列表，展示了如何在真正的 HPC 集群上使用 DAOS 存储模型和堆栈。

本文档包含以下部分：

- [存储管理和工作流程集成](https://docs.daos.io/v2.0/overview/use_cases/#61)
- 工作流程执行
  - [批量同步检查点(bulk)](https://docs.daos.io/v2.0/overview/use_cases/#63)
  - [生产者/消费者](https://docs.daos.io/v2.0/overview/use_cases/#64)
  - [并发生产者](https://docs.daos.io/v2.0/overview/use_cases/#65)
- [存储节点故障和重新同步](https://docs.daos.io/v2.0/overview/use_cases/#66)



## 存储管理和工作流程集成[¶](https://docs.daos.io/v2.0/overview/use_cases/#storage-management-workflow-integration)

在本节中，我们考虑两种不同的集群配置：

- 集群 A：所有或大部分计算节点都具有本地持久内存。也就是说，每个计算节点也是一个存储节点。
- 集群 B：存储节点专用于存储并在整个结构中传播。它们不用于计算，因此不运行任何应用程序代码。

在启动时，每个存储节点都会启动实例化服务线程的 DAOS 服务器。在集群 A 中，DAOS 线程绑定到有干扰的内核，如果使用 mOS，则与 FWK 交互。在集群 B 中，DAOS 服务器可以使用存储节点的所有核心。

DAOS 服务器然后加载存储管理模块。该模块扫描节点上的本地存储并将结果报告给指定的主 DAOS 服务器，该服务器汇总有关整个集群的已用和可用存储的信息。管理模块还检索故障域层次结构（从数据库或特定服务）并将其与存储信息集成。

然后，资源管理器使用 DAOS 管理 API 来查询可用存储并为要调度的新工作流分配一定数量的存储（即永久内存）。在集群 A 中，此分配请求可能会列出应该运行工作流的计算节点，而在情况 B 中，它可能会要求在一些已分配的计算节点附近进行存储。

一旦成功分配，主服务器将通过格式化 VOS (版本控制对象存储)布局（即 fallocate(1) 一个 PMEM 文件并创建 VOS 超级块）来初始化一个覆盖已分配存储的 DAOS 池，并启动池服务，这将启动 Raft 引擎负责池成员和元数据。此时，DAOS 池已准备好移交给实际工作流。

当工作流开始时，一个 rank 连接到 DAOS 池，然后使用 local2global() 生成一个全局连接句柄，并与使用 global2local() 创建本地连接句柄的所有其他应用程序 rank 共享它。此时，可以创建新容器并通过应用程序任务集体或单独打开现有容器。



## 工作流程执行[¶](https://docs.daos.io/v2.0/overview/use_cases/#workflow-execution)

我们考虑下[图](https://docs.daos.io/v2.0/overview/use_cases/#6a)中表示的工作流程。

![../graph/Fig_007.png](https://docs.daos.io/v2.0/graph/Fig_007.png)

每个绿色框代表一个不同的容器。所有容器都存储在灰色框表示的同一个 DAOS 池中。模拟从输入容器读取数据并将原始时间步长写入另一个容器。它还定期将检查点转储到专用的 ckpt 容器。下采样作业读取原始时间步并生成采样时间步以供后处理分析，后处理将分析数据存储到另一个容器中。



### 批量同步检查点[¶](https://docs.daos.io/v2.0/overview/use_cases/#bulk-synchronous-checkpoint)

防御性 I/O 用于管理运行时间大于平台平均故障间隔时间 (MTBF) 的大型仿真。模拟定期将当前计算状态转储到专用容器，用于在发生故障时保证前进进度。本节详细说明如何在 DAOS 存储堆栈之上实现检查点。我们首先考虑依赖阻塞屏障的传统方法，然后考虑更松散耦合的执行。

**阻挡屏障**

当模拟作业开始时，一项任务打开检查点容器并获取当前的全局 HCE。然后它获得一个 epoch 持有并与对等任务共享数据（容器句柄、当前 LHE 和全局 HCE）。每个任务通过读取等于全局 HCE 的 epoch 来检查保存到检查点容器的最新计算状态，并从上次检查点的位置恢复计算。

对于检查点，每个任务执行一个屏障以与其他任务同步，将其当前计算状态写入 epoch LHE 的检查点容器，刷新所有更新并最终执行另一个屏障。一旦所有任务都完成了最后一个屏障，一个指定的任务（例如等级 0）提交 LHE，然后在成功提交时增加一个。这个过程会定期重复，直到模拟成功完成。

**非阻塞屏障**

我们现在考虑另一种执行更松耦合的检查点方法。与前一种情况一样，一个任务负责打开检查点容器、获取全局 HCE、获取 epoch 持有并与其他对等任务共享数据。但是，任务现在可以按照自己的节奏检查其计算状态，而无需相互等待。在计算了 N 个时间步之后，每个任务在 LHE+1 时期将其状态转储到检查点容器，刷新更改并在完成后调用非阻塞屏障（例如 MPI_Ibarrier()）。然后再经过 N 个时间步，新的检查点被写入 epoch LHE+2，以此类推。对于每个检查点，epoch 编号都会递增。

此外，每个任务都会定期调用 MPI_Test() 来检查屏障是否完成，这允许它们回收 MPI_Request。在屏障完成后，一个指定的任务（通常为 0 级）也提交相关的纪元数。所有 epoch 都保证按顺序提交，并且每个提交的 epoch 都是一个新的一致检查点，可以从中重新启动。失败时，已由单个任务写入但未提交的检查点状态将自动回滚。



### 生产者/消费者[¶](https://docs.daos.io/v2.0/overview/use_cases/#producerconsumer)

在[上图中](https://docs.daos.io/v2.0/overview/use_cases/#6a)，我们有两个生产者/消费者示例。下采样作业使用模拟作业生成的原始时间步，并生成由后处理作业分析的采样时间步。DAOS 堆栈为生产者/消费者工作流程提供了特定机制，甚至允许消费者将其分析结果转储到与生产者相同的容器中。

**私人集装箱**

下采样作业打开采样时间步长容器，获取当前全局 HCE，获得一个 epoch 保持，并在 epoch LHE 将新的采样数据写入该容器。发生这种情况时，后处理作业会打开存储分析数据以进行写入的容器，检查最新分析的时间步长并在此容器上获得一个 epoch 保持。然后它打开采样的时间步长容器进行读取，并检查下一个要使用的时间步长是否准备好。如果不是，它等待一个新的全局 HCE 被提交（通过事件队列上的异步事件完成通知）并再次检查。当请求的时间步可用时，下采样作业会处理此新时间步的输入数据，将结果转储到其自己的容器中，并在其元数据中更新最新分析的时间步。

另一种方法是生产者作业为感兴趣的时期创建显式快照，并让分析作业等待和处理快照。这避免了处理每个提交的时期。

**共享容器**

我们现在假设存储采样时间步长的容器和存储分析数据的容器是一个容器。换句话说，下采样作业消耗输入数据并将输出数据写入同一个容器。

下采样作业打开共享容器，获得一个保持并将新的采样时间步转储到容器中。和以前一样，后处理作业也会打开容器，获取最新分析的时间步长，但在新的全局 HCE 准备好之前不会获得 epoch hold。一旦通知后处理作业有一个新的全局 HCE，它就可以分析新的采样时间步长，获得一个保持并将其分析的数据写入同一个容器。完成此操作后，后处理作业刷新其更新，提交持有的 epoch 并释放持有的 epoch。此时，它可以再次等待下采样作业生成新的全局 HCE。



### 并发生产者[¶](https://docs.daos.io/v2.0/overview/use_cases/#concurrent-producers)

在上一节中，我们考虑了生产者和消费者作业同时读取和写入同一个容器，但在不相交的对象中。我们现在考虑一个由并发生产者作业组成的工作流，这些作业以冲突和不协调的方式修改同一个容器。这实际上意味着两个生产者可以更新相同 KV 对象或文档存储的相同键或相同字节数组的重叠范围。该模型需要实现并发控制机制（不是 DAOS 的一部分）来协调冲突的访问。本节介绍了这种基于锁定的机制的示例，但也可以考虑替代方法。

工作流由两个应用程序组成，它们使用分布式锁管理器来序列化对 DAOS 对象的竞争访问。每个应用程序单独打开同一个容器，并在它想要修改容器中的某些对象时获取一个 epoch 保持。在修改对象之前，应用程序应该获取对象上的写锁。这个锁带有一个锁值块（LVB），存储了这个对象最后一次修改和提交的最后一个纪元号。一旦获得锁，写入者必须：

- 从一个 epoch 中读取，该 epoch 等于 LVB 中指定的 epoch 和句柄 LRE 中的最大值。
- 提交新的写入，其时代高于直播中的时代和当前持有的时代。

在应用程序完成、刷新和提交所有 I/O 操作后，使用修改对象的提交 epoch 更新 LVB，最终可以释放锁。



## 存储节点故障和重新同步[¶](https://docs.daos.io/v2.0/overview/use_cases/#storage-node-failure-and-resilvering)

在本节中，我们考虑一个连接到 DAOS 池和一个突然发生故障的存储节点的工作流。DAOS 客户端和与故障服务器通信的服务器都会遇到 RPC 超时并通知 RAS 系统。失败的 RPC 会重复发送，直到 RAS 系统或池元数据服务本身决定宣布存储节点死亡并将其从池映射中逐出。池映射更新与新版本一起传播到所有存储节点，这些节点会延迟（在 RPC 回复中）通知客户端新的池映射版本可用。因此，客户端和服务器最终都会收到故障通知并进入恢复模式。

服务器节点将合作为受影响的对象恢复不同服务器上的冗余，而客户端将进入降级模式并从其他副本读取，或从纠删码重建数据。此重建过程在容器仍在被访问和修改时在线执行。一旦为所有对象恢复了冗余，池映射将再次更新以通知所有人系统已从故障中恢复并且系统可以退出降级模式。