daos术语: docs/overview/terminology.md, https://docs.daos.io/v2.5/overview/terminology/
daos开发文档: https://docs.daos.io/v2.5/dev/development/
daos开发文档_容器增删改查: https://docs.daos.io/v2.5/user/container/, 用户文档: https://docs.daos.io/v2.5/user/workflow/
公共组件: https://github.com/daos-stack/daos/tree/master/src/common
c_raft实现: https://github.com/willemt/raft, 文档: raft.h, 

依赖缓存包: http://rz2fg6ogr.hn-bkt.clouddn.com/cache_tgz


欢迎对DAOS, SPDK, RDMA等高性能技术感兴趣的朋友加入[DAOS技术交流(群)]

rocky 是受支持的平台之一。 DAOS 还支持 RHEL 的其他变体以及 SLES/OpenSuSE

https://github.com/ssbandjl/daos.git
git remote add upstream https://github.com/daos-stack/daos.git
git remote -v
git fetch upstream
git merge upstream/master

daos客户端支持s3(ds3): https://github.com/daos-stack/daos/commit/f848300590bcca29147256f84d6f766e7faa0457

git clone --recurse xxx
git submodule update --init --recursive


docker
Dockerfile.el.9
docker build --tag daos-base:rocky8.6 --build-arg DAOS_AUTH=no https://github.com/daos-stack/daos.git#master:utils/docker/vcluster/daos-base/el8
cp dockerfile to daos root
PREFIX=/opt/daos -> Install file: "build/release/gcc/src/control/daos" as "/opt/daos/bin/daos"

版本: utils/build.config


vscode filter:
*.c, *.h, readme, *sh, *.py, *.go


cart流程:
1. 日志初始化/注册协议和操作码 crt_init_opt
2. 创建上下文（fabric->domain->endpoint->cq...） crt_context_create
3. 创建请求 crt_req_create / crt_corpc_req_create RPC请求集合 传入目的端点 -> ep_rank
4. 发送请求（请求跟踪和取消跟踪） crt_req_send / dss_rpc_send
5. 查看请求进度和回调（progress和trigger） crt_progress
6. 发送回复 crt_reply_send / crt_reply_get



hg_example:
server:
main -> HG_Set_log_level -> HG_Init -> HG_Init_opt -> HG_Core_init_opt -> HG_Core_set_more_data_callback

crt_context_create -> ... ->  HG_Core_init_opt -> hg_core_init -> NA_Initialize_opt -> na_info_parse 解析网络抽象信息 -> na_class_table -> check_protocol 检查协议 -> initialize 初始化 -> na_ofi_initialize 网络抽象初始化 -> na_ofi_check_interface 检查接口(getaddrinfo/getifaddrs/inet_ntop) -> HG_QUEUE_INIT(&priv->addr_pool) 初始化地址池 -> na_ofi_domain_open 打开域 -> na_ofi_getinfo 先查看信息 -> fi_fabric/fi_domain 分配保护域数据 ibv_alloc_pd -> hg_mem_pool_create 创建内存池 -> na_ofi_addr_create 创建地址,设置长度,放入地址池 -> na_ofi_endpoint_resolve_src_addr -> fi_getname -> vrb_dgram_ep_getname ->  
na_ofi_get_uri -> fi_av_straddr -> char *ofi_straddr -> na_ofi_addr_ht_lookup 查找或插入 -> src_addr = na_ofi_addr 设置端点源地址

fi_av_insert 将地址插入到地址向量




查池 ... -> ds_pool_query_handler -> pool_space_query_bcast -> bcast_create -> ds_pool_bcast_create -> crt_corpc_req_create -> crt_corpc_info_init 集体rpc初始化 -> crp_coll = 1 -> ...
 
crt_req_send -> crt_corpc_req_hdlr rpc请求集合控制器 -> co_pre_forward 转发前执行, 比如 ds_mgmt_tgt_map_update_pre_forward -> crt_tree_get_children -> crt_req_create_internal 创建N个内部请求 for (i = 0; i < co_info->co_child_num; i++) -> corpc_add_child_rpc 添加子rpc -> crt_req_send 回调(crt_corpc_reply_hdlr) 先发子RPC -> crt_rpc_common_hdlr  -> 发送后执行回调 crt_hg_req_send_cb -> 执行回调 crp_complete_cb -> crt_corpc_reply_hdlr

发送后
接收方(目标rank): crt_rpc_handler_common -> crt_corpc_common_hdlr -> crt_bulk_create -> crt_bulk_transfer -> 完成回调 crt_corpc_chained_bulk_cb -> crt_corpc_initiate 发起集体rpc -> crt_corpc_info_init -> crt_corpc_req_hdlr



corpc_add_child_rpc -> 继承父rpc部分属性 -> 将rpc插入co_child_rpcs尾部 -> 

map_update
ds_mgmt_tgt_map_update_pre_forward -> ds_mgmt_group_update


crt_tree_get_children -> CRT_TREE_PARAMETER_CHECKING -> crt_get_filtered_grp_rank_list 获取target组 -> to_get_children_cnt -> crt_knomial_get_children_cnt -> 


crt_corpc_reply_hdlr -> crt_corpc_complete

crt_corpc_reply_hdlr -> 


crt_corpc_req_hdlr -> 

SHIFT: 移位运算

树:
branch_ratio – 分支比率，对于 CRT_TREE_FLAT 被忽略。 对于 KNOMIAL 树或 KARY 树，有效值应在 [CRT_TREE_MIN_RATIO, CRT_TREE_MAX_RATIO] 范围内，否则将被视为无效参数
grp_rank_list 是用于构建树的目标组（在应用 filter_ranks 之后），其中的排名号用于主要组





rsvc_hash


Put a replicated service reference: 放置一个复制的服务参考

停止 RDB 副本数据库。 db 中的所有 TX 必须已经结束或仅在 rdb 中阻塞。


filter_invert: 倒置过滤

docker build  . -f utils/docker/Dockerfile.centos.7 -t daos:2.0.1
docker --debug  build . -f utils/docker/Dockerfile.el.8 -t daos

curl -sSfL --retry 10 --retry-max-time 60 -o ofi_patch_001 https://raw.githubusercontent.com/daos-stack/libfabric/master/daos-9173-ofi.patch
curl -sSfL --retry 10 --retry-max-time 60 -o ofi_patch_002 https://raw.githubusercontent.com/daos-stack/libfabric/master/daos-9376-ofi.patch
/home/daos/pre/build/external/release/ofi_patch_001

/home/daos/cache/patch/ofi_patch_001

resolve_patches


➜  daos git:(heads/v2.0.1) ✗ docker --version
Docker version 20.10.12, build e91ed57

online:
cd /root/github/storage/daos/docker
docker build https://github.com/daos-stack/daos.git\#v2.0.1 -f utils/docker/Dockerfile.centos.7 -t daos

local_tree:
git clone --recurse-submodules https://github.com/daos-stack/daos.git -b v2.0.1
docker build  . -f utils/docker/Dockerfile.centos.7 -t daos


预留大页: 
echo 4096 >/sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages; cat /proc/meminfo|grep -i huge

docker stop server && docker rm server
docker run -it -d --privileged --cap-add=ALL --name server -v /root/github/storage/daos/origin/docker:/home/daos/docker -v /dev:/dev -v /dev/hugepages:/dev/hugepages -v /sys/fs/cgroup:/sys/fs/cgroup:ro daos:2.0.1

docker run -it -d --privileged --cap-add=ALL --name daos1 -v /root/project/stor/daos/main/daos:/home/daos -v /dev:/dev -v /dev/hugepages:/dev/hugepages -v /sys/fs/cgroup:/sys/fs/cgroup:ro daos:almalinux9


docker run -it -d --privileged --cap-add=ALL --name daos1 -v /root/github/storage/daos/origin/docker:/root/github/storage/daos/origin/docker -v /dev:/dev -v /dev/hugepages:/dev/hugepages -v /sys/fs/cgroup:/sys/fs/cgroup:ro daos:2.0.1

docker run -it -d --privileged --cap-add=ALL --name daos2  -p 22223:22 -p 31436:31416 -v /root/github/storage/daos/origin/docker:/home/daos/docker -v /dev:/dev -v /dev/hugepages:/dev/hugepages -v /sys/fs/cgroup:/sys/fs/cgroup:ro daos:2.0.1

docker run -it -d --privileged --cap-add=ALL --name centos  -p 22222:22 -p 5555:5555 -v /root/project/stor/ceph/xb/docker/ceph:/root/project/stor/ceph/xb/docker/ceph 

/root/project/stor/ceph/xb/docker/ceph

61:
docker run -it -d --privileged --cap-add=ALL --name daos1 -v /home/xb/project/stor/daos/origin/docker/daos:/home/xb/project/stor/daos/origin/docker/daos -v /dev:/dev -v /dev/hugepages:/dev/hugepages -v /sys/fs/cgroup:/sys/fs/cgroup:ro daos:2.0.1
docker exec -u root -it daos1 bash -c 'cd /home/daos;exec "${SHELL:-sh}"'

main:
docker run -it -d --privileged --cap-add=ALL --name daos_main1 -v /root/project/stor/daos/main/daos:/root/project/stor/daos/main/daos -v /dev:/dev -v /dev/hugepages:/dev/hugepages -v /sys/fs/cgroup:/sys/fs/cgroup:ro daos:latest
docker exec -u root -it daos_main1 bash -c 'cd /root/project/stor/daos/main/daos;exec "${SHELL:-sh}"'

main_133:
docker run -it -d --privileged --cap-add=ALL --name daos_main1 -v /home/xb/project/stor/daos/main/daos:/home/xb/project/stor/daos/main/daos -v /dev:/dev -v /dev/hugepages:/dev/hugepages -v /sys/fs/cgroup:/sys/fs/cgroup:ro daos:latest
docker exec -u root -it daos_main1 bash -c 'cd /home/xb/project/stor/daos/main/daos;exec "${SHELL:-sh}"'

daos_restart(){
  docker restart daos_main1; docker exec -u root -it daos_main1 bash -c 'cd /home/xb/project/stor/daos/main/daos; ./start.sh; exec "${SHELL:-sh};"'
}




docker exec -it -u root server /bin/bash
export FI_LOG_LEVEL=debug
daos_server start -o /home/daos/daos/utils/config/examples/daos_server_local.yml

拷贝配置文件:
echo 'export PATH=/opt/daos/bin:$PATH' >> /root/.bashrc
mkdir -p /opt/daos/etc/
mkdir -p /etc/daos/certs
rm -f /opt/daos/etc/*.yml && cp utils/config/*.yml /opt/daos/etc/ && cp -r utils/config/examples/certs /etc/daos/
source /root/.bashrc

改权限:
chmod 0700 /etc/daos/certs/agent.key
chmod 0700 /etc/daos/certs/server.key
chmod 0700 /etc/daos/certs/admin.key


dmg sto scan
dmg net scan
dmg -i storage format

dmg system erase

# -e TZ=Asia/Shanghai
docker run -it -d --privileged --cap-add=ALL --name server -v /dev:/dev -v /dev/hugepages:/dev/hugepages daos
docker exec server daos_server start -o /home/daos/daos/utils/config/examples/daos_server_local.yml
docker exec server dmg -i storage format

daos1(){
	
  docker exec -u root -it daos bash -c 'cd /root/github/storage/daos/origin/docker/daos;exec "${SHELL:-sh}"'
}

vim /home/daos/daos/utils/config/examples/daos_server_local.yml
- D_LOG_MASK=DEBUG
- DD_SUBSYS=all
- DD_MASK=all


教程:
tour.md
创建池/创建容器
dmg system query -v
dmg pool create sxb -z 4g; dmg pool list -v  #dmg pool create --scm-size=8G --nvme-size=64G --label=samirrav_pool -u samirrav@, dmg pool create --scm-size=8G --nvme-size=64G samirrav_pool -u samirrav@ -g samirrav@
dmg pool query sxb
daos cont create --oclass=RP_3GX --properties=rf:1 --type POSIX --pool sxb --label sxb
daos container create --pool sxb --type POSIX --label sxb
daos container --verbose query -p sxb -c sxb
daos container create sxb --type POSIX sxb #### 2.3, daos [OPTIONS] container create [create-OPTIONS] [pool label or UUID] [label]
daos container query sxb sxb --verbose



新版本创建容器: daos cont create sxb --type POSIX sxb; daos cont list sxb --verbose

查询容器: daos cont list sxb --verbose; daos cont query sxb sxb



systemd
$ cat ~/.config/systemd/user/samirrav_dfuse.service
[Service]
ExecStart=dfuse  --foreground -m /scratch_fs/samirrav_dfuse/  --pool samirrav_pool --cont samirrav_cont
ExecStop=fusermount3 -u /scratch_fs/samirrav_dfuse/
[Install]
WantedBy=default.target
$
$ systemctl --user daemon-reload
$ systemctl --user list-unit-files | grep samirrav


查池
[root@e65332746210 daos]# dmg pool query sxb
Pool 9b6324da-f57a-4cd7-9586-989e9259f60e, ntarget=1, disabled=0, leader=0, version=1
Pool space info:
- Target(VOS) count:1
- Storage tier 0 (SCM):
  Total size: 240 MB
  Free: 240 MB, min:240 MB, max:240 MB, mean:240 MB
- Storage tier 1 (NVMe):
  Total size: 3.8 GB
  Free: 3.7 GB, min:3.7 GB, max:3.7 GB, mean:3.7 GB
Rebuild idle, 0 objs, 0 recs
[root@e65332746210 daos]# dmg pool list --verbose
Label UUID                                 State SvcReps SCM Size SCM Used SCM Imbalance NVME Size NVME Used NVME Imbalance Disabled UpgradeNeeded? 
----- ----                                 ----- ------- -------- -------- ------------- --------- --------- -------------- -------- -------------- 
sxb   9b6324da-f57a-4cd7-9586-989e9259f60e Ready 0       240 MB   206 kB   0%            3.8 GB    42 MB     0%             0/1      None           

[root@e65332746210 daos]# daos container query sxb sxb --verbose
  Container UUID              : 0b22f857-de4d-407d-a4a3-8ee620c6fa7d                        
  Container Label             : sxb                                                         
  Container Type              : POSIX                                                       
  Pool UUID                   : 9b6324da-f57a-4cd7-9586-989e9259f60e                        
  Container redundancy factor : 0                                                           
  Number of open handles      : 1                                                           
  Latest open time            : 0x10dd92ddb5880000 (2023-05-30 02:45:59.711932416 +0000 UTC)
  Latest close/modify time    : 0x10dd92dde3b00000 (2023-05-30 02:45:59.760330752 +0000 UTC)
  Number of snapshots         : 0                                                           
  Object Class                : UNKNOWN                                                     
  Dir Object Class            : UNKNOWN                                                     
  File Object Class           : UNKNOWN                                                     
  Chunk Size                  : 1.0 MiB                                                     


获取属性:
[root@e65332746210 sxb]# daos cont get-prop sxb sxb
Properties for container sxb
Name                                             Value                               
----                                             -----                               
Highest Allocated OID (alloc_oid)                1                                   
Checksum (cksum)                                 off                                 
Checksum Chunk Size (cksum_size)                 32 KiB                              
Compression (compression)                        off                                 
Deduplication (dedup)                            off                                 
Dedupe Threshold (dedup_threshold)               4.0 KiB                             
EC Cell Size (ec_cell_sz)                        64 KiB                              
Performance domain affinity level of EC (ec_pda) 1                                   
Encryption (encryption)                          off                                 
Global Version (global_version)                  2                                   
Group (group)                                    root@                               
Label (label)                                    sxb                                 
Layout Type (layout_type)                        POSIX (1)                           
Layout Version (layout_version)                  1                                   
Max Snapshot (max_snapshot)                      0                                   
Object Version (obj_version)                     1                                   
Owner (owner)                                    root@                               
Redundancy Factor (rd_fac)                       rd_fac0                             
Redundancy Level (rd_lvl)                        node (2)                            
Performance domain affinity level of RP (rp_pda) 3                                   
Server Checksumming (srv_cksum)                  off                                 
Health (status)                                  HEALTHY                             
Access Control List (acl)                        A::OWNER@:rwdtTaAo, A:G:GROUP@:rwtT 


调试:
dlv exec /opt/daos/bin/daos -- cont create --oclass=RP_3GX --properties=rf:1 --type POSIX --pool sxb --label sxb
gdb /opt/daos/bin/daos
set args cont create --oclass=RP_3GX --properties=rf:1 --type POSIX --pool sxb --label sxb1

池
gdb /opt/daos/bin/dmg
set args pool create sxb -z 4g
b xxx 
r


查容器:
daos cont query sxb sxb


挂载/卸载
mkdir -p /tmp/sxb; dfuse --mountpoint=/tmp/sxb --pool=sxb --cont=sxb; df -h   # mount.fuse3 dfuse /scratch_fs/daos_dfuse_samir -o pool=samirrav_pool,container=samirrav_cont
echo 'dfuse /scratch_fs/root_dfuse fuse3 pool=admin_pool,container=admin_cont,auto,x-systemd.requires=daos_agent.service    0 0' >> /etc/fstab
cd /tmp/sxb

umount /tmp/sxb

查看系统调用
strace -o xxx.log -fff exec -f config_file



参考: DaosObjectType.java
oclass: 对象类型(object class), 具有明确布局的副本对象, 第一个数字是副本数，G后面的数字代表冗余组的数量
2G1 : 2个副本, 分布在冗余组1
8GX : 8 个副本，它分布在池中的所有目标上


kill engine:
ps aux|grep '/opt/daos/bin/daos_engine'|grep -v grep|awk '{print$2}'|xargs kill -9

tail -f /tmp/daos_*.log

pkill daos_agent daos_server
daos_agent
daos_server start

pkill daos_agent daos_server;daos_agent & daos_server start

dmg server set-logmasks ERR,mgmt=debug,cart=debug,hg=debug,external=debug,object=debug

dmg server set-logmasks debug

umount /mnt/sxb

mkdir /mnt/sxb
dfuse --m=/mnt/sxb --pool=sxb --cont=sxb   # main: dfuse /mnt/sxb --pool=sxb --cont=sxb
cd /mnt/sxb

for i in {0..5};do
  echo "$i, `date`"
  dd if=/dev/zero of=$i bs=1M count=100 oflag=direct
  sleep 3
done

docker
pc(client) -> ssh -> ubuntu(code) -> map_volume -> docker
cp -r /home/daos/pre/build .


DAOS Tour test


code:




FI_ORDER_SAS: send after send

struct ofi_prov		*next: 提供者是一个单向链表



scons build, -> site_scons/prereq_tools/base.py -> 
require -> comp_def.configure() -> comp_def.build -> if has_changes or self.has_missing_targets



rebuild mercury, site_scons/prereq_tools/base.py, 
def _has_changes(self)
  if self.name == mercury:...
build cache
rm -f .sconsign.dblite

find . -name "mercury_config.h"
readlink -f ./mercury.build/src/mercury_config.h

头文件搜索路径:
export C_INCLUDE_PATH=$C_INCLUDE_PATH:/home/xb/project/stor/daos/origin/docker/daos/build/external/debug/mercury.build/src/na:/home/xb/project/stor/daos/origin/docker/daos/build/external/debug/mercury.build/src:/home/xb/project/stor/daos/origin/docker/daos/build/external/debug/mercury/src:/home/xb/project/stor/daos/origin/docker/daos/build/external/debug/mercury/src/na:/home/xb/project/stor/daos/origin/docker/daos/build/external/debug/mercury/src/util:/home/xb/project/stor/daos/origin/docker/daos/build/external/debug/mercury/src/proc_extra

echo '/lib64/' >> /etc/ld.so.conf
ldconfig
ldconfig -p|grep mercury
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/lib64/:/opt/daos/prereq/debug/mercury/lib/:/opt/daos/prereq/debug/mercury/include/
cd /home/xb/project/stor/daos/origin/docker/daos/build/external/debug/mercury/mercury/build/05_bulk
make



git commit --amend
git format-patch -1

reset single file
git checkout HEAD -- fi_domain.h


性能调优: https://docs.daos.io/v2.0/admin/performance_tuning/
As an example, an engine with targets: 16 and nr_xs_helpers: 4 would use the following tag distributions:
tag 0: metadata service
tag 1: monitoring service
tag 2-17: targets 0 to 15 (16 targets total)
tag 18-21: helper service

self_test:
export D_LOG_MASK=debug
export DD_SUBSYS=all
export DD_MASK=all

self_test -u --group-name daos_server --endpoint 0:2 --message-size '(b1048578 b1048578)' --max-inflight-rpcs 16 --repetitions 100
self_test -u --group-name daos_server --endpoint 0:2 --message-size '(0 0)' --max-inflight-rpcs 16 --repetitions 1000

主流程
main(int argc, char *argv[]) -> d_log_init -> 解析和校验参数 -> run_self_test -> cleanup

run_self_test -> 定义 latencies 内存结构 -> self_test_init 初始化cart -> crt_group_rank -> crt_group_lookup -> qsort 排序 -> d_iov_set -> crt_bulk_create -> test_msg_size -> cleanup

self_test_init -> crtu_test_init -> dc_agent_init 获取socket_path -> crtu_dc_mgmt_net_cfg_setenv 设置环境变量 -> crt_init 初始化传输层(crt_init_opt) crt_self_test_init 全局锁 -> crt_context_create -> crt_group_view_create -> crtu_dc_mgmt_net_cfg_rank_add -> progress_fn 推进rpc crt_progress -> crt_group_ranks_get 查找某个组的ranks -> crt_group_psr_set 设置服务主rank -> crtu_wait_for_ranks -> crt_rank_self_set 设置自己的rank


crt_init_opt -> d_log_init -> crt_setup_log_fac -> data_init 初始化全局配置/流控 -> prov_data_init 设置属性 -> crt_hg_init HG/NA日志初始化 -> crt_grp_init -> crt_plugin_init 回调 -> crt_self_test_init -> crt_opc_map_create -> crt_internal_rpc_register -> crt_proto_register_internal(&cpf) -> crt_proto_register -> crt_opc_reg_internal -> crt_opc_reg 设置属性 回调 -> opc_info->coi_crf = crf 操作码信息_crt请求格式, 控制器

crt_opc_map_create -> crt_opc_map_L2_create

crt_grp_init -> crt_primary_grp_init -> crt_grp_priv_create -> grp_priv_init_membs -> crt_grp_lc_create


cg_credit_ep_ctx: 流控, 每个目标EP CTX的inflight RPC 的积分限制

crt_context_create:
crt_context_provider_create -> 网络抽象初始化 -> ofi建连接(走内核,成本高) ->  内存池, 地址池(priv->addr_pool), 地址hash表(domain->addr_ht) -> 


crt_context_create -> crt_context_provider_create -> crt_context_init -> crt_hg_ctx_init -> crt_hg_get_addr -> d_list_add_tail 插入链表尾部 -> crt_provider_inc_cur_ctx_num 上下文索引号自增 -> 启动传感器sensors? -> crt_provider_name_get 获取提供者名 -> d_tm_add_metric 添加指标 -> crt_swim_init 初始化swim? -> 慢网(sockets/tcp_rxm)


crt_context_init -> d_binheap_create_inplace crt_timeout_bh_ops cc_bh_timeout -> d_hash_table_create_inplace epi_table_ops cc_epi_table


crt_hg_ctx_init -> crt_hg_class_init -> HG_Context_create -> HG_Context_set_data -> crt_hg_pool_init

crt_hg_class_init -> crt_get_info_string -> HG_Init_opt -> crt_hg_get_addr -> crt_hg_reg_rpcid


HG_Init_opt -> HG_Core_init_opt -> HG_Core_set_more_data_callback

HG_Core_init_opt -> hg_core_init -> NA_Initialize_opt -> NA_Msg_get_max_tag  na_ofi_msg_get_max_tag -> hg_hash_table_new

NA_Initialize_opt -> na_info_parse 解析和填充主机信息 -> check_protocol 先检查协议 na_ofi_check_protocol -> initialize na_ofi_initialize

na_ofi_initialize -> na_ofi_prov_name_to_type -> na_ofi_check_interface -> HG_QUEUE_INIT(&priv->addr_pool) 初始化地址池 -> na_ofi_domain_open -> hg_mem_pool_create -> 


HG_Context_create -> 


crt_group_view_create 创建本地组视图(仅客户端) -> crt_grp_priv_create 初始化链表 -> grp_priv_init_membs -> crt_grp_lc_create uri和地址查找缓存lookup_cache -> d_list_add_tail 将gp_link插入全局链表

crtu_dc_mgmt_net_cfg_rank_add 管理网络配置/添加rank -> dc_get_attach_info 获取附着信息(包含所有rank的uri) -> crt_group_primary_rank_add 将uri插入crt上下文的查找缓存表中, 针对0号tag, 将rank加入组成员列表中

crt_progress -> crt_hg_progress poll cq -> crt_context_timeout_check 超时检查 -> crt_exec_progress_cb


crtu_wait_for_ranks -> sem_init -> crt_req_create -> crt_req_set_timeout -> crt_req_send -> crtu_sync_timedwait
crt_req_create -> crt_req_create_internal -> crt_rpc_priv_alloc 分配rpc请求私有数据 -> crt_rpc_priv_init 设置初始状态:rpc_priv->crp_state = RPC_STATE_INITED -> crt_opc_lookup 查找操作码 


RPC_STATE_INITED -> 

查找目标地址:
crt_req_send -> crt_req_send_internal -> crt_req_ep_lc_lookup -> crp_hg_addr (na_addr) -> 地址作为 HG_Create 的参数(目的地址) -> crt_req_send_immediately -> crt_hg_req_create RPC_STATE_REQ_SENT 创建请求,设置状态为发送 -> crt_hg_req_send -> HG_Forward -> HG_Core_forward -> forward(hg_core_handle) -> hg_core_forward_na -> NA_Msg_send_unexpected -> na_ofi_msg_send_unexpected -> na_ofi_msg_send -> fi_tsend

目标地址: crp_tgt_uri
crt_req_send_internal -> crt_req_ep_lc_lookup -> crt_req_fill_tgt_uri -> crp_tgt_uri

crt_issue_uri_lookup


crt_hg_req_create -> 控制器复用:HG_Reset -> 不复用:HG_Create -> 



crt_hg_req_send -> HG_Forward(rpc_priv->crp_hg_hdl, crt_hg_req_send_cb 由HG转发(同时设置完成回调) -> 






复用控制器
rpc_priv->crp_hdl_reuse

coi_no_reply:1 /* one-way */ 单程(无需响应,默认)


test_msg_size -> crt_req_create -> 

crt_req_create(crt_ctx, endpt, CRT_OPC_SELF_TEST_START
CRT_OPC_SELF_TEST_STATUS_REQ -> crt_bulk_transfer -> crt_hg_bulk_transfer -> HG_Bulk_transfer_id, HG_Bulk_transfer -> hg_bulk_transfer -> hg_bulk_transfer_na -> hg_bulk_na_put -> NA_Put -> put -> na_ofi.c -> na_ofi_put -> na_ofi_rma fi_writemsg -> writemsg -> vrb_msg_ep_rma_writemsg -> wr.opcode = IBV_WR_RDMA_WRITE -> vrb_send_iov -> vrb_post_send -> ibv_post_send



crt_bulk_create -> crt_hg_bulk_create -> HG_Bulk_create -> hg_bulk_create -> hg_bulk_create_na_mem_descs -> hg_bulk_register -> NA_Mem_handle_create mem_handle_create -> NA_Mem_register na_ofi_mem_register -> fi_mr_regv -> fi_mr_key


crt scons 编译:
gurt/SConscript
cart/SConscript
cart_ctl
export D_LOG_MASK=DEBUG
export DD_MASK=all
export DD_SUBSYS=all
cart_ctl enable_fi -g daos_server -u
Build cart_ctl -> cart_ctl.c -> main -> parse_args -> crtu_test_init -> dc_agent_init -> ctl_init -> d_log_fini

ctl_init -> crtu_cli_start_basic -> sem_init -> crtu_wait_for_ranks -> ctl_register_ctl -> crt_req_create -> crt_req_send ctl_cli_cb -> crtu_sem_timedwait


crtu_cli_start_basic -> crtu_dc_mgmt_net_cfg_setenv -> ctl_init -> crt_context_create -> crtu_progress_fn -> crt_group_view_create -> crtu_dc_mgmt_net_cfg_rank_add -> crt_group_size -> crt_group_ranks_get -> crt_group_psr_set

ctl_init -> crt_init_opt



crtu_test_init -> 

swim:
self_id:18446744073709551615, swim_prot_period_len:1000,swim_suspect_timeout:8000,swim_ping_timeout:900,sc_next_tick_time:94491384(10/23-15:32:10.61)
daos_server -> engine -> main(int argc, char **argv) -> parse(argc, argv) 解析参数 -> server_init -> d_tm_record_timestamp -> dss_srv_init -> dss_xstreams_init -> dss_start_xs_id -> dss_start_one_xstream -> ABT_thread_create -> dss_srv_handler -> crt_context_create -> crt_context_provider_create -> crt_swim_init -> swim_init

阻止所有除了故障之外可能的信号
engine -> server_fini

server_init -> daos_debug_init -> dss_engine_metrics_init -> drpc_init -> register_dbtree_classes -> dss_topo_init -> abt_init -> dss_module_init interface初始化 -> crt_init_opt 网络初始化-> dss_module_init_all -> vos,rdb,rsvc,security,mgmt,dtx,pool,cont,obj,rebuild 模块初始化 -> dss_srv_init 服务初始化


crt_context_provider_create -> crt_provider_inc_cur_ctx_num 创建crt上下文后,将index自增1 -> cpg_ctx_num++ -> crt_context_register_rpc_task 注册公用rpc控制器 -> crt_context_idx  ctx->cc_idx = cur_ctx_num ->

send_request -> crt_swim_send_request -> crt_ctx = crt_context_lookup(ctx_idx) -> ctx->cc_idx == ctx_idx -> crt_req_create(crt_ctx, &ep, opc, &rpc) -> crt_req_send(rpc, crt_swim_cli_cb, ctx)


crt_swim_init -> swim_init -> crt_swim_rank_add -> crt_proto_register -> crt_register_progress_cb(crt_swim_progress_cb, crt_ctx_idx, NULL) 设置回调 -> swim_progress
swim_init -> 设置上下文属性

swim_prot_period_len: ping周期(1000ms)
swim_suspect_timeout: 质疑超时(8000ms)

crt_proto_register -> crt_proto_register_common -> crt_proto_reg_L1 -> crt_proto_reg_L2 -> crt_proto_reg_L3

if (send_updates) -> swim_updates_send -> 


engine -> server_init -> crt_init_opt -> crt_grp_init -> crt_primary_grp_init -> crt_grp_priv_create -> grp_priv_init_membs -> crt_grp_lc_create


server_init -> dss_module_init_all -> dss_module_init_one -> sm_init() -> ds_mgmt_init -> ds_mgmt_system_module_init 

daos_server -> cmd.start = server.Start -> func Start 启动服务 -> registerEvents 注册事件 -> OnLeadershipGained 获得领导力 -> startJoinLoop -> func (svc *mgmtSvc) joinLoop -> func (svc *mgmtSvc) doGroupUpdate -> MethodGroupUpdate -> DRPC_METHOD_MGMT_GROUP_UPDATE -> ds_mgmt_drpc_group_update (in MS leader 主服务器) -> ds_mgmt_group_update_handler -> ds_rsvc_request_map_dist 广播集群map -> ABT_cond_broadcast -> sc_map_dist -> mgmt_svc_map_dist_cb -> map_update_bcast map更新广播 -> crt_corpc_req_create 创建rpc集合请求 MGMT_TGT_MAP_UPDATE 目标map更新 -> crt_grp_priv_get_primary_rank -> crt_corpc_info_init  -> d_rank_list_dup_sort_uniq -> dss_rpc_send -> crt_reply_get

dss_rpc_send -> crt_req_send(rpc, rpc_cb, &eventual) -> 


ds_mgmt_drpc_group_update 更新组信息 map_version -> ds_mgmt_group_update_handler -> ds_mgmt_group_update 组更新  REPLACE -> init_map_distd -> map_distd ->  d_hash_rec_find(& -> crt_group_primary_modify 编辑主要组(原子增/删/替换) -> crt_group_mod_get 获取添加和删除列表 -> grp_add_to_membs_list 添加成员 / crt_group_rank_remove_internal 移除rank -> crt_swim_rank_add -> grp_lc_uri_insert_internal_locked 插入uri查找缓存 ->



请求异步集群位图分发。 这最终会触发 ds_rsvc_class.sc_map_dist，它必须由 rsvc 类实现。

MethodSetRank -> ds_mgmt_drpc_set_rank -> crt_rank_self_set -> grp_add_to_membs_list -> crt_provider_get_ctx_list -> crt_hg_get_addr -> crt_grp_lc_uri_insert


grp_add_to_membs_list -> grp_get_free_index -> crt_swim_rank_add -> grp_regen_linear_list


广播一个条件。 ABT_cond_broadcast() 向所有在条件变量 cond 上阻塞的服务员发出信号。 调用者不需要持有与 cond 关联的互斥锁。 如果当前没有服务员在 cond 上阻塞，则此例程无效。


集体传播的树形拓扑
k-nomial tree k项树是一种在软件中实现集体通信操作的有效方法。 与 n 叉树不同，k 项树中的子节点数量随着任务深度的增加而减少（即，没有任务的子节点比根节点多）。 优点是更早开始通信的任务执行更多的工作，从而减少了集体操作的总延迟。 相比之下，在 n-ary 树中，较早开始通信的任务会较早完成，但会增加总延迟。 概念通过 KNOMIAL_PARENT、KNOMIAL_CHILDREN 和 KNOMIAL_CHILD 函数支持 k-nomial 树，如下所述。

该图是随着时间向下流动的结构。 也就是说，对于在 2 项树上表示的多播操作，任务 0 在第一个时间步向任务 1 发送消息。 然后，任务 0 发送到任务 2，而任务 1 发送到任务 3。最后一步，任务 0 发送到任务 4，任务 1 发送到任务 5，任务 2 发送到任务 6，任务 3 发送到任务 7—— 全部同时进行。 假设计算中总共有八个任务，以下表达式也成立：



swim_member_new_remote -> 

swim状态机:
SCS_TIMEDOUT -> SCS_SELECT (第一次未获取到目标) -> 获取到有效目标 -> SCS_BEGIN


swim_member_update_suspected -> 

sed -i 's/PermitRootLogin prohibit-password/PermitRootLogin yes/' /etc/ssh/sshd_config
# sed 's@session\s*required\s*pam_loginuid.so@session optional pam_loginuid.so@g' -i /etc/pam.d/sshd

CMD ["/usr/sbin/sshd", "-D"]
ssh-keygen -t rsa -f /etc/ssh/ssh_host_rsa_key
/usr/sbin/sshd


for ip in 172.17.0.3 172.17.0.4;do rsync -rucalpzv /opt/daos root@$ip:/opt/;done

run_all "sed -i s/DD_SUBSYS=all/DD_SUBSYS=swim,cart,mgmt/g /opt/daos/etc/daos_server.yml"
run_all "sed -i s/#- FI_LOG_LEVEL=debug/- FI_LOG_LEVEL=debug/g /opt/daos/etc/daos_server.yml"

run_all "sed -i /.*FI_LOG_LEVEL=.*/d /opt/daos/etc/daos_server.yml"
run_all "sed -i '/env_vars:/a \ \ - FI_LOG_LEVEL=debug' /opt/daos/etc/daos_server.yml"


Hybrid Logical Clock (HLC): hlc 混合逻辑时钟, 

ips2='172.17.0.2 172.17.0.3 172.17.0.4'
function run_all(){
	local command=$*
	if [[ $* == "" ]]; then
	    echo "$1 cmd"
	else
		for ip in $ips2; do
			echo -e  "\n\033[32m`date +'%Y/%m/%d %H:%M:%S'` $ip $*\033[0m"
                        if [[ $ip == '172.17.0.2' ]];then
                            ${command}
			else
			    ssh $ip "${command}"
                        fi
		done
	fi
}

crt_progress(dmi->dmi_ctx, dx->dx_timeout) 0ms -> crt_exec_progress_cb -> cb_func = cbs_prog[i].cpcp_func -> swim_progress

git format-patch -1 24ea9400e30d67458bb2b970b52b81d4fdfe8f59
git format-patch -1
#mac: cd /Users/xb/OneDrive/storage/daos && scp root@xb-ubuntu:/root/github/storage/daos/origin/docker/daos/0001-build-docker-image-release-v2.0.1.patch .



egrep -v 'grp_lc_uri_insert_internal_locked|prealloc_requests|grp_li_uri_set' daos_engine.0.log |less



查找时排除
build,cache,tests,


mochi rdma mercury bulk:
https://mochi.readthedocs.io/en/latest/mercury/05_bulk.html
客户端:
HG_Init -> HG_Context_create -> MERCURY_REGISTER -> HG_Addr_lookup -> lookup_callback  HG_Trigger HG_Progress -> HG_Create -> HG_Bulk_create -> HG_Forward -> save_completed 转发完成后回调 -> HG_Get_output -> assert

服务端:
build/external/debug/mercury/mochi/05_bulk/server.c
HG_Init -> ... -> MERCURY_REGISTER save -> HG_Get_input -> HG_Bulk_create -> HG_Bulk_transfer -> fwrite -> HG_Respond

dump
获取缓存uri:
cart_ctl get_uri_cache --group-name daos_server -u

查看上下文:
cart_ctl list_ctx --group-name daos_server -u --rank 1

cart_ctl get_pid --group-name daos_server -u --rank 0-2


操作码: CRT_CTL_RPCS_LIST -> cpf.cpf_name  = "ctl" -> 
crt_proto_rpc_format

opc_info->coi_rpc_cb 收到rpc请求时回调 hg_proc_info->rpc_cb = rpc_cb -> crt_handle_rpc  crt_rpc_common_hdlr -> 

crt_hg_reg -> crt_rpc_common_hdlr

调度:
req_kickoff_internal -> 




sched_run -> sched_start_cycle -> process_all -> d_hash_table_traverse(info->si_pool_hash, process_pool_cb, dx) 遍历一个哈希表，对每一项调用遍历回调函数。 一旦回调返回非零就中断 -> sri_req_cnt -> 
crt_context_register_rpc_task -> dss_rpc_hdlr rpc控制器 -> sched_req_enqueue 请求入队 -> sri_req_cnt



daos_obj_update


对象更新
obj_tgt_update -> ds_obj_remote_update -> DAOS_OBJ_RPC_TGT_UPDATE -> crt_req_send(req, shard_update_req_cb, remote_arg) 发送RPC -> 



engine -> for (;;) 一直循环 -> crt_progress -> crt_hg_progress -> HG_Progress 推进RPC -> HG_Trigger 触发回调


hg_atomic_queue_push completion_queue -> hg_thread_cond_signal

HG_Trigger -> HG_Core_trigger -> hg_core_trigger -> hg_completion_entry = hg_atomic_queue_pop_mc(context->completion_queue) 从完成队列中取出一个条目 -> hg_core_trigger_entry 根据条目内容触发 -> hg_cb(&hg_core_cb_info) 执行用户回调(最终执行crt_req_send传入的完成回调)


coi_no_reply -> CRT_HG_ONEWAY_RPCID: 单程rpc(禁用响应) 

发送请求
crt_req_send(req, shard_update_req_cb, remote_arg) 将用户回调设置为完成回调:crp_complete_cb, HG_Forward 在 crt_hg_req_send_cb 中执行完成回调:crp_complete_cb

查询:
查leader: dmg sys leader-query

打开err日志:
dmg server set-logmasks err


vos_tests: gdb /opt/daos/bin/vos_tests ->  main -> vos_tests.c -> test_ut_vos.test.sh
set args -p 
daos_debug_init
strcpy(vos_path, "/mnt/daos")
vos_self_init(vos_path, false, BIO_STANDALONE_TGT_ID)
  ABT_init(0, NULL)
  vos_standalone_tls_init(DAOS_TGT_TAG)
  vos_self_nvme_init(db_path)
    dbtree_class_register(DBTREE_CLASS_IV, BTR_FEAT_UINT_KEY | BTR_FEAT_DIRECT_KEY, &dbtree_iv_ops)
    dbtree_class_register(DBTREE_CLASS_IFV, BTR_FEAT_UINT_KEY | BTR_FEAT_DIRECT_KEY,&dbtree_ifv_ops) -> AOS-11757 common：为固定大小值添加 DBTREE_CLASS_IFV (#10457)，为了避免当值是固定大小时在 btree 记录中进行额外的、不必要的分配，请内联用户定义的记录。 我们可以将其应用于内存树和持久格式树，因为 vea_format 将在池创建的上下文中运行，并且新创建的池不需要保持向后兼容性。 vea 没有从旧池升级的路径，但这是我们可以忍受的限制。 稍微修改兼容性代码以简化兼容性标志的设置。从最近意外添加的 run_test.sh 中删除 -x
    fd = open(nvme_conf, O_RDONLY, 0600)
    rc = bio_nvme_init(nvme_conf, VOS_NVME_NUMA_NODE, VOS_NVME_MEM_SIZE, VOS_NVME_HUGEPAGE_SIZE, VOS_NVME_NR_TARGET, true) -> bio_nvme_init (nvme_conf=nvme_conf@entry=0x55555687e0f0 "/mnt/daos/daos_nvme.conf", numa_node=numa_node@entry=-1, mem_size=mem_size@entry=1024, hugepage_size=hugepage_size@entry=2, tgt_nr=tgt_nr@entry=1,bypass_health_collect=bypass_health_collect@entry=true) -> 全局nvme初始化, struct bio_nvme_data nvme_glb -> Rename EIO (Extent I/O) to BIO (Blob I/O)
      D_INIT_LIST_HEAD(&nvme_glb.bd_bdevs)
      ABT_mutex_create(&nvme_glb.bd_mutex)
      ABT_cond_create(&nvme_glb.bd_barrier)
      DAOS_DMA_CHUNK_MB	8	/* 8MB DMA chunks */ -> 每个 chunk 8MB
      bio_chk_sz = ((uint64_t)size_mb << 20) >> BIO_DMA_PAGE_SHIFT -> 8<<20>>12=2048
      d_getenv_int("DAOS_SPDK_MAX_UNMAP_CNT", &bio_spdk_max_unmap_cnt) -> 32
      d_getenv_int("DAOS_MAX_ASYNC_SZ", &bio_max_async_sz) -> 1MB
      spdk_bs_opts_init(&nvme_glb.bd_bs_opts, sizeof(nvme_glb.bd_bs_opts))
      nvme_glb.bd_bs_opts.cluster_sz = DAOS_BS_CLUSTER_SZ -> 32MB
      nvme_glb.bd_bs_opts.max_channel_ops = BIO_BS_MAX_CHANNEL_OPS -> 通道数=4096
      bio_spdk_env_init()
        spdk_log_set_print_level(SPDK_LOG_ERROR)
        spdk_env_opts_init(&opts)
      bio_nvme_configured -> 检查是否配置了指定类型的NVMe设备，当指定SMD_DEV_TYPE_MAX时，如果配置了任意类型的设备则返回true, TODO：将 opts.mem_size 设置为 nvme_glb.bd_mem_size 目前我们无法保证干净关闭（没有泄漏大页）。 设置mem_size可能会导致EAL：没有足够的内存可用错误，并且DPDK将无法初始化
      set_faulty_criteria -> DAOS-4713 bio：引入可重试的 DER_NVME_IO (#10589)，引入可重试的错误 DER_NVME_IO，一旦从 SSD 读取时获取请求遇到 NVMe I/O 错误，此 -DER_NVME_IO 将返回给客户端，然后客户端将 使用与 -DER_CSUM 错误相同的机制来重试从副本（如果有）中获取，此补丁还包括自动 SSD 故障检测的代码更改，自动故障功能现在默认禁用，一旦启用，将返回 -DER_NVME_IO 同样进行 NVMe 写入，然后客户端不会使 I/O 失败，而是不断重试，直到 SSD 被标记为故障并且受影响的目标被标记为 DOWN -> 设置故障标准
        d_getenv_bool("DAOS_NVME_AUTO_FAULTY_ENABLED", &glb_criteria.fc_enabled)
      ...
  vos_mod_init()
  vos_db_init_ex(db_path, "self_db", true, true)
  db = vos_db_get()
  rc = smd_init(db)
  bio_xsctxt_alloc(&self_mode.self_xs_ctxt, tgt_id, true)


启动容器:
daos_start_all

停止所有服务:
daos_stop_all


客户端:
文件系统操作表: dfuse_pool_ops

查看系统调用:
strace -fff test/write_file /tmp/sxb/test1


gdb调试dfuse:
gdb attach `ps aux|grep dfuse|grep -v grep|awk '{print$2}'`


gdb attach `ps aux|grep daos_engine|grep -v grep|awk '{print$2}'`


kill dfuse
kill -9 `ps aux|grep dfuse|grep -v grep|awk '{print$2}'`

gdb条件断点:
(gdb) b ds_obj_rw_handler if opc==DAOS_OBJ_RPC_FETCH







vos初始化
vos_db_init



bug:
停服务,引用计数不对
HG_Context_destroy
  hg_core_context_destroy 1 remaining


单元测试, daos_test, 
run_test.sh
  eq_test 事件队列测试

拦截库IL, 
ioil_do_writex
一个名为 libioil 的拦截库可用于 DFuse。 该库与 DFuse 结合使用，允许拦截 POSIX I/O 调用并通过 libdaos 直接从应用程序上下文发出 I/O 操作，而无需任何应用程序更改。 这为 I/O 数据提供了内核旁路，从而提高了性能
unified namespace integration: 集成统一命名空间

客户端
dfuse


git raft子项目: git submodule update && git submodule update

编译spdk:
cd build/external/debug/spdk
./configure --help
yum install ncurses-devel ncurses
./configure --prefix="/opt/daos/prereq/debug/spdk" --disable-tests --disable-unit-tests --disable-apps --without-vhost --without-crypto --without-pmdk --without-rbd --with-rdma --without-iscsi-initiator --without-isal --without-vtune --with-shared

../share/daos/control/setup_spdk.sh
/opt/daos/prereq/debug/spdk/share/spdk/scripts/setup.sh

Running commands in /home/xb/project/stor/daos/origin/docker/daos
command:cp -r cache/spdk /home/xb/project/stor/daos/origin/docker/daos/build/external/debug/spdk
Running commands in /home/xb/project/stor/daos/origin/docker/daos/build/external/debug/spdk
command:git checkout v21.07




qa:
spdk缺elftools
centos7
pip3 install pyelftools

spdk:
Install CentOS SCLo RH repository:
yum install centos-release-scl-rh
Install CUnit rpm package:
# yum install CUnit





调试手段
gdb step
1. add sleep 10s in init, src/engine/init.c:1093
2. gdb attach, break, continue
3. step


安装dlv: go install github.com/go-delve/delve/cmd/dlv@latest
export PATH=/root/go/bin:$PATH
参考: https://github.com/go-delve/delve/tree/master/Documentation/installation
dlv调试go:
dlv exec /opt/daos/bin/daos_server -- start

证书:
container: mkdir -p /etc/daos/certs/
ubuntu: docker cp /root/github/storage/daos/origin/docker/daos/utils/config/examples/certs daos:/etc/daos/certs/

分配rpc:
crt_rpc_priv_alloc
crt_rpc_inout_buff_init

代码规范: 注释
/**
 * Set the timeout value for an RPC request.
 *
 * \param[in] timeout_sec      timeout value in seconds. value of zero will be
 *                             treated as invalid parameter.
 *
 * \return                     DER_SUCCESS on success, negative value if error
 */
int
crt_req_set_timeout(crt_rpc_t *req, uint32_t timeout_sec);


openssl/kdf.h, yum install -y openssl11-devel

安装依赖
source "$scriptsdir/pkgdep/$id.sh" -> centos.sh

code_flow
dfs_mount 挂载daos文件系统
spdk -> dfs_mount(ch->pool, ch->cont, O_RDWR, &ch->dfs)
	prop = daos_prop_alloc(0)
	daos_cont_query(coh, NULL, prop, NULL)
	entry = daos_prop_entry_get(prop, DAOS_PROP_CO_LAYOUT_TYPE)
	daos_acl_principal_to_uid(entry->dpe_str, &dfs->uid)
	DAOS_PROP_CO_ROOTS superblock or root
	open_sb(coh, false, dfs->super_oid, &dfs->attr, &dfs->super_oh)
	open_dir(dfs, NULL, amode | S_IFDIR, 0, &root_dir, 1, &dfs->root)
	if (amode == O_RDWR)
		daos_cont_alloc_oids(coh, 1, &dfs->oid.lo, NULL) 超级块OID=0, 根OID=1
	
打开文件系统
dfs_open(ch->dfs, NULL, daos->disk.name, mode, fd_oflag, daos->oclass, 0, NULL, &ch->obj)
	dfs_open_stat


dmg -i storage format

build_client(only):
scons-3 BUILD_TYPE=debug TARGET_TYPE=debug client install --build-deps=yes


engine start -> engine/init.c -> main

spdk
spdk_blob_io_write

拦截库:libioil -> IOIL_SRC = ['int_posix.c', 'int_read.c', 'int_write.c'] -> src/client/dfuse/il/int_write.c -> ioil_do_pwritev -> 自动去掉前缀 dfuse_ -> __attribute__((weak, alias("dfuse_" #name)))
writev -> dfuse_writev -> pwritev_rpc -> bytes_written = ioil_do_pwritev
pwritev -> dfuse_pwritev
d_iov_set
dfs_write



为挂载点、池、容器添加对位置参数的支持, 如果要通过 fstab 挂载 dfuse，则只给出一个选项，即挂载点，因此将其放在第一位，但如果给定，也接受池和容器。 检修 dfuse_pool_connect 函数, 这允许通过标准的 mount 命令和 fstab 通过如下一行来安装dfuse：dfuse /mnt/dfuse fuse3 默认值，用户 0 0





DAOS用户态文件系统, 写流程
master -> src/client/dfuse/ops/write.c -> dfuse_cb_write(fuse_req_t req, fuse_ino_t ino, struct fuse_bufvec *bufv, off_t position, struct fuse_file_info *fi)
struct dfuse_projection_info *fs_handle = fuse_req_userdata(req)
eqt_idx = atomic_fetch_add_relaxed(&fs_handle->di_eqt_idx, 1) -> 原子递增,每次返回+1前的值, 比如: eqt_idx=7
eqt = &fs_handle->di_eqt[eqt_idx % fs_handle->di_eq_count] -> 取余打散到eq
ev = d_slab_acquire(eqt->de_write_slab) -> 分配EV, 需要提前注册: d_slab_register(&fs_handle->di_slab, &write_slab, eqt, &eqt->de_write_slab)
ev->de_complete_cb = dfuse_cb_write_complete
dfs_write(oh->doh_dfs, oh->doh_obj, &ev->de_sgl, position, &ev->de_ev) -> dfs_write(dfs_t *dfs, dfs_obj_t *obj, d_sg_list_t *sgl, daos_off_t off, daos_event_t *ev)
  daos_array_write(obj->oh, DAOS_TX_NONE, &iod, sgl, ev) -> daos_array_write(daos_handle_t oh, daos_handle_t th, daos_array_iod_t *iod, d_sg_list_t *sgl, daos_event_t *ev)
    dc_task_create(dc_array_write, NULL, ev, &task) -> 关联EV和task
      sched = daos_ev2sched(ev) -> 拿到调度器指针, 初始化调度器
    return dc_task_schedule(task, true)
sem_post(&eqt->de_sem) -> 唤醒EQ
d_slab_restock(eqt->de_write_slab) -> 重用slab



vos_blob_format_cb
  bio_write_blob_hdr
    bio_write
      bio_rw
        bio_rwv
          bio_iod_post 提交io描述
            dma_rw
              nvme_rw
                spdk_blob_io_write rw_completion
                  blob_request_submit_op
                    blob_request_submit_op_single
                      bs_batch_write_dev
                        blob_bdev->bs_dev.write
                        bdev_blob_write
                          spdk_bdev_write_blocks


rw_completion
  spdk_thread_send_msg


客户端mount, master, gdb --args /opt/daos/bin/dfuse --mountpoint=/tmp/sxb --pool=sxb --cont=sxb -f -> 默认后台启动
dfuse -m /mnt/sxb --pool sxb --cont sxb | dfuse --mountpoint=/tmp/sxb --pool=sxb --cont=sxb
dfuse_main.c -> main
  daos_debug_init(DAOS_LOG_DEFAULT)
    d_log_init_adv 高级日志初始化, 客户端日志文件
      log_file = getenv(D_LOG_FILE_ENV) export D_LOG_FILE=/tmp/daos_client.log
      debug_prio_err_load_env
      d_log_open
        freopen(mst.log_file 重新关联标准输出或错误输出
        setlinebuf(stderr) 设置错误输出为行缓冲
    d_log_sync_mask
  dfuse_info->di_eq_count = 1
  daos_init -> 初始化客户端库
    daos_debug_init
    daos_hhash_init_feats
    dc_agent_init
    dc_job_init
    dc_mgmt_net_cfg
    daos_eq_lib_init -> 初始化事件队列库 -> static tse_sched_t daos_sched_g -> 指向不属于 EQ 一部分的事件的全局调度程序的指针。 作为 EQ 一部分初始化的事件将在该 EQ 调度程序中进行跟踪
      crt_init_opt
      crt_context_create(&daos_eq_ctx) -> 全局共享网络上下文, 所有事件队列(eq)共享使用这个上下文
      tse_sched_init(&daos_sched_g, NULL, daos_eq_ctx) -> 初始化调度器(无事件队列), 为无eq事件设置调度器
    dc_mgmt_net_cfg_check
    pl_init
    dc_mgmt_init
    dc_pool_init
    dc_cont_init
    dc_obj_init
  dfuse_fs_init(dfuse_info) -> daos用户态文件信息=文件系统控制器
    D_ALLOC_ARRAY(fs_handle->di_eqt, fs_handle->di_eq_count) -> eq数组
    d_hash_table_create_inplace dpi_pool_table 打开的池表， 创建hash表 大小=power2(n次方)， 操作方法
    dpi_iet open inodes
    for (i = 0; i < fs_handle->di_eq_count; i++)
      struct dfuse_eq *eqt = &fs_handle->di_eqt[i] -> 根据传入的EQ数量, 将eq与文件系统句柄中的eq表绑定
      eqt->de_handle = fs_handle -> 互存指针,双向绑定
    sem_init(&eqt->de_sem, 0, 0) -> 在 eq 之前创建信号量，因为无法检查 sem_init() 是否已被调用，如果没有调用 sem_destroy 也是无效的。 这样我们就可以避免添加额外的内存来跟踪信号量的状态
    daos_eq_create(&eqt->de_eq) -> 一个事件队列关联一个网络上下文， 跟踪池的多个事件 -> 创建事件队列。 事件队列用于保存和池化多个事件。 创建的每个事件队列都将创建一个与事件队列关联的网络（cart）上下文。 网络上下文创建是一项昂贵的操作，并且在某些系统上网络上下文的数量可能受到限制。 因此，建议不要在用户应用程序或中间件中创建大量事件队列
      eq = daos_eq_alloc() -> 分配eq
        D_INIT_LIST_HEAD(&eq->eq_running)
        D_INIT_LIST_HEAD(&eq->eq_comp)
        daos_hhash_hlink_init(&eqx->eqx_hlink, &eq_h_ops)
        return eq
      crt_context_create(&eqx->eqx_ctx)
        crt_contpext_provider_create
          crt_context_init
      daos_eq_insert(eqx)
        daos_hhash_link_insert(&eqx->eqx_hlink, DAOS_HTYPE_EQ) -> 插入全局hash表(struct daos_hhash_table	daos_ht)
      daos_eq_handle(eqx, eqh)
        daos_hhash_link_key(&eqx->eqx_hlink, &h->cookie) -> 关联key
      tse_sched_init(&eqx->eqx_sched, NULL, eqx->eqx_ctx) -> 初始化调度器 -> struct tse_sched_private -> 调度器及队列,属性等
        struct tse_sched_private	*dsp = tse_sched2priv(sched) -> 设置调度器私有指针dsp
        D_INIT_LIST_HEAD(&dsp->dsp_init_list); -> 初始队列
        D_INIT_LIST_HEAD(&dsp->dsp_running_list); -> 运行队列
        D_INIT_LIST_HEAD(&dsp->dsp_complete_list) -> 完成队列
        D_INIT_LIST_HEAD(&dsp->dsp_sleeping_list) -> 睡眠队列
        D_INIT_LIST_HEAD(&dsp->dsp_comp_cb_list); -> 完成回调队列
        tse_sched_register_comp_cb(sched, comp_cb, udata) -> 初始回调为空
          dsc->dsc_comp_cb = comp_cb -> 设置调度器的完成回调和回调参数(udata)
          d_list_add(&dsc->dsc_list, &dsp->dsp_comp_cb_list) -> 将调度器的完成回调 -> 调度器的完成回调队列
        sched->ds_udata = udata -> 将网络上下文 daos_eq_ctx 设置到调度器的用户数据指针上(也可以是回调数据等)
      daos_eq_putref(eqx) -> 减一次引用计数(ch_rec_decref)
  duns_resolve_path
  dfuse_pool_connect
  dfuse_cont_open
  dfuse_fs_start 启动文件系统
    d_hash_rec_insert(&fs_handle->dpi_iet 将根插入hash表, 在 dfuse_reply_entry 中也会插入: d_hash_rec_find_insert(&fs_handle->dpi_iet
    d_slab_init(&fs_handle->di_slab, fs_handle)
    for (i = 0; i < fs_handle->di_eq_count; i++)
      d_slab_register(&fs_handle->di_slab, &read_slab, eqt, &eqt->de_read_slab)
        create_many(type)
          ptr   = create(type) -> create(struct d_slab_type *type)
            type->st_reg.sr_init(ptr, type->st_arg) -> dfuse_event_init -> ev->de_eqt = handle -> 为ev绑定daos_event_t
            if (!type->st_reg.sr_reset(ptr)) -> dfuse_read_event_reset(void *arg) -> 重置读事件ev
              D_ALLOC(ev->de_iov.iov_buf, DFUSE_MAX_READ) -> 读最大1MB
              ev->de_sgl.sg_nr       = 1
              daos_event_init(&ev->de_ev, ev->de_eqt->de_eq, NULL) -> 父事件为空, 也支持父事件, 如: daos_event_init, Parent – “父”事件，如果没有父事件，则可以为 NULL。 如果它不为 NULL，则调用者将永远不会看到此事件的完成，相反，只有当父级的所有子级完成时才会看到父级的完成。 然而，与父事件相关联的操作可以在其子事件之前启动或完成。 父事件完成只是将多个事件完成状态合并为一个的简单方法。(child_events[i], DAOS_HDL_INVAL, &event)
                evx->evx_status	= DAOS_EVS_READY
                if (daos_handle_is_valid(eqh)) -> 句柄有效
                  eqx = daos_eq_lookup(eqh)
                  evx->evx_ctx = eqx->eqx_ctx
                  evx->evx_sched = &eqx->eqx_sched -> 继承EQ的网络和调度器
          entry = ptr + type->st_reg.sr_offset
          d_list_add_tail(entry, &type->st_free_list) -> 将对象加入空闲列表,计数器+1
          type->st_free_count++
        d_list_add_tail(&type->st_type_list, &slab->slab_list) -> 将slab放入列表备用
    for (i = 0; i < fs_handle->di_eq_count; i++) 
    dfuse_progress_thread pthread_create(&fs_handle->dpi_thread, NULL, dfuse_progress_thread, fs_handle) 异步进度线程，该线程在启动时使用事件队列启动，并阻塞在信号量上，直到创建异步事件，此时线程唤醒并在 daos_eq_poll() 中忙于轮询直到完成
      sem_wait
      daos_eq_poll  从 EQ 中检索完成事件 -> return epa.count
        daos_eq_lookup 查找私有事件队列
          daos_hhash_link_lookup
        crt_progress_cond(epa.eqx->eqx_ctx, timeout, eq_progress_cb, &epa)
          eq_progress_cb -> 事件驱动调度器执行
        dfuse_launch_fuse(fs_handle, &args) 创建fuse文件系统
          fuse_session_new(args, &dfuse_ops, sizeof(dfuse_ops), fs_handle)
          fuse_session_mount
          dfuse_send_to_fg
          dfuse_loop
  dfuse_fs_fini


dfuse_progress_thread
  rc = sem_wait(&fs_handle->dpi_sem) 等dpi_sem信号
  daos_eq_poll
  daos_event_fini 完成一个事件。 如果事件已被传递到任何 DAOS API，则它只能在从 EQ 中轮询出来时才能完成，即使通过调用 daos_event_abort() 中止也是如此。 如果事件是用父事件初始化的，那么该事件将从父事件的子列表中删除。 如果ev本身是一个父事件，那么这个函数会finalize所有的子事件和ev, 此调用返回后，ev 中的条目不应被视为有效
  ev->de_complete_cb(ev) -> dfuse_cb_write_complete 写完成回调

struct fuse_lowlevel_ops dfuse_ops dfuse低层操作对象
  .create		= df_ll_create
	.open		= dfuse_cb_open,
	.release	= dfuse_cb_release,
	.write_buf	= dfuse_cb_write,
	.read		= dfuse_cb_read,
	.readlink	= dfuse_cb_readlink,
	.ioctl		= dfuse_cb_ioctl,

struct fuse_file_info libfuse


客户端写数据：xb/write.c -> write(fd, direct_write_buf, BUF_SIZE)
write -> dfuse_cb_write 回调写 src/client/dfuse/fuse3
  fuse_req_userdata
  fuse_req_ctx
  fuse_buf_size(bufv)
  ibuf = FUSE_BUFVEC_INIT(len) 分配本地缓冲区
  DFUSE_TRA_DEBUG 调试
  fuse_buf_copy(&ibuf, bufv, 0)
  dfuse_cache_evict
  d_slab_acquire 以高效的方式分配数据
  fuse_buf_copy libfuse
  daos_event_init 线程事件初始化
    evx->evx_status	= DAOS_EVS_READY
    D_INIT_LIST_HEAD(&evx->evx_child) 初始化链表
    daos_eq_putref 从事件队列继承传输上下文
  ev->de_complete_cb = dfuse_cb_write_complete -> 设置回调
  d_iov_set(&ev->de_iov, ibuf.buf[0].mem, len)  # 设置io向量， 将第二参数的地址和长度赋值给第一个参数
  ev->de_sgl.sg_iovs = &ev->de_iov   sgl分散聚集列表
  readahead ie_truncated 预读和截断
  dfs_write (文件系统， 对象，sgl列表，文件(对象)偏移，事件) 将数据写到文件对象
    事件为空
      daos_event_launch
      daos_event_complete
    daos_event_errno_rc(ev) 将错误码转正
      daos_ev2evx(ev)
    daos_array_write 写数组对象: daos_array_write(obj->oh, DAOS_TX_NONE, &iod, sgl, ev)
      dc_task_create(dc_array_write, NULL, ev, &task) -> 创建任务
      args = dc_task_get_args(task)
      dc_task_schedule(task, true)  task与args做转换: dc_task_get_args 调度任务
    return daos_der2errno(rc)
  sem_post(&fs_handle->dpi_sem)   解锁信号量(+1,如果大于0,其他线程将被唤醒执行),唤醒线程（线程同步）: dfuse_progress_thread sem_wait(&fs_handle->dpi_sem)


dc_array_write
  daos_task_get_args task和args可互转
  dc_array_io -> opc = DAOS_OPC_ARRAY_WRITE 操作码是写数组  读:DAOS_OPC_ARRAY_READ
    array_hdl2ptr
    io_extent_same
    D_INIT_LIST_HEAD(&io_task_list)
    daos_task_create(DAOS_OPC_ARRAY_GET_SIZE 短读任务 DAOS_OPC_ARRAY_READ
    while (u < rg_iod->arr_nr) 遍历每个范围，但同时组合属于同一 dkey 的连续范围。 如果用户给出的范围不增加偏移量，则它们可能不会合并，除非分隔范围也属于同一个 dkey
      compute_dkey 计算分布式key 在给定此范围的数组索引的情况下计算 dkey。 还计算从我们开始的索引开始，dkey 可以保存的记录数写作。 相对于 dkey 的记录索引
      struct io_params *prev, *current 如果有多个dkey io, 则通过链表连接起来
      num_ios++
      d_iov_set(dkey, &params->dkey_val, sizeof(uint64_t));
      d_iov_set(&iod->iod_name, &params->akey_val, 1);
      compute_dkey 再次计算dkey
      create_sgl 创建分散聚集列表
      daos_task_create(DAOS_OPC_OBJ_FETCH 读: DAOS_OPC_ARRAY_READ 按索引号 -> dc_obj_fetch_task
      daos_task_create(DAOS_OPC_OBJ_UPDATE 写 或 DAOS_OPC_ARRAY_PUNCH truncate dc_funcs[opc].task_func 客户端方法数组
      daos_task_get_args
      tse_task_register_deps 注册在计划任务之前需要完成的依赖任务。 依赖任务无法进行, 如果一个任务依赖于其他任务，只有依赖的任务完成了，才可以将任务添加到调度器列表中
      tse_task_list_add(io_task, &io_task_list)  d_list_add_tail(&dtp->dtp_task_list, head); 添加任务到链表
    tse_task_register_comp_cb(task, free_io_params_cb, &head, sizeof(head))  为任务注册完成回调
    if (op_type == DAOS_OPC_ARRAY_READ && array->byte_array) 短读
      tse_task_register_deps(task, 1, &stask) 注册依赖任务
      tse_task_list_add(stask, &io_task_list) 加到io任务列表
    tse_task_list_sched(&io_task_list, false); 调度执行
    array_decref(array)
    tse_task_register_cbs(stask, check_short_read_cb 读回调
    tse_sched_progress(tse_task2sched(task)) 推进/处理, 先处理依赖任务
      tse_sched_run


check_short_read_cb

通过以下对象连接
.cpf_name =
daos_opc_t
dc_funcs[opc].task_func 客户端方法数组

DAOS_OPC_OBJ_UPDATE 写
  dc_obj_update_task DAOS_OBJ_RPC_UPDATE
    obj_req_valid(task, args, DAOS_OBJ_RPC_UPDATE
      obj_auxi = tse_task_stack_push(task, sizeof(*obj_auxi))
      tse_task_stack_pop
    dc_tx_attach(args->th, obj, DAOS_OBJ_RPC_UPDATE, task) 如果事务有效(hdl.cookie == 1), 则走dtx
  dc_obj_update 否则
      obj_task_init_common(task, DAOS_OBJ_RPC_UPDATE
        tse_task_stack_push
        shard_task_list_init(obj_auxi)
      obj_rw_req_reassemb 重新组装
      dkey_hash = obj_dkey2hash
      obj_req_get_tgts 获取对象对应的目标
        obj_dkey2grpmemb
          obj_dkey2grpidx
            pool_map_ver = pool_map_get_version(pool->dp_map)
            grp_size = obj_get_grp_size(obj)
            grp_idx = d_hash_jump(hash, obj->cob_shards_nr / grp_size) how hash generate? obj with pool
        obj_shards_2_fwtgts
          obj_shard_tgts_query 分片目标查询
            obj_shard_open
              dc_obj_shard_open
                pool_map_find_target 二分查找
                  comp_sorter_find_target(sorter, id)
                    daos_array_find
                      array_bin_search
          obj_grp_leader_get
            pl_select_leader obj_get_shard
              array_bin_search 二分查找 daos_obj_classes
      tse_task_register_comp_cb(task, obj_comp_cb, NULL, 0)
      obj_csum_update(obj, args, obj_auxi)
      obj_rw_bulk_prep
      obj_req_fanout(obj, obj_auxi, dkey_hash, map_ver, epoch, shard_rw_prep, dc_obj_shard_rw, task)  扇出 shard_io_cb = io_cb = dc_obj_shard_rw


ds_obj_rw_handler 接收端的回调
  obj_ioc_begin 访问VOS前的各种检查
  obj_rpc_is_fetch
  process_epoch
  obj_rpc_is_fetch
  rc = dtx_begin 返回超时?
    dtx_handle_init
      dtx_shares_init(dth) 初始化以下链表, 提交,中断,活动,检查
      dtx_epoch_bound
      vos_dtx_rsrvd_init(dth)
  obj_local_rw 本地读写


创建任务, daos_task_create
创建一个异步任务并将其与 daos 客户端操作相关联。 对于同步操作，请为该操作使用特定的 API。 通常，此 API 用于需要将一系列 daos 操作排队到 DAOS 异步引擎中的用例，这些任务之间的执行顺序具有特定的依赖性。 例如，用户可以创建任务来打开一个对象，然后使用插入到打开任务更新中的依赖项来更新该对象。 对于更简单的工作流程，用户可以使用基于事件的 API 而不是任务。


dfuse_start
  dfuse_progress_thread


重要结构：

tse_task_t	*io_task = NULL; io任务



pool_map
dfuse_cb_write position=0
  dfuse_cb_write_complete 回调
  dfs_write
    dc_array_write
    daos_task_create DAOS_OPC_OBJ_UPDATE
    dc_obj_update_task
    dc_obj_update
      obj_update_shards_get
      obj_rw_bulk_prep
      obj_req_fanout shard_rw_prep dc_obj_shard_rw = shard_io_cb 请求扇出
        io_prep_cb shard_rw_prep 发送io前执行的回调
        shard_io 分片IO
          obj_shard_open
            map_ver
            dc_obj_shard_open 打开分片
            dc_cont_tgt_idx2ptr 根据容器handler和taget索引，获取池的目标pool_target
              pool
              dc_hdl2pool
                daos_hhash_link_lookup(poh.cookie)
              pool_map_find_target
              dc_pool_put
                daos_hhash_link_putref
              dc_cont_put
          shard_auxi->shard_io_cb(obj_shard,...) -> dc_obj_shard_rw 对象分片读写回调
          obj_shard_close



import struct
pool_target


main
dfuse_start
  dfuse_lanuch_fuse
    ll_loop_fn
      dfuse_loop
        start_one
          dfuse_do_work
            fuse_session_process_buf_int libfuse3.so.3
              do_write_buf(req, in->nodeid, inarg, buf)
                se->op.write_buf(req, nodeid, &bufv, arg->offset, &fi)
                dfuse_cb_write step?




dc_obj_shard_rw 客户端对象分片读写(读写对象分片)
  obj_shard_ptr2pool(shard) 根据分片获取池
  obj_req_create opc = DAOS_OBJ_RPC_UPDATE -> ds_obj_rw_handler
  uuid_copy
  daos_dti_copy 拷贝dtx_id
  跳过ec逻辑
  tse_task_register_comp_cb
  daos_rpc_send
    crt_req_send daos_rpc_cb -> tse_task_complete 发送完成回调流程:hg -> crt_hg_req_send_cb -> crp_complete_cb -> -> daos_rpc_cb -> dc_rw_cb

超时检测和业务回调是在不同的线程中并发执行

ds_obj_rw_handler
  obj_rw_reply
    obj_reply_set_status 与 obj_reply_get_status 成对使用
      ((struct obj_rw_out *)reply)->orw_ret = status 设置回复状态


怎么处理超时

crt_swim_cli_cb
ca_arrays

tse测试, test: src/common/tests/sched.c -> gdb build/debug/gcc/src/common/tests/sched -> cd build/debug/gcc/src/common/tests/; gdb sched -> int main(int argc, char **argv)
sched_ut_setup -> daos_debug_init(DAOS_LOG_DEFAULT)
sched_uts
  sched_test_1 -> Scheduler create/complete/cancel
  sched_test_3 -> Task Reinitialization in Completion CB -> 反复执行,直到满足重入次数(REINITS)
  sched_test_6 -> Task Dependencies 依赖任务
    tse_sched_init(&sched, NULL, 0)
    Test N -> 1 dependencies -> 1个依赖
    tse_task_create(check_func_n, &sched, counter, &task) -> 主任务
    tse_task_create(inc_func, &sched, counter, &tasks[i]) -> 创建依赖任务
    tse_task_schedule(tasks[i], false) -> 调度依赖任务
    tse_task_register_deps(task, NUM_DEPS, tasks) -> 注册依赖任务, 多个子任务依赖1个主任务
    tse_task_schedule(task, false) -> 调度主任务
    tse_sched_progress(&sched) -> 执行调度器(主任务依赖计数器不为0, 不会加入运行队列, 只能执行依赖任务) -> inc_func
    tse_task_complete(tasks[i], 0) -> 完成依赖(子任务)
    tse_sched_progress(&sched) -> 再次执行调度器(依赖计数器为0, 可以加入运行队列, 执行主任务) -> check_func_n
    tse_task_complete(task, 0) -> 完成主任务
    tse_sched_check_complete(&sched)
    -------------------------------------------
    Test 1 -> N dependencies -> 多个依赖
    tse_task_create(inc_func, &sched, counter, &task) -> 主任务
    tse_task_create(check_func_1, &sched, counter, &tasks[i]) 
    tse_task_register_deps(tasks[i], 1, &task) -> 
    tse_task_schedule(tasks[i], false)
    tse_task_schedule(task, false)
    tse_sched_progress(&sched); -> 先执行依赖任务 -> inc_func
    tse_task_complete(task, 0); -> 先完成依赖任务
    tse_sched_progress(&sched); -> 再执行主任务(多次) -> check_func_1
    tse_task_complete(tasks[i], 0) -> 再依次完成数组中的主任务
    tse_sched_check_complete(&sched)
  ...
sched_ut_teardown -> daos_debug_fini()


tse: 调度, 1300+400, 涉及文件: event.h, daos_event.h, tse.h, tse.c, tse_internal.h
dc_task_create
  sched = daos_ev2sched(ev) -> EV事件转调度器
  tse_task_create(func, sched, NULL, &task) -> 初始化 tse_task。 该任务(task)会被添加到调度器(sched)任务列表中，稍后被调度，如果提供了依赖任务，则该任务将被添加到依赖任务的dep列表中，一旦依赖任务完成，则添加该任务到调度程序列表(先完成依赖任务, 然后添加主任务)
  task_ptr2args 指针转参数
    D_INIT_LIST_HEAD(&dtp->dtp_list) -> 初始化任务链表, 调度时插入到调度器的初始化队列(dsp_init_list) -> d_list_add_tail(&dtp->dtp_list, &dsp->dsp_init_list)
    D_INIT_LIST_HEAD(&dtp->dtp_task_list)
  	D_INIT_LIST_HEAD(&dtp->dtp_dep_list)
	  D_INIT_LIST_HEAD(&dtp->dtp_comp_cb_list)
	  D_INIT_LIST_HEAD(&dtp->dtp_prep_cb_list)
    ...
    dtp->dtp_func	  = task_func
    ...
    tse_task_buf_embedded 获取任务的嵌入式缓冲区，用户可以使用它来携带功能参数。 任务的嵌入式缓冲区有大小限制，如果 buf_size 大于限制，此函数将返回 NULL。 用户应通过 tse_task_set_priv() 使用私有数据来传递大参数。 MSC - 我将其更改为只是一个缓冲区，而不是像以前那样, 不断给一个额外的指针指向大的预涂层缓冲区。 以前的方式不适用于公共用途。我们现在应该使它更简单，更通用，如下面的评论
      tse_task_buf_size
        return (size + 7) & ~0x7
  args->ta_magic = DAOS_TASK_MAGIC
  tse_task_register_comp_cb(task, task_comp_event, NULL, 0) -> dtc->dtc_cb = cb task_comp_event 注册完成回调
    register_cb(task, true, comp_cb, arg, arg_size)
      d_list_add(&dtc->dtc_list, &dtp->dtp_comp_cb_list) 插入到列表开始处
  failed -> tse_task_decref(task) -> 如果失败则判断引用计数




执行任务的回调，如果所有的CB都执行完则返回true
并且不重新启动任务。 如果任务被用户重新初始化，则意味着
它又在飞行中，所以我们在重新初始化它的当前 CB 处中断，
并返回 false，表示任务未完成。 所有剩余的 CB
未执行的仍然附加，但已执行的
此时已经从列表中删除
tse_task_complete_callback
  ret = dtc->dtc_cb(task, dtc->dtc_arg); task_comp_event


rpc发送回调 / ...
tse_task_complete
  done = tse_task_complete_callback(task) -> 执行回调, 执行任务的回调，如果所有CB都执行完毕，则返回true，并且不重新初始化任务。 如果任务被用户重新初始化，则意味着它再次处于运行状态，因此我们在重新初始化它的当前 CB 处中断，并返回 false，
意思是任务没有完成。 所有剩余未执行的 CB 仍保持附加状态，但已执行的 CB 此时已从列表中删除
    d_list_for_each_entry_safe(dtc, tmp, &dtp->dtp_comp_cb_list, dtc_list) -> 遍历任务的完成列表
    d_list_del(&dtc->dtc_list) -> 将任务从完成队列摘下来
    ret = dtc->dtc_cb(task, dtc->dtc_arg) -> 执行任务注册时设置的回调, 参考 -> d_list_add(&dtc->dtc_list, &dtp->dtp_comp_cb_list)
  D_MUTEX_LOCK(&dsp->dsp_lock) -> 锁调度器
  tse_task_complete_locked(dtp, dsp)
    d_list_move_tail(&dtp->dtp_list, &dsp->dsp_complete_list) -> 完成后, 将任务移动到调度器的完成队列(run -> com?)
  tse_sched_process_complete(dsp) -> 更新调度列表
    D_MUTEX_LOCK(&dsp->dsp_lock) -> 对调度器加锁
    d_list_splice_init(&dsp->dsp_complete_list, &comp_list)
    tse_task_post_process(task) -> 检查完成列表中的任务、依赖任务状态检查、调度状态更新等。此后任务将移至完成列表
    ...


crt_rpc_completed call 2？ 重复完成，duplicated completions

daos_rpc_send -> crt_req_send(rpc, daos_rpc_cb, task)
daos_rpc_cb
  tse_task_complete
    tse_sched_process_complete(dsp) -> 更新调度器中的任务列表(状态)
      post_procee
        tse_task_complete_callback
          dtc_cb 执行注册时的回调 register_cb -> dc_rw_cb
          task_comp_event -> daos_event_complete(task_ptr2args(task)->ta_ev, task->dt_result)
            eqx = daos_eq_lookup(evx->evx_eqh)
              hlink = daos_hhash_link_lookup(eqh.cookie)
              return container_of(hlink, struct daos_eq_private, eqx_hlink)
            daos_event_complete_locked(eqx, evx, rc) -> master
              evx->evx_status = DAOS_EVS_COMPLETED -> 事件置为已完成状态
              daos_event_complete_cb(evx, rc)
                d_list_for_each_entry_safe(ecl, tmp, &evx->evx_callback.evx_comp_list
                ecl->op_comp_cb -> N
            event_complete
              complete_locked
                daos_event_complete_cb
                  d_list_for_each_entry_safe
                  d_list_del_init
                  __gurt_list_del

dc_rw_cb 
  opc_get
  DAOS_FAIL_CHECK
dc_obj_shard_rw
  tse_task_register_comp_cb(task, dc_rw_cb...)  dtc->dtc_cb = cb   <- tse_task_complete_callback
    dc_rw_cb
      rc = obj_reply_get_status  DER_SHUTDOWN(-2017) | DER_IO(-2001) 2008 NOTLEADER
        ((struct obj_rw_out *)reply)->orw_ret

dc_task_schedule
  task_is_valid
  daos_event_launch
  tse_task_schedule
    tse_task_schedule_with_delay 与 tse_task_schedule 相同，如果 instant(立即执行) 为 false，则期望任务不会在 delay 微秒内执行
      ready = (dtp->dtp_dep_cnt == 0 && d_list_empty(&dtp->dtp_prep_cb_list)) -> 任务准备好了(无依赖任务且预执行(回调)链表为空)
      ...
      dtp->dtp_wakeup_time = daos_getutime() + delay -> 延迟执行
      tse_task_insert_sleeping(dtp, dsp)
      ...
      dtp->dtp_func(task) -> 准备好了, 立即执行任务

daos_event_comp_list 事件完成列表


dc_rw_cb DER_NO_HDL 'Invalid handle' 1002 追rc
  opc = OBJ_RPC_UPDATE = 0
  rc = obj_reply_get_status orw_ret




crt_ctx_epi_abort
  crt_rpc_complete



dc_pool_query


.sr_reset   = dfuse_write_event_reset
  daos_event_init(&ev->de_ev, ev->de_eqt->de_eq, NULL)
  
daos_event_init
  D_CASSERT
  D_INIT_LIST_HEAD(&evx->evx_child);
	D_INIT_LIST_HEAD(&evx->evx_link);
	D_INIT_LIST_HEAD(&evx->evx_callback.evx_comp_list);
  eqx = daos_eq_lookup(eqh)
    hlink = daos_hhash_link_lookup(eqh.cookie)
      d_hhash_link_lookup(daos_ht.dht_hhash, key)
  daos_eq_putref(eqx)
  evx->evx_sched = &daos_sched_g -> 绑定到全局调度器



daos_hhash_init_feats
  d_hhash_create dht_hhash



读
dc_array_read
  dc_array_io DAOS_OPC_ARRAY_READ



读
dc_obj_fetch_task
  obj_req_valid 校验请求
  obj_task_init
  obj_req_with_cond_flags
    obj_cond_fetch_prep 预处理, 将 obj 任务拆分为多个子任务
  obj_fetch_shards_get
  obj_shards_2_fwtgts
  obj_csum_fetch
  obj_rw_bulk_prep
  obj_req_fanout shard_rw_prep dc_obj_shard_rw 请求扇出
  ...
obj_task_complete(task, rc)

重复完成 coredump
dfuse_ops
.lookup		= df_ll_lookup 按名称查找目录条目并获取其属性
df_ll_lookup
  // 查inodes表
  d_hash_rec_find(&fs_handle->dpi_iet, &parent, sizeof(parent))  d_hash_rec_insert(&fs_handle->dpi_iet 插入hash表
  parent_inode->ie_dfs->dfs_ops->lookup(req, parent_inode, name) 调用父节点的查找方法
    dfuse_cb_lookup
  d_hash_rec_decref(&fs_handle->dpi_iet, rlink)

dfuse_cb_getattr 先获取属性
dfuse_dfs_ops
.lookup		= dfuse_cb_lookup
分配ie
dfs_lookupx 查询条目,获取属性, 生成?
  dfs_lookup_rel_int
    check_name 检查文件名和文件名长度
    get_daos_obj_mode 返回对象模式, 只读|读写
    fetch_entry 获取条目/查条目
      d_iov_set(&dkey, (void *)name, len)
      d_iov_set(&iod->iod_name, INODE_AKEY_NAME
      DAOS_IOD_ARRAY
      sgl->sg_iovs	= sg_iovs
      daos_obj_fetch 从数组中查询对象 ioms: 存储缓冲层(接收缓冲区)
        dc_obj_fetch_task_create -> 创建获取任务
          DAOS_API_ARG_ASSERT(*args, OBJ_FETCH) 通过断言检查参数预定义大小和传入的参数大小
          dc_task_create(dc_obj_fetch_task, tse, ev, task) dc_obj_fetch_task -> func -> task_func -> dtp_func daos任务私有回调
            daos_event_priv_get(&ev) 获取基于线程的私有事件, static __thread daos_event_t	ev_thpriv; 线程私有数据
            sched = daos_ev2sched(ev)
            tse_task_create
        dc_task_schedule(task, true) -> 调度,要求立即执行, 调度由dc_task_create_ev()创建的任务，如果任务的关联事件是私有事件，则该函数将等待任务完成，否则立即返回，通过测试事件或轮询EQ来发现其完成
          tse_task_schedule(task, instant)
        if (daos_event_is_priv(ev)) -> 有私有事件
          daos_event_priv_wait() -> 等事件
            crt_progress_cond(evx->evx_ctx, ev_prog_timeout, ev_progress_cb, &epa)
              HG_Trigger -> HG_Core_trigger -> hg_core_forward_cb -> daos_rpc_cb -> tse_task_complete

    switch (entry.mode & S_IFMT) 判断条目类型 文件,符号链接, 
      daos_array_open_with_attr 普通文件 S_IFREG
        dc_array_open
          daos_task_create(DAOS_OPC_OBJ_OPEN 创建对象打开任务, 创建一个异步任务并将其与 daos 客户端操作相关联。 对于同步操作，请为该操作使用特定的 API。 通常，此 API 用于需要将一系列 daos 操作排队到 DAOS 异步引擎中的用例，这些任务之间的执行顺序具有特定的依赖性。 例如，用户可以创建任务来打开一个对象，然后使用插入到打开任务更新中的依赖项来更新该对象。 对于更简单的工作流程，用户可以使用基于事件的 API 而不是任务
          tse_task_register_deps 注册依赖任务
            for num_deps 遍历依赖任务数量
              tse_task_add_dependent(task, dep_tasks[i])
                Add dependent 依赖 -> 主任务
                  d_list_add_tail(&tlink->tl_link, &dep_dtp->dtp_dep_list) 添加到依赖链表
          tse_task_register_comp_cb(task, open_handle_cb 注册打开完成回调
          tse_task_schedule(open_task, false) 调度任务
          tse_sched_progress(tse_task2sched(task)) 推进任务
          daos_task_create(DAOS_OPC_OBJ_FETCH 查看元数据
atomic_store_relaxed(&ie->ie_ref, 1); 初始化引用计数 原子操作
dfs_obj2id 文件系统转daos对象 128位(32+96)
dfuse_compute_inode
S_ISDIR 目录
  check_for_uns_ep 统一命名空间
dfuse_reply_entry

io故障

打开回调
open_handle_cb
  open_with_attr = 1
  array_hdl_link(array) 
  *args->oh = array_ptr2hdl(array)
    daos_hhash_link_lookup(oh.cookie) 根据key查找hash表


dc_array_set_size


0x00001 ----> oh



dc_array_get_size
dc_array_stat
DAOS_OPC_OBJ_QUERY_KEY DAOS_OBJ_RPC_QUERY_KEY
dc_obj_query_key
  obj_task_init(api_task, DAOS_OBJ_RPC_QUERY_KEY
  queue_shard_query_key_task
    shard_query_key_task // 客户端pool map err
    dc_obj_shard_query_key
      obj_shard_query_key_cb


opc=0x4090009 DAOS_OBJ_RPC_QUERY_KEY 查键
ds_obj_query_key_handler_1

全局rpc操作码: cg_opc_map


open shard
obj_shard_open 18


shard_query_key_task

daos_task_create(DAOS_OPC_OBJ_QUERY_KEY


dc_obj_open_task_create
  dc_task_create(dc_obj_open, "obj open")
  


obj_rw_req_reassemb 重新组装对象读写请求


编译镜像: docker build  . -f utils/docker/Dockerfile.el.8 -t daos


components:
site_scons/components/__init__.py

cache:
RUN: git clone https://github.com/pmodels/argobots.git /home/daos/pre/build/external/release/argobots
tar -zcvf cache.tgz cache

rocky8:
yum install -y vim net-tools gdb


cp daos_server_docker_main.yml /opt/daos/etc/daos_server.yml
mkdir -p /etc/daos/certs
cp -r daosCA/certs/* /etc/daos/certs/


write.c, new, master, dfuse写流程
master: src/client/dfuse/dfuse_main.c:659 -> main
...
start_one(struct dfuse_tm *dtm)
pthread_create(&dt->dt_id, NULL, dfuse_do_work, dt)
dfuse_do_work
  fuse_session_process_buf_int -> 处理通用缓冲区中提供的原始请求 fusion_buf 可能包含内存缓冲区或管道文件描述符
  dfuse_cb_write
    dfuse_mcache_evict -> 将缓存置为无效
    ev = d_slab_acquire(eqt->de_write_slab) -> d_slab_acquire(struct d_slab_type *type) -> 获取一个新对象，这是在关键路径上考虑的，所以应该尽可能轻量级
      type->st_no_restock++ -> 补货
      if (type->st_free_count == 0)
        int count = restock(type, 1) -> 剩余个数为0时需要补货
      ptr = (void *)entry - type->st_reg.sr_offset -> 从空闲表中拿一个
    ibuf.buf[0].mem = ev->de_iov.iov_buf -> 将ev与ibuf通过iov_buf关联
    fuse_buf_copy(&ibuf, bufv, 0)
    ev->de_complete_cb = dfuse_cb_write_complete -> 为ev设置回调
    dfs_write
      daos_array_write(obj->oh, DAOS_TX_NONE, &iod, sgl, ev)
        dc_task_create(dc_array_write, NULL, ev, &task) -> tse 创建任务

    sem_post
    d_slab_restock
    return


...
daos_progress
  tse_sched_progress(sched)



tse_task_list_traverse -> 遍历列表 \a head 上的所有任务，对每个任务调用参数为 \a arg 的 \a cb。 用户可以自由地从\a cb执行中的列表\a head中删除任务

depend_task, 遍历, event_queue, 40%, 


obj_shard_task_sched(obj_auxi, epoch);



#0  dc_rw_cb (task=0x7f9198004530, arg=<optimized out>) at src/object/cli_shard.c:952
...
#0  tse_task_complete_callback (task=task@entry=0x7f68ec004d20) at src/common/tse.c:515
...
#0  tse_task_complete (task=0x7f68ec004d20, ret=0) at src/common/tse.c:858
#1  0x00007f693ce7cf48 in daos_rpc_cb (cb_info=<optimized out>) at src/client/api/rpc.c:24
#2  0x00007f693b13bd8a in crt_hg_req_send_cb (hg_cbinfo=<optimized out>) at src/cart/crt_hg.c:1364
#3  0x00007f693a2e34ce in hg_core_forward_cb (callback_info=<optimized out>) at /home/xb/project/stor/daos/main/daos/build/external/debug/mercury/src/mercury.c:990
#4  0x00007f693a2f3034 in hg_core_trigger_entry (hg_core_handle=0x5608ec45c780) at /home/xb/project/stor/daos/main/daos/build/external/debug/mercury/src/mercury_core.c:5406
#5  hg_core_trigger (context=0x5608ec2c9500, timeout_ms=timeout_ms@entry=0, max_count=max_count@entry=256, actual_count_p=actual_count_p@entry=0x7f68f3ffda54) at /home/xb/project/stor/daos/main/daos/build/external/debug/mercury/src/mercury_core.c:5267
#6  0x00007f693a2fbdbb in HG_Core_trigger (context=<optimized out>, timeout=timeout@entry=0, max_count=max_count@entry=256, actual_count_p=actual_count_p@entry=0x7f68f3ffda54)
    at /home/xb/project/stor/daos/main/daos/build/external/debug/mercury/src/mercury_core.c:6419
#7  0x00007f693a2e748e in HG_Trigger (context=context@entry=0x5608ec266b70, timeout=timeout@entry=0, max_count=max_count@entry=256, actual_count_p=actual_count_p@entry=0x7f68f3ffda54)
    at /home/xb/project/stor/daos/main/daos/build/external/debug/mercury/src/mercury.c:2197 -> ibv_poll_cq
#8  0x00007f693b1490e9 in crt_hg_progress (hg_ctx=hg_ctx@entry=0x5608ec253b18, timeout=timeout@entry=0) at src/cart/crt_hg.c:1540
#9  0x00007f693b0f9eea in crt_progress_cond (crt_ctx=0x5608ec253b00, timeout=timeout@entry=0, cond_cb=cond_cb@entry=0x7f693ce6732b <ev_progress_cb>, arg=arg@entry=0x7f68f3ffdb00) at src/cart/crt_context.c:1668
#10 0x00007f693ce712f3 in daos_event_priv_wait () at src/client/api/event.c:1276 ->  废弃 -> dc_task_new + dc_task_schedule, DAOS-6958 事件：出现进度错误时正确的事件处理 (#4911)crt_progress 有时会返回非超时的错误。 这些在同步 IO 调用中没有得到正确处理，导致私有事件对于后续 IO 处于不可用状态。此 PR 检查购物车中的错误，并在返回错误时重新初始化私有事件，以便可以重新使用它，而不是返回给用户
#11 0x00007f693ce7e93c in dc_task_schedule (task=<optimized out>, instant=instant@entry=true) at src/client/api/task.c:124  -> 执行任务主体函数: dc_obj_fetch_task -> 请求扇出 -> 走网络send接口, 发送完成后, 通过trigger调发送回调: daos_rpc_cb
# dc_obj_fetch_task_create -> dc_task_create(dc_obj_fetch_task, tse, ev, task)
#12 0x00007f693ce77e72 in daos_obj_fetch (oh=..., oh@entry=..., th=..., th@entry=..., flags=flags@entry=8, dkey=dkey@entry=0x7f68f3ffdc30, nr=nr@entry=1, iods=<optimized out>, sgls=0x7f68f3ffdc10, maps=0x0, ev=0x0) at src/client/api/object.c:170
#13 0x00007f693c7377b7 in fetch_entry (ver=<optimized out>, oh=..., th=..., th@entry=..., name=<optimized out>, name@entry=0x7f68ec0045c0 "write_test_file", len=len@entry=15, fetch_sym=fetch_sym@entry=false, exists=0x7f68f3ffde47, entry=0x7f68f3ffde60, 
    xnr=0, xnames=0x0, xvals=0x0, xsizes=0x0) at src/client/dfs/dfs.c:658
#14 0x00007f693c73d5a2 in entry_stat (dfs=dfs@entry=0x5608ec71f9e0, th=th@entry=..., oh=..., name=name@entry=0x7f68ec0045c0 "write_test_file", len=len@entry=15, obj=obj@entry=0x7f68ec004590, get_size=<optimized out>, stbuf=<optimized out>, 
    obj_hlc=<optimized out>) at src/client/dfs/dfs.c:1000
#15 0x00007f693c75440e in dfs_osetattr (dfs=0x5608ec71f9e0, obj=0x7f68ec004590, stbuf=stbuf@entry=0x7f68f3ffe2f0, flags=flags@entry=4) at src/client/dfs/dfs.c:5344
#16 0x00005608ea254967 in dfuse_cb_setattr (req=0x7f68ec0042f0, ie=0x7f68ec003830, attr=0x7f68f3ffe2f0, to_set=0) at src/client/dfuse/ops/setattr.c:108
#17 0x00005608ea221ba7 in df_ll_setattr (req=0x7f68ec0042f0, ino=<optimized out>, attr=0x7f68f3ffe2f0, to_set=1056, fi=<optimized out>) at src/client/dfuse/dfuse_fuseops.c:245
#18 0x00007f693c09ef6a in do_setattr () from /lib64/libfuse3.so.3
#19 0x00007f693c0a0dba in fuse_session_process_buf_int () from /lib64/libfuse3.so.3
#20 0x00005608ea22599a in dfuse_do_work (arg=0x5608ec70e470) at src/client/dfuse/dfuse_thread.c:60
#21 0x00007f693d2361ca in start_thread () from /lib64/libpthread.so.0
#22 0x00007f693ba9de73 in clone () from /lib64/libc.so.6




Snappy 是一个压缩/解压库。 它的目的不是最大程度地压缩或与任何其他压缩库兼容； 相反，它旨在实现非常高的速度和合理的压缩。 例如，与 zlib 的最快模式相比，Snappy 对于大多数输入来说要快一个数量级，但生成的压缩文件要大 20% 到 100%。 在 64 位模式的 Core i7 处理器的单核上，Snappy 以大约 250 MB/秒或更多的速度压缩，并以大约 500 MB/秒或更多的速度解压缩。
压缩/解压: 
https://github.com/google/snappy
http://halobates.de/snappy.html

cart主流程:
1. 日志初始化/注册协议和操作码 crt_init_opt
2. 创建上下文（fabric->domain->endpoint->cq...） crt_context_create
3. 创建请求 crt_req_create / crt_corpc_req_create RPC请求集合 传入目的端点 -> ep_rank  DAOS_OBJ_RPC_TGT_UPDATE 回调操作码
4. 发送请求（请求跟踪和取消跟踪） crt_req_send / dss_rpc_send   shard_update_req_cb 发送回调
5. 查看请求进度和回调（progress和trigger） crt_progress 超时或有完成事件产生时函数返回
6. 发送回复 crt_reply_send / crt_reply_get

接收端: 从队列中处理, 执行公共控制器, 执行rpc对应的回调



crt_context_create


engine -> for (;;) 一直循环 -> crt_progress -> crt_hg_progress -> HG_Progress 推进RPC -> HG_Trigger 触发回调

进展/推进/进度
crt_progress
  crt_hg_progress
    do 每轮total = 256
      HG_Progress
        HG_Core_progress
          hg_core_progress
            do while(now < deadline)
              context->poll_set
              hg_core_poll_try_wait
                NA_Poll_try_wait
                    na_poll_try_wait -> na_ofi_poll_try_wait
                       fi_trywait
                            wait_try -> util_wait_fd_try
                                rxm_ep_trywait_eq
                                    fi_trywait
                                        vrb_trywait
                                            vrb_save_wc
                                              insert_tail saved_wc_list
                                fi_poll -> util_poll_run
                                    fi_cq_read

                                    fi_eq_read
                            vrb_save_wc
              hg_core_poll_wait
                hg_poll_wait
                    epoll_wait 等完成队列事件
                hg_core_progress_na HG_CORE_POLL_NA 处理事件
                    NA_Trigger
                        hg_atomic_queue_pop_mc(na_private_context->completion_queue) 有ofi完成事件
                        completion_data.callback
                        hg_core_recv_output_cb | hg_core_send_input_cb -> hg_core_complete_na -> hg_core_complete -> push completion_queue
                    NA_Progress
                        for(;;)
                            ops->progress na_ofi_progress
                            do
                                fi_wait
                                na_ofi_cq_read 读完成队列并处理事件 *****
                                    fi_cq_readfrom
                                        ofi_cq_readfrom
                                            cq->progress(cq) ofi_cq_readfrom
                                            cq->read_entry cirq aux_queue 辅助队列 ofi_cq_init中设置read_entry方法=read_func, util_cq_read_tagged ...
                                    fi_cq_readerr(rxm_ep->msg_cq,...) fi_cq_err_entry
                                      ofi_cq_readerr
                                        ofi_cirque_head 从环形链表上取出1个元素
                                        ofi_cirque_discard
                                na_ofi_cq_process_event 接收端通过tag关联发送端的请求,接收到请求
                                na_ofi_cq_process_retries retry_op_queue 处理重试
                                    fi_tsend
                                    fi_trecv
                                    fi_rma_op
                                    na_ofi_op_cancel
                                    na_ofi_complete

      HG_Trigger
        hg_core_rpc_info->rpc_cb -> common回调(crt_rpc_handler_common)
        hg_cb(&hg_core_cb_info) 转发回调
  crt_context_timeout_check 超时检查
  crt_exec_progress_cb 执行回调
    

crt_context_timeout_check
  while (1)
    d_binheap_root
  crt_exec_timeout_cb
  crt_req_timeout_hdlr
    crt_req_timeout_reset
    switch (rpc_priv->crp_state)
      default
        crt_req_abort(&rpc_priv->crp_pub)
          crt_hg_req_cancel
            HG_Cancel
            
超时取消：
crt_req_abort
    crt_hg_req_cancel
        HG_Cancel
          Let ony one thread
          hg_atomic_or32 HG_CORE_OP_CANCELED 设置已取消标记位
          NA_Cancel
            na_ofi_cancel
              NA_OFI_OP_CANCELING 拦截第一次进去后的请求
              NA_OFI_OP_QUEUED 队列中 则移除队列， 取消入队标记位，设置已取消
                na_ofi_op_cancel 执行取消
                  fi_cancel
                  fi_signal 支持信号通知的不需等待
                    na_cb_completion_add push completion_queue
                    NA_Trigger
                        completion_data.callback(&completion_data.callback_info)
                        hg_core_send_input_cb




HG_Progress -> hg_core_progress -> hg_core_poll_try_wait -> NA_Poll_try_wait -> na_poll_try_wait -> na_ofi_poll_try_wait -> ctx->fi_cq->fid > fi_trywait -> vrb_save_wc(cq, &wc); 保存完成工作请求 -> hg_core_poll_wait -> hg_poll_wait -> epoll_wait(poll_set->fd -> hg_core_progress_na -> NA_Progress 先执行推进 -> hg_thread_cond_timedwait(&na_private_context->progress_cond -> ops->progress -> na_ofi_progress 处理 ->  na_ofi_cq_read 读完成队列 ->  fi_cq_readfrom -> ofi_cq_readfrom -> na_ofi_cq_process_event 处理完成队列事件 -> 
通过tag匹配, 发送671 -> 接收端tag: 10000671
hg_core_recv_input_cb
  hg_core_process_input 处理输入
  hg_core_complete_na 完成 期望计数=实际计数
  crt_swim_srv_cb 执行上层回调 Incoming  rpc注册的控制器 .prf_hdlr = crt_swim_srv_cb
  crt_reply_send -> 发送前需要用(crt_req_create)填充回复数据到(cr_output)
    crt_hg_reply_send
      HG_Respond(rpc_priv->crp_hg_hdl, crt_hg_reply_send_cb, rpc_priv, &rpc_priv->crp_pub.cr_output) -> 回复数据设置到(cr_output)
        HG_Core_respond




epoll_wait
max_events
realloc poll_set->events  x2




na_ofi_context_create
    fi_wait_open
        ofi_wait_fd_open
            wait->util_wait.wait_try = util_wait_fd_try;

crt_req_send -> crt_req_send_internal -> crt_req_ep_lc_lookup -> crp_hg_addr (na_addr) -> 地址作为 HG_Create 的参数 -> crt_req_send_immediately -> crt_hg_req_create RPC_STATE_REQ_SENT 创建请求,设置状态为发送请求 -> crt_hg_req_send -> HG_Forward -> HG_Core_forward -> forward(hg_core_handle) -> hg_core_forward_na -> NA_Msg_send_unexpected -> na_ofi_msg_send_unexpected -> na_ofi_msg_send -> fi_tsend



crt_context_create -> ... ->  HG_Core_init_opt -> hg_core_init -> NA_Initialize_opt -> na_info_parse 解析网络抽象信息 -> na_class_table -> check_protocol 检查协议 -> initialize 初始化 -> na_ofi_initialize 网络抽象初始化 -> na_ofi_check_interface 检查接口(getaddrinfo/getifaddrs/inet_ntop) -> HG_QUEUE_INIT(&priv->addr_pool) 初始化地址池 -> na_ofi_domain_open 打开域 -> na_ofi_getinfo 先查看信息 -> fi_fabric/fi_domain 分配保护域数据 ibv_alloc_pd -> hg_mem_pool_create 创建内存池 -> na_ofi_addr_create 创建地址,设置长度,放入地址池 -> na_ofi_endpoint_resolve_src_addr -> fi_getname -> vrb_dgram_ep_getname -> 


注册rpc prf回调
crt_opc_reg(opc_info, opc, prf->prf_flags, crf, prf->prf_hdlr, prf->prf_co_ops);

创建上下文: 
dss_srv_handler
  ...
  tse_sched_init
  ...
crt_context_create
    crt_context_register_rpc_task(dmi->dmi_ctx, dss_rpc_hdlr /* process_cb */, dss_iv_resp_hdlr /* iv_resp_cb */, dx); 注册rpc处理回调
        cc_rpc_cb = dss_rpc_hdlr 公共RPC回调
        cc_iv_resp_cb = dss_iv_resp_hdlr  iv RPC回调
    crt_context_init
    crt_hg_ctx_init
        crt_hg_class_init
            HG_Init_opt
              HG_Init_opt
                NA_Initialize_opt 网络抽象类初始化 class: ofi Protocal: verbs;ofi_rxm Hostname: mlx5_bond_1/ip:50177
                  na_private_class->na_class.ops = na_class_table[plugin_index] 抽象网络表 na_ofi_class_ops_g
                  na_class.ops->initialize 初始化
                  na_class_ops NA_PLUGIN_OPS(ofi) 插件实现
                  fi_getinfo
                  ofi_check
            crt_hg_get_addr
            crt_hg_reg_rpcid
                crt_hg_reg CRT_HG_RPCID | CRT_HG_ONEWAY_RPCID 单程
                    crt_proc_in_common
                    crt_proc_out_common
                      rpc_priv = container_of(data, struct crt_rpc_priv, crp_pub.cr_output) -> reply 回复数据
                    crt_rpc_handler_common <- hg_proc_info->rpc_cb <- hg_core_rpc_cb <- hg_core_rpc_info->rpc_cb <- hg_core_process
                HG_Registered_disable_response 单程禁用响应
        HG_Context_create
        HG_Context_set_data
        crt_hg_pool_init
    crt_hg_get_addr
    crt_provider_get_ctx_list
    crt_provider_inc_cur_ctx_num
    crt_swim_init


crt_rpc_handler_common
    HG_Get_info
    HG_Context_get_data
    crt_hg_unpack_header
    crt_opc_lookup
    crt_hg_header_copy
    crt_rpc_priv_init
      crp_completed = 0
    crt_rpc_common_hdlr 不是集合rpc
        crt_grp_priv_get_primary_rank
        crt_rpc_cb_customized 自定义回调, 并且非心跳rpc, crt_opc_is_swim, 那么就执行自定义回调
        crt_ctx->cc_rpc_cb(..., crt_handle_rpc) cart的rpc控制器
        cc_rpc_cb = dss_rpc_hdlr -> sched_create_thread(dx, func -> ABT_thread_create -> crt_handle_rpc
        ... 入队，出队
        crt_handle_rpc
            rpc_priv->crp_opc_info->coi_rpc_cb(rpc_pub) 执行回调,如 obj_req_create DAOS_OBJ_RPC_TGT_UPDATE 对应的 ds_obj_tgt_update_handler


更新目标， 落盘
ds_obj_tgt_update_handler
    obj_ioc_begin 访问vos前的各种检查
        obj_ioc_begin_lite 设置 lite IO 上下文，目前仅适用于复合 RPC
        obj_capa_check
        obj_ioc_init_oca
    dtx_handle_resend
    vos_dtx_commit
    dtx_begin
    obj_local_rw
        obj_local_rw_internal
            bio_iod_prep
            obj_bulk_transfer
            bio_iod_post
                dma_rw
                    nvme_rw
                        spdk_blob_io_write  完成回调 rw_completion
                            blob_request_submit_op
                                blob_request_submit_op_single
                                bs_batch_write_dev
                                    blob_bdev->bs_dev.write
                                    bdev_blob_write
                                    spdk_bdev_write_blocks
                        spdk_blob_io_read



crt_hg_progress
    HG_Trigger
        hg_core_trigger
            hg_atomic_queue_pop_mc(context->completion_queue)
            hg_core_trigger_entry
                hg_core_process




crt_req_send(req, shard_update_req_cb, remote_arg)  complete_cb = shard_update_req_cb
    crt_context_req_track
        crt_set_timeout
        epi_req_waitq 如果拥塞就进等待队列
        crt_req_timeout_track
        epi_req_q 否则进飞行队列
    crt_req_send_internal
        crt_req_ep_lc_lookup
            crt_req_send_immediately
                crt_hg_req_create
                    HG_Create
                      hg_core_create
                        hg_core_alloc_na 分配操作id
                    HG_Reset 重用 rpc_priv->crp_hdl_reuse
                crt_hg_req_send
                    HG_Forward(hdl, crt_hg_req_send_cb,...) 回调机制:forward_cb -> request_callback -> hg_cb(由HG_Trigger执行回调)
                        HG_Core_get_rpc_data
                        hg_set_struct
                            proc_cb in_proc_cb|out_proc_cb  crt_proc_common_hdr 被 HG_Register 注册
                        HG_Core_forward(...,hg_core_forward_cb,...)
                            forward(hg_core_handle) hg_core_forward_na|hg_core_forward_self
                                hg_core_forward_na
                                    hg_core_gen_request_tag
                                    NA_Msg_recv_expected hg_core_recv_output_cb NA_OFI_OP_RESET
                                    NA_Msg_send_unexpected hg_core_send_input_cb
                                        na_ofi_msg_send_unexpected
                                            na_ofi_msg_send
                                                fi_rx_addr index, bits, 偏移和位运算计算出地址
                                                fi_tsend -> rxm_ep_tsend
                                                    ssize_t rxm_get_conn 新建或复用连接
                                                    rxm_send_common
                                                na_ofi_op_retry -> na_ofi_cq_process_retries HG_QUEUE_POP_HEAD







# cart 流程

## 简介
CaRT 是用于大数据和 百万兆级 HPC 的开源 RPC 传输层。 它支持传统的 P2P RPC 发送和集体 RPC，后者在一组目标服务器上调用 RPC，并具有可扩展的基于树的消息传播。
在项目中，主要提供RPC和BULK(大块)数据传输， swim协议（集群心跳检测）， rank组员管理， IV接口等功能


## 术语
cart： 支持集合和单个RPC请求的传输模块
iv： incast variable， IV变量


## 主流程

1. 日志初始化/注册协议和操作码 crt_init_opt(带选项初始化)， 在 engine  -> init.c -> server_init 中初始化网络层
2. 创建上下文 （fabric->domain->endpoint->cq...） crt_context_create， 在engine -> srv.c -> dss_srv_handler 中创建单个cart上下文(dss_xstreams_init 中指定个数)
3. 创建请求 crt_req_create / crt_corpc_req_create RPC请求集合 传入目的端点 -> ep_rank
4. 发送请求（请求跟踪和取消跟踪） crt_req_send / dss_rpc_send
5. 查看请求进度和回调（progress和trigger） crt_progress 超时或有完成事件产生时函数返回
6. 发送回复 crt_reply_send / crt_reply_get


带选项初始化：主要完成日志初始化， 分配超时回调， 事件控制器， 注册RPC协议和操作码
crt_init_opt daos_crt_init_opt_get
    d_log_init
    crt_setup_log_fac 设置日志基础设施
      注册日志设施
    d_fault_inject_init 故障注入
    data_init
        dump_envariables 打印环境变量
        start_rpcid 随机生成起始rpcid
        d_tm_add_metric 添加指标等
    prov_data_init 设置提供者属性，初始化上下文双向链表
      D_INIT_LIST_HEAD cpg_crt_list 上下文列表
    crt_hg_init 设置hg日志流， 可强制设置为debug
        hg_log_set_func(crt_hg_log) 设置日志方法 
        HG_Set_log_level 日志级别和模块hg,na
          hg_log_name_to_level 字符串 宏 enum枚举
            X(a, b, c) b, HG_LOG_LEVELS
    crt_grp_init 初始化组
        crt_primary_grp_init
            crt_grp_priv_create
            grp_priv_init_membs
            crt_grp_lc_create
                d_hash_table_create_inplace gp_lookup_cache
                d_hash_table_create_inplace gp_uri_lookup_cache
    crt_plugin_init
        cpg_timeout_cbs 分配超时回调
        cpg_event_cbs 事件控制器回调
    crt_self_test_init
    crt_opc_map_create
        crt_opc_map_L2_create 初始化前16个entry， L2分配32个entry
    crt_internal_rpc_register
        crt_proto_register_internal(&cpf)
            crt_proto_reg_L1
                crt_proto_reg_L2
                    crt_proto_reg_L3
        crt_proto_register(&cpf)


HG_Set_log_level
  hg_log_get_subsys
    hg_log_subsys_g
  hg_log_set_level(log_level)
    hg_log_outlet_update_all()
      HG_QUEUE_FOREACH 
  hg_log_set_subsys(new_subsys_ptr) set hg
    hg_log_outlet_reset_all
    enable
    hg_Log_outlet_update_all



队列：mercury_queue.h
HG_QUEUE_HEAD
HG_QUEUE_ENTRY(hg_log_outlet) entry 



## QA



目标对象更新
obj_tgt_update -> ds_obj_remote_update -> DAOS_OBJ_RPC_TGT_UPDATE -> crt_req_send(req, shard_update_req_cb, remote_arg) 发送RPC -> 




obj_req_create DAOS_OBJ_RPC_TGT_UPDATE 获取模块的上下文:dss_get_module_info()->dmi_ctx
    crt_req_create
        crt_req_create_internal
            crt_rpc_priv_alloc
                crt_opc_lookup 查找操作码
            crt_rpc_priv_init rpc初始化
                rpc_priv->crp_completed = 0 完成状态初始化为0
                crt_common_hdr_init 服务端
                crt_rpc_inout_buff_init 转发请求复用父RPC的输入缓冲区 输入/输出缓冲区初始化

                  





参考：
DAOS 使用 IV（incast 变量）在单个 IV 命名空间下的服务器之间共享值和状态，该命名空间被组织为树。 树根称为 IV 领导者，服务器可以是叶子或非叶子。 每个服务器维护自己的 IV 缓存。 在 fetch 期间，如果本地缓存不能满足请求，它将请求转发给它的父节点，直到到达根节点（IV leader）。 至于更新，它首先更新它的本地缓存，然后转发给它的父节点直到它到达根节点，然后根节点将更改传播到所有其他服务器。 IV 命名空间是每个池的，它在池连接期间创建，并在池断开连接期间销毁。 要使用 IV，每个用户都需要在 IV 命名空间下注册自己以获得一个标识，然后它将使用这个 ID 来获取或更新自己在 IV 命名空间下的 IV 值。








dss_rpc_hdlr -> sched_req_enqueue 调度请求入队 -> should_enqueue_req?(dx->dx_main_xs) 主线程才入队(如VOS) ->  req_get(对端 req_put) -> req_enqueue 请求入队 -> d_list_add_tail(&req->sr_link, &sri->sri_req_list) -> d_list_for_each_entry_safe(req, tmp, list, sr_link) 遍历队列另一端(process_req_list)

process_all -> policy_ops[sched_policy].process_io(dx) -> .process_io = policy_fifo_process -> process_req_list -> process_req -> req_kickoff(dx, req) 开始处理请求 -> crt_handle_rpc

rpc公共回调
dss_rpc_hdlr(...,crt_handle_rpc...)
  opc_get_mod_id(rpc->cr_opc) 获取模块id 偏移+掩码 daos_modeul_id
  SCHED_REQ_ANONYM 匿名 sra=调度请求属性
  struct dss_module	*module = dss_module_get(mod_id);
  module->sm_mod_ops->dms_get_req_attr 获取属性
  sched_req_enqueue 入队  real_rpc_hdlr = crt_handle_rpc req->sr_func
    should_enqueue_req SCHED_REQ_ANONYM 匿名不入队列
    req_enqueue
      d_list_add_tail(&req->sr_link, &sri->sri_req_list)





dss_sched_init
  sched_run
    sched_start_cycle(data, pools)
      process_all
        policy_ops[sched_policy].process_io(dx) 处理io
          policy_fifo_process 先进先出
            process_req_list
              req_kickoff
                req_kickoff_internal(dx, &req->sr_attr, req->sr_func,req->sr_arg)  sr_func -> crt_handle_rpc 不入队列，直接处理回调
                  sched_create_thread(dx, func func -> crt_handle_rpc
                    ABT_thread_create(abt_pool, func, arg, t_attr, thread)
                      crt_handle_rpc



常见操作码 注册操作码
.cpf_name   =  "pool"
src/cart/crt_register.c:488 crt_proto_register_common()
internal  0xff000000
management, 0x1000000
pool, 0x2000000
cont, 0x3000000
daos-object, 0x4000000
swim, 0xfe000000
...

打开调试
HG_LOG_LEVEL=debug
FI_LOG_LEVEL=debug

D_LOG_MASK=DEBUG
DD_SUBSYS=all  |hg,external,mercury,rpc,corpc
DD_MASK=all

重要结构：
struct crt_proto_rpc_format 协议rpc格式
prf_flags = 是否排队（队列头） CRT_RPC_FEAT_QUEUE_FRONT coi_queue_front
prf_hdlr = 回调控制器
prf_req_fmt = 输入/输出格式


rdma write, 单边写, 
NA_Put
na_ofi_put
na_ofi_rma fi_writemsg = fi_rma_op   .writemsg = 
.writemsg = vrb_msg_ep_rma_writemsg
vrb_msg_ep_rma_writemsg -> prov/verbs/src/verbs_rma.c
  struct ibv_send_wr wr
  wr.opcode = IBV_WR_RDMA_WRITE | wr.opcode = IBV_WR_RDMA_WRITE_WITH_IMM 写|立即数写
  vrb_send_iov(ep, &wr, msg->msg_iov, msg->desc -> ssize_t vrb_send_iov
    wr->sg_list = alloca(sizeof(*wr->sg_list) * count)
    wr->sg_list[i].addr = (uintptr_t) iov[i].iov_base
    wr->sg_list[i].length = iov[i].iov_len;
    wr->send_flags = IBV_SEND_INLINE ?IBV_SEND_FENCE
    wr->sg_list[0]
    wr->num_sge =
    vrb_post_send(ep, wr, flags) -> prov/verbs/src/verbs_ep.c -> ssize_t vrb_post_send
      ibv_post_send(ep->ibv_qp, wr, &bad_wr)




vrb_poll_cq
    ibv_poll_cq


dfuse_core_dump:

HG_Forward
    crt_proc_coon_hdr
        crt_proc_memcpy


crp_req_hdr

vrb_eq_cm


ds_pool_list_cont_handler


bulk rdma收发文件

server:
./server addr
main
  HG_Init
  HG_Addr_self
  HG_Addr_to_string
  HG_Context_create
  HG_Register_data
  do
    HG_Trigger
    HG_Progress
  save
    HG_Bulk_transfer origin and local 对齐
  HG_Context_destroy
  HG_Finalize

client
./client protocal server_addr filename
main
  access(save_op.filename, F_OK) 检查文件是否存在
  lookup_callback  
    save_operation* save_op = (save_operation*)(callback_info->arg); 回调通过指针传递参数
    FILE* file = fopen(save_op->filename,"r");
    fseek(file, 0L, SEEK_END)
    save_op->size = ftell(file) 计算文件位置
    fseek(file, 0L, SEEK_SET);
    save_op->buffer = calloc(1, save_op->size);
    fread(save_op->buffer,1,save_op->size,file)  // fread和fwrite的用法详解（以数据块的形式读写文件）
    HG_Bulk_create (void**) &(save_op->buffer) 传入文件内容buf指针地址（二级指针）
      hg_buld malloc/memset 0
      hg_atomic_init32 ref_count 1 初始化引用计数
      转分段 注册分段 hg_bulk_register_segments 最大静态段=8， HG_BULK_STATIC_MAX
      hg_bulk_create_na_mem_descs 单独注册内存描述 individually 分为静态staic和动态数组dynamic
        hg_bulk_register 注册段 传递索引i
          NA_Mem_handle_create
            mem_handler_create -> na_ofi_mem_handler_create
              NA_UNUSED 编译参数 忽略编译告警
              calloc
              na_ofi_mem_desc s[0] 单段， d 多段
              flags & 0xff 全1
          NA_Mem_register
            mem_register na_ofi_mem_register
              requested_key 自增
              NA_MEM_READ_ONLY 
              access = FI_REMOTE_READ | FI_WRITE 远程读和本地写
              fi_mr_regv 注册内存向量
                rxm_mr_regv
                  rxm_mr_get_msg_access
                  fi_mr_regv
                    vrb_mr_regv
                      vrb_mr_cache_reg
                        ofi_mr_rbt_find 如果有缓存就找红黑树
                        util_mr_cache_create
                        add_region
                          vrb_mr_reg_common
                            ibv_reg_mr 注册内存
                            md->mr_fid.key = md->mr->rkey 远程键
                            md->lkey = md->mr->lkey 本地键
                            vrb_eq_write_event 写完成事件
                  rxm_mr_init
              fi_mr_key 取回内存key
              ...desc.info.fi_mr_key = 远程键
          NA_Mem_handle_get_serialize_size na_ofi_mem_handle_get_serialize_size
            sizeof iovec info + iovcnt
    HG_Forward
    save_completed

example_snappy_server
main
  HG_Version_get
  fopen
  fpritnf
  fclose

client
main
  fgets(target_addr_string, PATH_MAX, na_config)
  strrchr char *strrchr(const char *str, int c) 在参数 str 所指向的字符串中搜索最后一次出现字符 c（一个无符号字符）的位置





fi_cq_init
    slist_init(&cq->aux_queue) 辅助队列

util/cq：将 oflow_err_list 重命名为 aux_queue auxiliary queue
溢出/错误列表是引用的辅助队列由主要CQ。 用于记录错误完成以及当主 CQ 已满时的成功完成。这两种情况都是意想不到的，但可能会发生，例如，当冲洗张贴收到。 这经常发生在清理期间使用 RxM。重命名是澄清和修复的更大努力的一部分如何使用主CQ和辅助队列。


fi_cancel
  rxm_ep_cancel
    rxm_passthru_info 直通 FI_PROTO_RXM_TCP
    fi_cancel
    rxm_ep_cancel_recv
      ofi_ep_lock_acquire
      dlist_remove_first_match
      err_entry.err = FI_ECANCELED
      ofi_cq_write_error
        ofi_cq_insert_error(cq, err_entry)
            ofi_cq_insert_aux(cq, entry) 插入元素到辅助队列尾部 cq满后，往辅助队列插入元素
                ofi_cirque_tail(cq->cirq)
                slist_insert_tail(&entry->list_entry, &cq->aux_queue) 在fi_cq_readfrom中读取 cq->read_entry cirq aux_queue 关键队列(辅助队列)
        cq->wait->signal(cq->wait) fd

HG_Cancel
    NA_OFI_OP_QUEUED 队列中，则移除 HG_QUEUE_REMOVE



swim:
crt_req_send crt_swim_cli_cb

HG_Context_create
    NA_Context_create_id
    NA_Poll_get_fd
        na_ofi_pool_get_fd na_pool_get_fd 从网络插件获取方法
            fi_control FI_GETWAIT fi_cq->fd
                ofi_cq_control
                    memcpy pollfds->fds

        HG_POLLIN 服务端监听可读事件
        hg_poll_create
            hg_poll_set->fd = epoll_create1  轮询集
        hg_poll_add(context->poll_set, na_poll_fd, &event)  关键fd = na_poll_fd 拿到cq对应的fd，加入到epoll里面
            struct epoll_event ev
            epoll_ctl(poll_set->fd,...)



na_ofi_context

rxm_cq_open
    ofi_cq_init ofi_cq_readfrom
    rxm_ep_progress
        fi_cq_read
        rxm_handle_comp_error 错误处理 rxm_err

    
hg_log_write
  buf 256B
  hg_time_get_current(&tv) 获取当前时间
  vsnprintf
  hg_log_func_g fprintf 
  ERROR && debug_Log && level >= MIN_DEBUG 日志级别为ERR
    hg_dlog_dump
    hg_dlog_resetlog





fi_trecv 往自己的接收队列中防止WQE，无需建立连接
    rxm_ep_trecv_comm

reboot -f eio比超时来的慢， 延迟超过30s
cancel status:4 posted

1. 取消的时候，send没有调回掉
2. reboot -f, 原来的连接发不通




na_ofi_cancel


client io 跌零原因：
1. io超时


hg,ofi日志原理：

dump_envariables()

autotest

hg_log_set_subsys_level
    subsys:hg



setlinebuf

环境变量:
env_vars
setupGrpc
  getGrpcOpts
  grpc.NewServer(srvOpts...)
  ctlpb.RegisterCtlSvcServer(srv.grpcServer, srv.ctlSvc)
  getSrxSetting
    EnvVars 配置文件: type Config struct
    engineVals
  mgmtpb.RegisterMgmtSvcServer(srv.grpcServer, srv.mgmtSvc)
  srv.sysdb.ConfigureTransport(srv.grpcServer, tSec)

  

updateFabricEnvars
  deviceAlias, err := netdetect.GetDeviceAlias(ctx, cfg.Fabric.Interface)
  "OFI_DOMAIN=" + deviceAlias
  cfg.WithEnvVars(envVar)
    mergeEnvVars(c.EnvVars, newVars)

setDaosHelperEnvs(cfg, os.Setenv)

启动服务器
server_start: /usr/bin/daos_server start
func main()
  parseOpts(os.Args[1:], &opts, log) 解析参数
    cfgCmd.loadConfig(opts.ConfigPath) 加载配置
    cmd.Execute(cmdArgs)


src/control/cmd/daos_server/start.go
func (cmd *startCmd) Execute(args []string)
  func Start(log *logging.LeveledLogger, cfg *config.Server)
    processConfig
    newServer(ctx, log, cfg, faultDomain)
    createServices -> srv.createServices(ctx) -> srv.sysdb, err = newManagementDatabase
      CreateDatabaseConfig(cfg) -> raftDirName := "control_raft"
    initNetwork 初始化网络
      func netInit
        netdetect.Init(ctx)
          initLib  
            version := C.hwloc_get_api_version()  hwloc.h 查看cpu拓扑: https://blog.csdn.net/whatday/article/details/106170613
            https://www.open-mpi.org/projects/hwloc/doc/
            topology, err := hpa.topology_init()
              hwloc_topology_init
          numNUMANodes
          getCoreCount
          initDeviceScan
        netdetect.NumNumaNodes(ctx)
        cfg.CheckFabric(ctx)
          GetDeviceClassFn  func GetDeviceClass  cat /sys/class/net/enp2s0/type
        updateFabricEnvars
    initStorage 初始化存储
      prepBdevStorage iommuDetected iommu内存管理 -> 仅当使用非模拟 NVMe 并且用户无特权时才执行这些检查, 比如非root用户
      NvmePrepare
        PrepareBdevs
          func (p *Provider) Prepare
            BdevPrepare
              func (h *bdevPrepHandler) Handle
                h.bdevProvider.Prepare(pReq)
                  func (s *spdkSetupScript) Prepare  spdkSetupPath      = "../share/daos/control/setup_spdk.sh"
                  s.runCmd(s.log, env, s.scriptPath)
      大页检查 hugePages, err := hpiGetter()  func GetHugePageInfo()  cat /proc/meminfo | grep -e "^Huge"
      srv.ctlSvc.Setup()
        func (n *NvmMgmt) Discover()
        C.nvm_get_number_of_devices https://github.com/intel/ipmctl ipmctl is a utility for configuring and managing Intel® Optane™ Persistent Memory modules (PMem).
        C.nvm_get_devices
        func (cr *cmdRunner) GetPmemNamespaces()
          ndctl list -N -v
        scs.NvmeScan
        scs.storage.SetBdevCache(*nvmeScanResp)
    addEngines
      registerTelemetryCallbacks
      NvmeScan
      createEngine
      SetBdevCache
      AddInstance
    setupGrpc
    registerEvents
    srv.start(ctx, shutdown)
      func (h *EngineHarness) Start
        for _, ei := range h.Instances()
          ei.Run

func (ei *EngineInstance) Run
  ei.format
    awaitStorageReady
    createSuperblock
  ei.start
  func (r *Runner) Start
    r.Config.CmdLineArgs
    r.Config.CmdLineEnv
    return r.run(ctx, args, env, errOut)
    func (r *Runner) run
      binPath, err := common.FindBinary(engineBin) daos_engine
      cmd := exec.Command(binPath, args...)
      cmd.Env = env
      cmd.Start() 启动engine


processConfig
  cfg.Validate(log)
  checkFabricInterface(ec.Fabric.Interface, lookupNetIF)
    net.InterfaceByName(name)
  SaveActiveConfig
    cfg.SaveToFile(activeConfig)
  setDaosHelperEnvs
     setenv(pbin.DaosAdminLogFileEnvVar, cfg.HelperLogFile) 特权帮助日志 DAOS_ADMIN_LOG_FILE DAOS_FIRMWARE_LOG_FILE
  getFaultDomain
    getDefaultFaultDomain
      system.NewFaultDomainFromString(domainStr) /rack0/pdu1/hostname PDU（Power Distribution Unit） 电源分配单元


type server struct
newServer(ctx, log, cfg, faultDomain)
  harness := NewEngineHarness(log).WithFaultDomain(faultDomain) type EngineHarness struct 引擎套子



srv.createServices(ctx) scaffolding 创建rpc，事件服务等脚手架
  dbReplicas, err := cfgGetReplicas 访问点
  sysdb, err := system.NewDatabase  control_raft
    RaftDir:    cfgGetRaftDir(srv.cfg)
    db.setReplica(repAddr)
  srv.membership = system.NewMembership
  cliCfg := control.DefaultConfig localhost:10001
  rpcClient := control.NewClient
  srv.pubSub = events.NewPubSub(ctx, srv.log) 事件分发原语
    go ps.eventLoop(ctx)
      for
        select
          ps.handlers[newSub.topic]
          ps.publish(ctx, event)
          ps.updateFilter(fu)
  srv.OnShutdown(srv.pubSub.Close)
  srv.evtForwarder = control.NewEventForwarder(rpcClient, srv.cfg.AccessPoints)
  srv.evtLogger = control.NewEventLogger(srv.log)
    newSyslogger(sev.SyslogPriority(), flags)
  srv.ctlSvc = NewControlService(srv.log, srv.harness, srv.cfg, srv.pubSub)
    NewStorageControlService
      newStorageControlService DefaultProvider
  srv.mgmtSvc = newMgmtSvc(srv.harness, srv.membership, sysdb, rpcClient, srv.pubSub)

os.Setenv


srx: 共享接收上下文

NewCommandLineErrorLogger

ofi错误级别日志: ERROR: daos_engine:0 libfabric
cmd.Stderr = &cmdLogger{
  logFn:  r.log.Error,
  prefix: fmt.Sprintf("%s:%d", engineBin, r.Config.Index),
}


ofi动态开日志设计思路：
1. ofi暴露修改日志级别的接口 fi_set_log_level
2. hg暴露
3. 

op->cancel


cgo
daos
ValidateProviderConfig
  fi_getinfo



动态开日志：
d_logfac_is_enabled
d_log_setmasks

cart_ctl
  crt_fill_set_log
  


self_test -u --group-name daos_server --endpoint 1:2 --message-size "(0 0)" --max-inflight-rpcs 16 --repetitions 1000

cross_server
self_test -u --group-name daos_server --endpoint 0-2:2 --message-size "(0 0) (b1048578 0) (0 b1048578)" --max-inflight-rpcs 16 --repetitions 100
self_test -u --group-name daos_server --endpoint 0-2:2   --master-endpoint 0:2   --message-sizes "b1048576"   --max-inflight-rpcs 16 --repetitions 100
self_test -u --group-name daos_server --endpoint 0-2:2    --message-sizes "b1048576"   --max-inflight-rpcs 16 --repetitions 100

b1048576     1Mb bulk transfer Get and Put
b1048576 0   1Mb bulk transfer Get only
0 b1048576   1Mb bulk transfer Put only
I2048        2Kb iovec Input and Output
i2048 0      2Kb iovec Input only
0 i2048      2Kb iovec Output only

self_test.c
cart_self_test
main
  d_log_init
  parse_message_sizes_string
  run_self_test
    self_test_init
    crt_group_rank
    crt_group_lookup
    qsort st_compare_endpts
    D_ALLOC_ARRAY(latencies, num_ms_endpts); 一级指针,二级指针均可分配
    for master endpoint
      d_iov_set 设置io向量
    crt_bulk_create CRT_BULK_RW 创建大块数据handler
    randomize_endpts 是否随机化endpoint
    test_msg_size
      crt_req_create CRT_OPC_SELF_TEST_START
      crt_req_send start_test_cb
      sched_yield()确保当前进程在资源竞争严重时，给其他进程执行机会来提高性能
      poll master node 轮训主节点
      crt_req_create CRT_OPC_SELF_TEST_STATUS_REQ 查看进度
      crt_req_send  status_req_cb
      print_results 打印结果


输入: sizes_ptr:b1048576, tuple_tokens:(),
输出token数量
两层遍历

对端收到rpc,执行CRT_OPC_SELF_TEST_START对应的回调
crt_self_test_start_handler
  (args->endpts.iov_buf_len & 0x7) != 0) 不是8的倍数
  D_ALLOC_PTR(g_data) 分配全局数据
  d_iov_set
  crt_bulk_create CRT_BULK_RO
  memset(cb_args->buf, 0xC5, alloc_buf_len); 填充随意的数据
  d_iov_set crt_st_get_aligned_ptr 对齐
  crt_bulk_create
  open_sessions(); 打开会话
    ST_SET_NUM_INFLIGHT(g_data->num_endpts);
    for (i = 0; i < g_data->num_endpts; i++) 遍历端点 
      crt_req_create CRT_OPC_SELF_TEST_OPEN_SESSION  crt_self_test_open_session_handler
      crt_req_send open_session_cb


crt_self_test_open_session_handler
  alloc_buf_entry
  found_session = find_session(session_id, NULL);
  addref_session(new_session);
  crt_reply_send(rpc_req)



open_session_cb


stop server then start
rxm_handler_error fi_eq_readerr 111 connection refused


TAILQ_INIT(&targets)

环境变量：
en_variables.md





objectCmd
poolCmd

fsCmd
containerCmd

daos
src/control/cmd/daos/main.go
  main()
    parseOpts
      p.ParseArgs(args)
        CommandHandler 命令控制器， 执行命令
          poolSetAttrCmd
            Execute 执行命令

func (cmd *poolSetAttrCmd) Execute
  resolveAndConnect
    cmd.connectPool
       C.daos_pool_connect2
        dc_task_create(dc_pool_connect, NULL, ev, &task)
        return dc_task_schedule(task, true);
  setDaosAttribute 设置池属性
    daos_pool_set_attr
      dc_task_create(dc_pool_set_attr, NULL, ev, &task)
        dc_pool_set_attr
          attr_check_input
          pool_req_prepare POOL_ATTR_SET ds_pool_attr_set_handler
            rsvc_client_choose 
            pool_req_create
          attr_bulk_create
          tse_task_register_comp_cb(task, pool_req_complete
          daos_rpc_send(cb_args.pra_rpc, task)
            crt_req_send(rpc, daos_rpc_cb, task)

          


dc_pool_connect
  pool = dc_task_get_priv(task)
  init_pool
  dc_pool_connect_internal


ds_pool_attr_set_handler
  pool_svc_lookup_leader
  rdb_tx_begin
  ds_rsvc_set_attr
    crt_bulk_get_len
    d_iov_set
    crt_bulk_create
    attr_bulk_transfer CRT_BULK_GET
      crt_bulk_transfer bulk_cb -> ABT_eventual_set 唤醒等待的协程 -> ABT_eventual_wait
    rdb_tx_update
  rdb_tx_commit
  ds_rsvc_set_hint
  pool_svc_put_leader
  crt_reply_send


2007 update pool



客户端
dfuse
dfuse_get_fuse_ops 函数操作表


debug log summary


0x38 RPC_STATE_REQ_SENT


NA_LOG_DEBUG
  HG_LOG_WRITE_DEBUG
    HG_LOG_OUTLET level
    hg_dlog_addlog 添加日志记录到dlog
    hg_log_write
      hg_dlog_dump 导出日志文件到指定的流
      hg_dlog_resetlog 


客户端分片
cli_shard.c
dc_obj_shard_query_key
  tse_task_register_comp_cb(task, obj_shard_query_key_cb, &cb_args, sizeof(cb_args))
  obj_shard_query_key_cb 2007 pool老旧需要更新stale 由 dtc_cb 触发执行 D_FOREACH_DAOS_ERR  D_FOREACH_DAOS_ERR DER_STALE 陈旧错误




 

obj_shard_open




reclaim completed pool 2007


na_ofi_complete
  na_cb_completion_add
    push completion_queue
      HG_Progress ... NA_Trigger



已经完成，可能是由于重复完成 already completed, possibly due to duplicated completions
疑点
1.设置REPLY_RECVED后又会恢复, timeout中的0x39如果设置上的? 超时先进来,读到0x39, 
2.untrack失败?
3.同一个rpc, 并行没锁住?


crt_hg_req_send_cb
  crp_complete_cb -> complete_cb
  调用 crt_req_send 传入的回调，如： shard_update_req_cb
  crt_context_req_untrack 取消rpc超时跟踪(注意超时后也可能会执行此动作)



mgmt_svc_map_dist_cb
  ntf_crtctx_update


分配操作id
hg_core_alloc_na
  NA_Op_create

释放
NA_OFI_OP_RELEASE


hg_client



hg_server




tse_task_reint_with_dely


客户端coredump, 安全遍历链表出现段错误, d_list_for_each_entry_safe signal 11 Segmentation fault, 客户端读文件
dfuse_do_work
  dfuse_do_work
    dfuse_cb_read dfuse_cb_read
    dfs_read
    dfs_read_int dfs_read_int
      dc_task_create(dc_array_read, NULL, ev, &task)
      tse_task_register_cbs(task, NULL, NULL, 0, read_cb, NULL, 0) -> 支持前置cb和后置cb
      dc_task_schedule(task, true)
        tse_task_schedule
          tse_task_schedule_with_delay
    dc_array_read DAOS_OPC_ARRAY_READ -> dc_array_io   task1
      daos_task_create(DAOS_OPC_ARRAY_GET_SIZE, tse_task2sched(task)
        dc_task_create(dc_funcs[opc].task_func, sched, NULL, &task)
        dc_task_depend(task, num_deps, dep_tasks)
      
      ...
    tse_sched_progress  task2
    tse_sched_run -> while (1) -> 循环处理
      processed += tse_sched_process_init(dsp) -> 处理调度程序的初始化列表和休眠列表。 这首先将所有现在应该醒来的任务从睡眠列表移动到初始化列表的尾部，然后执行调度程序初始化列表中没有依赖项的所有任务的所有主体函数, 调度器主函数, 在队列间移动任务, 从头部开始处理任务
      processed += tse_sched_process_complete(dsp)
        d_list_splice_init(&dsp->dsp_complete_list, &comp_list) -> 连接两个列表并重新初始化空列表。 列表的内容添加到头部的开头。 返回时列表为初始化为空(可理解为从一个队列移动到新的队列)
        d_list_for_each_entry_safe(dtp, tmp, &comp_list, dtp_list) -> 遍历新的完成队列
        d_list_del_init(&dtp->dtp_list)
        tse_task_post_process(task) -> 检查完成列表中的任务、依赖任务状态检查、调度状态更新等。此后任务将移至完成列表
          while (!d_list_empty(&dtp->dtp_dep_list))  -> 依赖任务上的任务依赖队列不为空(说明主任务还有依赖任务)
            dtp_tmp->dtp_dep_cnt-- -> 将主任务依赖-1

            ...
            bumped = true -> 
          dsp->dsp_inflight-- 调度器中的飞行计数器减1
        tse_sched_priv_decref(dsp) -> 调度器引用-1
        tse_task_decref(task) -> 任务引用-1 -> 或释放task -> zombie -> D_FREE(task)
      completed = tse_sched_check_complete(sched) -> 检查调度器是否是完成状态, 即: 初始队列和睡眠队列都为空, 且无飞行的任务
        completed = (d_list_empty(&dsp->dsp_init_list) &&
		     d_list_empty(&dsp->dsp_sleeping_list) &&
		     dsp->dsp_inflight == 0); -> 完成条件: 调度器初始队列和睡眠队列为空,且调度器上的飞行任务个数为0
    tse_task_prep_callback
    check_short_read_cb
    tse_task_complete
    check_short_read_cb
    tse_task_complete
    tse_sched_process_complete
    tse_task_post_process
    tse_task_complete_callback
    task_comp_event
    daos_event_complete(struct daos_event *ev, int rc) 事件完成, 移动到完成队列
      daos_event_complete_locked
        daos_evx2ev(evx)  字段间的转换 container_of | (struct daos_event_private *)&ev->ev_private = daos_ev2evx
        daos_event_complete_cb 事件完成回调
          d_list_for_each_entry_safe 遍历给定类型的列表以防止删除列表条目
            d_list_del_init(&ecl->op_comp_list)
            err = ecl->op_comp_cb(ecl->op_comp_arg, daos_evx2ev(evx), rc) 执行回调 -> cb -> 





dsc_progress_start
  dss_ult_create(dsc_progress, dx, DSS_XS_SELF, 0, DSS_DEEP_STACK_SZ, NULL)
  dsc_progress
    tse_sched_progress



条件回调, 进度回调
eq_progress_cb
  tse_sched_progress
  d_list_for_each_entry_safe



dc_obj_query_key_task_create
  dc_task_create dc_obj_query_key -> dtp_func

查找
df_ll_lookup
cb_lookup
lookupx
lookup_rel_int
array_get_size
  dc_task_create(dc_array_get_size, NULL, ev, &task)
  dc_task_schedule
    daos_event_priv_wait 如果关联事件是私有事件，这个函数会等到完成，否则它立即返回
      ev_progress_cb
        tse_sched_progress
          dtp_func -> dc_obj_query_key

踩内存
git show 4002fb82e25ef26e38750ffb884e7416842356e8
daos_event_priv_wait -> crt_progress_cond(evx->evx_ctx, 0, ev_progress_cb, &epa) -> ev_progress_cb -> 
daos_event_init -> D_MUTEX_INIT


pool_map_update
故障流程 心跳 swim swim_progress
c -> s
kill s
swim ping timeout
swim suspect
swim suspect timeout
swim confirm dead swim_member_dead
notify dead swim_updates_notify -> crt_swim_notify_rank_state
group_map_update to leader
group_map update bcast
making Excluded

疑问:
1. confirm dead 只需要14s, 但是从 notify 到 excluded 却要70s?
2. rank重新加入, io为何跌零?
3. Invalid Handle 问题跟踪? 社区?


无效控制器 invalid handle 1002
ds_pool_start
  dss_ult_create pool_fetch_hdls_ult
  pool_fetch_hdls_ult
    ds_pool_iv_conn_hdl_fetch


DTX_ABORT
dtx_handler
  case DTX_ABORT
    vos_dtx_abort
    dout->do_status = rc
    rc = crt_reply_send(rpc)


dtx_refresh_internal | dtx_rpc_internal
dtx_req_list_send
  ABT_future_create(len, dtx_req_list_cb, &future)
    abort
      drr_result -1002

1. drr->drr_result = -1002
2. dra->dra_result = -1002

dtx_req_list_cb invalid handle 1002


dtx中断, 中断dtx
dtx_abort
  dtx_rpc_prep DTX_ABORT
    dha->dha_opc = opc = DTX_ABORT
    dss_ult_create(dtx_rpc_helper
    dtx_rpc_internal
      dtx_req_list_send
        ABT_future_create(len, dtx_req_list_cb, &future) 创建未来, 在 dtx_handler 中执行 dtx_req_list_cb 回调
        dra->dra_future = future
        dtx_req_send
        ABT_thread_yield -> 如果计算尚未完成，则交给调度程序, 临时让出cpu
  vos_dtx_abort
  dtx_rpc_post




tse子任务
obj_shard_task_sched
  tsa_sched == false
  tse_task_complete 父子任务调度


查询缓存的池映射
create_map_refresh_rpc
  POOL_TGT_QUERY_MAP
  ds_pool_tgt_query_map_handler



obj_ioc_init


umount -t fuse.daos /home/cont_ssd
dfuse_launch_fuse 阻塞,直到完成
  fuse_session_unmount
停止文件系统
dfuse_fs_stop
  ino_flush



vos_obj_iter_fini
  daos_handle_is_inval 无效控制器

evt_iter_delete


vos_obj_iter_prep
iter_decref
vos_iter_finish

dfuse异常退出, agent触发, system stop
libabt.so
make_fcontext
  ABTD_ythread_func_wrapper
    crt_handle_rpc <- crt_rpc_common_hdlr
      ds_pool_evict_handler POOL_EVICT 驱逐池
        pool_disconnect_hdls
          ds_cont_close_by_pool_hdls
            rdb_lc_interate
              rdb_vos_iterate
                vos_iter_prepare
                  vos_obj_iter_fini


ds_pool_svc_check_evict
  pool_req_create(info->dmi_ctx, &ep, POOL_EVICT, &rpc)


cont_snapshots_refresh_ult
  cont_iv_snapshots_refresh
    ds_iv_fetch

查看符号表
strings xxx


do_target_id
pool_map_target


发送集合请求
crt_corpc_req_hdlr
  co_pre_forward
    crt_iv_sync_corpc_pre_forward 在集合rpc发送前执行cart iv同步
      call_pre_sync_cb
        ivo_pre_sync ivc_pre_sync
        pool_iv_pre_sync
          ABT_cond_signal(pool->sp_fetch_hdls_cond) 满足条件, 触发信号,唤醒等待该信号的线程继续执行
            pool_fetch_hdls_ult



pool_iv_update


crt_ivsync_rpc_issue


ds_rsvc_start
  entry = d_hash_rec_find(&rsvc_hash, id->iov_buf, id->iov_len)
    svc = rsvc_obj(entry)
  start(class, id, db_uuid, caller_term, create, size, replicas, arg, &svc)
    alloc_init(class, id, db_uuid, &svc)
    rdb_create(svc->s_db_path, svc->s_db_uuid, term, size, replicas, &rsvc_rdb_cbs, svc, &storage)
    rdb_start(storage, &svc->s_db) 
    dss_sleep(1 /* ms */) -> 如果创建具有初始成员资格的副本，我们将引导数据库（通过 sc_bootstrap 或外部机制）。 如果我们是“提名”副本，则无需等待选举超时就开始竞选
    rdb_campaign(svc->s_db) -> 举行新的选举（成为领导者的竞选活动）。 必须是投票副本 -> rdb_raft_campaign(db)
      node = raft_get_my_node(db->d_raft)
      rdb_raft_save_state(db, &state)
      raft_election_start(db->d_raft) -> raft_become_candidate(me_)
        raft_set_state(me_, RAFT_STATE_CANDIDATE)
        raft_node_vote_for_me(raft_get_my_node(me_), 1)
        raft_randomize_election_timeout
        raft_send_requestvote -> me->cb.send_requestvote(me_, me->udata, node, &rv) -> static raft_cbs_t rdb_raft_cbs -> rdb_raft_cb_send_requestvote(raft_server_t *raft, void *arg, raft_node_t *node, msg_requestvote_t *msg)
          rdb_create_raft_rpc(RDB_REQUESTVOTE, node, &rpc) -> 创建raft数据库投票请求 -> rdb_requestvote_handler
            ep.ep_tag = daos_rpc_tag(DAOS_REQ_RDB, 0) -> rpc类型 -> DAOS RPC请求类型（确定目标处理标签/上下文）
          rdb_send_raft_rpc(rpc, db)
        raft_count_votes(me_)
      rdb_raft_check_state(db, &state, rc)
    bootstrap_self(svc, arg)
      rsvc_class(svc->s_class)->sc_bootstrap(svc, arg)
  d_hash_rec_insert(&rsvc_hash, svc->s_id.iov_buf, svc->s_id.iov_len, &svc->s_entry, true /* exclusive */)

...  
rdb_requestvote_handler(crt_rpc_t *rpc)
  db = rdb_lookup(in->rvi_op.ri_uuid)
  raft_recv_requestvote
    raft_get_current_term
    raft_set_current_term
    raft_become_follower
      raft_set_state(me_, RAFT_STATE_FOLLOWER)
    __should_grant_vote
    raft_vote_for_nodeid
  rdb_raft_check_state
    committed = raft_get_commit_idx(db->d_raft)
    compaction_rc = rdb_raft_trigger_compaction -> 检查日志是否应该被压缩。 如果是这样，则通过拍摄快照来触发压缩（即，在我们的实现中简单地增加日志基索引）




na_ofi_mem_buf_register

---------------------------------------- DL ----------------------------------------
daos_server start -> daos_engine -> 引擎启动 -> gdb attach `ps aux|grep '/opt/daos/bin/daos_engine' |grep -v grep|awk '{print$2}'` -> b dss_engine_metrics_init -> c
opts.name = "daos_engine"
engine/init.c
main(int argc, char **argv)
engine启动
parse 解析命令行参数: /usr/bin/daos_engine -t 24 -x 2 -g daos_server -d /var/run/daos_server -T 2 -n /mnt/daos0/daos_nveme.conf -p 0 -I 0 -r 24567 -H 2 -s /mnt/daos0
-p 绑定到哪个numa节点(pinned_numa_node)
-t 24 targets数量
-x xshelpernr 2个辅助线程

docker
"/opt/daos/bin/daos_engine",  "-t",  "1",  "-x",  "0",  "-g",  "daos_server",  "-d",  "/var/run/daos_server",  "-T",  "2",  "-n",  "/mnt/daos/daos_nvme.conf",  "-I",  "0",  "-r",  "8192",  "-H",  "2",  "-s",  "/mnt/daos"

禁用透明大页
daos_register_sighand(SIGILL, print_backtrace); 注册自己的信号和打印堆栈
server_init(int argc, char *argv[])
  hlc_recovery_begin
  gethostname
  daos_debug_set_id_cb(server_id_cb)
  daos_debug_init 日志级别 d_debug_data dlog_fac d_log_open D_LOG_STDERR_IN_LOG 合并错误消息 如何接管fabric日志？ if （merge）客户端动态开日志
  freopen setlinebuf fileno
  d_tm_init 遥测指标初始化
  dss_engine_metrics_init
  d_tm_record_timestamp
  drpc_init 初始化drpc客户端
  register_dbtree_classes 注册数据库树, b树
    dbtree_class_register
  dss_topo_init 初始服务器拓扑, numa, cpu core, bitmap位图等
  abt_init(argc, argv) -> 协程 argobots初始化, 机器人
    int	nrequested = abt_max_num_xstreams() -> 0
    nrequired = 1 /* primary xstream */ + DSS_XS_NR_TOTAL -> 1主 + sys + tgt + offload
    atoi(env) 环境变量转整型
    set_abt_max_num_xstreams(max(nrequested, nrequired)) -> 5 两者中的较大值, 设置环境变量 ABT_MAX_NUM_XSTREAMS, 输入整型, 转为字符串char*
      D_ASPRINTF(value, "%d", n);
    ABT_init(argc, argv)
  dss_module_init 服务端模块初始化
    drpc_hdlr_init 注册表 static drpc_handler_t *registry_table 静态 drpc_handler_t *注册表, 函数指针
  dss_ctx_nr_get 获取cart上下文总数
  crt_init_opt 初始化网络层
  if (dss_mod_facs & DSS_FAC_LOAD_CLI) 如果需要加载客户端软件栈,默认不执行
    daos_init 
  daos_hhash_init 初始化hash表
    daos_hhash_init_feats(D_HASH_FT_GLOCK | D_HASH_FT_LRU)
      d_hhash_create(feats | D_HASH_FT_NO_KEYINIT_LOCK, D_HHASH_BITS, &daos_ht.dht_hhash) 创建全局hash表 daos_ht
  pl_init 初始化放置模块, 创建放置映射hash表
  d_hhash_set_ptrtype 服务端使用指针类型的控制器
  ds_iv_init
    D_INIT_LIST_HEAD(&ds_iv_ns_list) 初始化iv,ns链表头
    D_INIT_LIST_HEAD(&ds_iv_class_list)
    ds_iv_ns_tree_topo = crt_tree_topo(CRT_TREE_KNOMIAL, 4) 初始化树拓扑,位运算,移位,或
  modules_load 加载模块信息: vos, rdb, rsvc, security, mgmt, dtx, pool, cont, obj, rebuild
    dss_module_load
      lmod->lm_dss_mod = smod 设置加载模块为动态打开的模块
      d_list_add_tail(&lmod->lm_lk, &loaded_mod_list) 动态打开 dlopen,并加入链表
  mem_leak_check_start();
  hlc_recovery_end 结束 HLC 恢复
  dss_set_start_epoch
  dss_module_init_all
    dss_register_key
    d_list_for_each_entry_safe(lmod, tmp, &loaded_mod_list 遍历之前加载的模块链表
      dss_module_init_one(lmod, &fac)
        rc = smod->sm_init() // vos_mod_init, 
  dss_srv_init 初始化服务(服务初始化)
    ...
    pthread_key_create(&dss_tls_key, NULL)
    vos_standalone_tls_init(DAOS_SERVER_TAG - DAOS_TGT_TAG) -> DAOS-13515 vos：使用 vos 格式存储 SMD (#12251)，现在对于 SSD 上的 MD，我们使用 LMMDB 将 SMD 存储在外部路径上，这为灾难性恢复工作引入了一些额外的工作。 需要额外的工具来解析和验证 LMMDB。 统一使用 VOS 格式会很好，想法是我们仍然可以将 SMD 存储在外部路径上，但不使用 LMMDB，仍然保留 VOS 格式，这可能没问题，因为性能对 SMD 操作不敏感。删除主线程 TLS，主线程和所有 xstream 将访问由主线程分配的独立 vos tls 内存
      self_mode.self_tls = vos_tls_init(tags, 0, -1)
    dss_sys_db_init() -> SRS-394 通用：SSD 上的元数据第 1 阶段 (#12228)，将 SSD 上的元数据功能添加到 DAOS。 对于第一阶段，所有元数据都存储在 DRAM 中。 SSD 上的元数据 blob 在逻辑上将是 DRAM 的精确副本。 为了避免对 SSD 进行过于频繁的字节粒度更新，我们转而提交足够的信息来重放对预写日志 (WAL) 的更新。 后台检查点进程定期运行，以将 DRAM 池与 SSD 元数据 Blob 同步，并回收 WAL 中的空间。 元数据 blob (DAV) 内的初始空间分配器基于 PMDK 分配器。 该补丁还包含支持 PMDK 和 MD on SSD 操作模式所需的各种控制平面更改。 SMD 在此版本中用 lmdb 替换了 VOS，但除此之外，根本性更改位于 VOS 层或以下, lmdb参考: https://zhuanlan.zhihu.com/p/70359311, http://www.lmdb.tech/doc/
      bio_nvme_configured -> 检查是否配置了指定类型的NVMe设备，当指定SMD_DEV_TYPE_MAX时，如果配置了任意类型的设备则返回true
        nvme_glb.bd_nvme_conf -> /mnt/daos/daos_nvme.conf
      vos_db_init(bio_nvme_configured(SMD_DEV_TYPE_META) ? sys_db_path : dss_storage_path) -> 在/mnt/daos下初始化元数据 -> vos_db_init_ex(db_path, NULL, false, false)
    ...
    vos_db_init(dss_storage_path)  sys_db.c dss_storage_path=/mnt/daos
    bio_nvme_init 全局nvme初始化
      d_getenv_bool
      open(nvme_conf, O_RDONLY, 0600) 打开nvme配置文件
      smd_init(db) 服务元数据管理(每个服务) Per-Server Metadata Management (SMD),smd使用sysdb作为kv存储元数据
      spdk_bs_opts_init 将 spdk_bs_opts 结构初始化为默认的 blobstore 选项值。
      bio_spdk_env_init 环境初始化
        spdk_env_opts_init(&opts) 初始化选项 lib/env_dpdk/init.c -> spdk默认初始化
          opts->name = SPDK_ENV_DPDK_DEFAULT_NAME;
          opts->core_mask = SPDK_ENV_DPDK_DEFAULT_CORE_MASK;
          opts->shm_id = SPDK_ENV_DPDK_DEFAULT_SHM_ID;
          opts->mem_size = SPDK_ENV_DPDK_DEFAULT_MEM_SIZE;
          opts->main_core = SPDK_ENV_DPDK_DEFAULT_MAIN_CORE;
          opts->mem_channel = SPDK_ENV_DPDK_DEFAULT_MEM_CHANNEL;
          opts->base_virtaddr = SPDK_ENV_DPDK_DEFAULT_BASE_VIRTADDR;
        bio_add_allowed_alloc
          read_config
            spdk_json_parse
          spdk_json_find_array(ctx->values, "subsystems", NULL, &ctx->subsystems)
          ctx->subsystems_it = spdk_json_array_first(ctx->subsystems)
          spdk_json_decode_object
          spdk_json_strequal(ctx->subsystem_name, "bdev")
          spdk_json_strequal(ctx->subsystem_name, BIO_DEV_TYPE_VMD) -> vmd
          check_vmd_status(ctx, vmd_ss, &vmd_enabled)
          add_bdevs_to_opts
        spdk_env_init 初始化或重新初始化环境库。 对于初始化，必须在使用此库中的任何其他函数之前调用它。 对于重新初始化，参数 `opts` 必须设置为 NULL，并且必须在同一进程中通过 spdk_env_fini() 完成环境库后调用
          pci_env_reinit
          build_eal_cmdline
            push_arg
          rte_eal_init(g_eal_cmdline_argcount, dpdk_args)  初始化环境抽象层 (EAL)。 此函数将仅在 MAIN lcore 上执行，并尽快在应用程序的 main() 函数中执行。 它将 WORKER lcores 置于 WAIT 状态。
          spdk_env_dpdk_post_init
        spdk_unaffinitize_thread 移除当前线程cpu亲和性
        spdk_thread_lib_init 初始化线程库。 必须在分配任何线程之前调用一次。
    bio_register_bulk_ops(crt_bulk_create, crt_bulk_free)
    dss_xstreams_init
    bio_nvme_ctl  操纵全局 NVMe 配置/状态
    drpc_listener_init 启动drpc监听
      generate_socket_path 生成socket路径 drpc_listener_socket_path=/var/run/daos_server/daos_engine_pid_sock
        drpc_listener_start_ult
  server_init_state_init
  drpc_notify_ready
  server_init_state_wait
  dss_module_setup_all 初始化服务端所有模块
  crt_register_event_cb(dss_crt_event_cb, NULL)
  crt_register_hlc_error_cb(dss_crt_hlc_error_cb, NULL)
  dss_xstreams_open_barrier
  d_tm_record_timestamp
  d_tm_set_counter(metrics->rank_id, dss_self_rank())



vos_db_init
  vos_db.db_pub.sd_fetch	  = db_fetch
  ...
  db_open_create
    mkdir(vdb->db_path, 0777)
    vos_pool_create
    vos_pool_open
    vos_cont_create
    db_upsert


dss_xstreams_init
  dss_start_xs_id 0,1..., 计算偏移, 如: xs_id = dss_sys_xs_nr + dss_tgt_nr + i;
    hwloc_bitmap_first 计算位图中的第一个索引（最低有效位）
    dss_start_one_xstream(obj->cpuset, xs_id) 用计算的cpu集启动, 内部绑核
      dss_xstream_alloc(cpus)
        dx->dx_cpuset = hwloc_bitmap_dup(cpus)
      dss_sched_init
        ABT_sched_def		sched_def
        .init	= sched_init,
        .run	= sched_run,
        sched_info_init(dx)
          info->si_cur_ts = daos_getmtime_coarse() 毫秒
          d_hash_table_create si_pool_hash 创建hash表 大小为2的4次方=16
          prealloc_requests(info, SCHED_PREALLOC_INIT_CNT) 预分配请求 8192
            D_ALLOC_PTR(req)
            D_INIT_LIST_HEAD(&req->sr_link)
            d_list_add_tail(&req->sr_link, &info->si_idle_list) 插入链表 
        sched_create_pools(dx); 创建3个池 从预定义类型创建新池。 ABT_pool_create_basic() 创建一个新池，由池类型 kind、访问类型 access 和自动标志 automatic 给出，并通过 newpool 返回其句柄。 kind 指定 newpool 的实现。 有关预定义池的详细信息，请参阅#ABT_pool_kind。 access 提示创建的池的使用。 Argobots 可以为具有更受限制的访问类型的池选择优化的实现（#ABT_POOL_ACCESS_PRIV 是最严格的访问类型）。 有关详细信息，请参阅#ABT_pool_access。 如果 automatic 为 ABT_FALSE，则 newpool 不会自动释放，因此 newpool 在使用后必须由 ABT_pool_free() 释放，除非 newpool 与主执行流的主调度程序相关联。
          ABT_pool_create_basic(ABT_POOL_FIFO, ABT_POOL_ACCESS_MPSC 创建的池可能只被一个执行流弹出
          ABT_sched_config_create
          ABT_sched_create 
          使用调度程序定义创建新的调度程序。ABT_sched_create() 创建一个新的调度器，由定义 def 和调度器配置 config 定义，并通过 newsched 返回它的句柄。def 必须定义所有非可选函数。 有关详细信息，请参阅 ABT_sched_def。newsched 与 pools 数组关联，它有 num_pools 个 ABT_pool 句柄。 如果池的第 i 个元素是 ABT_POOL_NULL，则新创建具有默认池配置的默认 FIFO 池并用作第 i 个池。
      ABT_xstream_create_with_rank 创建具有特定等级的新执行流。 ABT_xstream_create_with_rank() 使用调度程序 sched 创建一个新的执行流，并通过 newxstream 返回其句柄。 如果 sched 为 ABT_SCHED_NULL，则使用具有基本 FIFO 队列和默认调度程序配置的默认调度程序。
      ABT_thread_attr_create
      ABT_thread_attr_set_stacksize 在 ULT 属性中设置堆栈大小。 ABT_thread_attr_set_stacksize() 设置 ULT 属性 attr 中的堆栈大小 stacksize（以字节为单位）。 如果堆栈内存已由 ABT_thread_attr_set_stack() 设置，此例程将更新堆栈大小，同时将堆栈内存保留在 attr 中。
      ABT_thread_create dss_srv_handler


dss_srv_handler 服务控制器，设置cpu亲和性，初始化TLS，crt， nvme，gc, nvme, 启动网络poll
  dss_xstream_set_affinity 设置亲和性
  hwloc_set_cpubind 绑核
  hwloc_set_membind 将当前进程或线程的默认内存绑定策略设置为更喜欢由 set 指定的 NUMA 节点。 这是最便携的形式，因为它允许 hwloc 使用基于进程的操作系统功能或基于线程的操作系统功能，具体取决于可用的功能。 如果指定了 ::HWLOC_MEMBIND_BYNODESET，则集合被视为节点集。 否则它是一个cpuset。
  dss_tls_init 初始化本地存储, 为特定线程分配 dss_thread_local_storage 并将指针存储在特定于线程的值中，该值可以随时使用 dss_tls_get() 获取。
    dss_thread_local_storage_init obj_tls_init
      dtls->dtls_values[i] = dmk->dmk_init(xs_id, tgt_id) 
      dmk->dmk_init(xs_id, tgt_id) -> dss_srv_tls_init | vos_tls_init 
    pthread_setspecific(dss_tls_key, dtls)
  dss_get_module_info
    dss_module_key_get
  crt_context_create
  crt_context_register_rpc_task dss_rpc_hdlr dss_iv_resp_hdlr 注册两个公共回调
  crt_context_idx
  tse_sched_init 使用可选的完成回调和指向用户数据的指针初始化调度程序。 调用者负责完成或取消调度程序。
  bio_xsctxt_alloc 初始化spdk环境和nvme上下文, 为主 XS 初始化 SPDK env 和 per-xstream NVMe 上下文
  dss_nvme_poll_ult
  for (;;) 死循环
    crt_progress

vos_tls_init 初始化vos本地线程存储
  D_INIT_LIST_HEAD(&tls->vtl_gc_pools)
  vos_obj_cache_create(LRU_CACHE_BITS 创建lru
  d_uhash_create vtl_pool_hhash vtl_cont_hhash
  umem_init_txd(&tls->vtl_txd) 为避免为每个事务分配阶段数据，umem 用户应准备每个 xstream 阶段数据并通过 umem_init_txd() 对其进行初始化，此每个 xstream 阶段数据将用于同一 xstream 中的所有事务
    txd->txd_magic = UMEM_TX_DATA_MAGIC
  vos_ts_table_alloc
    vos_ts_copy VOS_TS_TYPE_CONT 不执行此逻辑
      daos_dti_copy
        *des = *src 赋值
        memset(des, 0, sizeof(*des)) 置0
    lrua_array_alloc
  d_tm_add_metric

dss_nvme_poll_ult
  dss_get_module_info
  dss_current_xstream
  bio_nvme_poll -> poll nvme完成事件
    spdk_thread_poll
    is_bbs_owner
    bio_bs_monitor
    is_init_xstream
    scan_bio_bdevs
    bio_led_event_monitor
  ABT_thread_yield



nvme配置文件
/mnt/daos1/daos_nvme.conf

subsystem: bdev
trtype: PCIe
method: bdev_nvme_attach_controller


核心数
主线程XS
首选线程数

dss_topo_init 初始化拓扑
  numa_obj = hwloc_get_obj_by_depth
  dss_tgt_nr = dss_tgt_nr_get numa，核， 分配算法  超额认购 oversubscribe 计算每个引擎多少个target


core 核。 一个计算单元（可以由多个 PU Processing Unit，也就是逻辑处理器共享）
处理器单元的缩写（不是进程！）。 hwloc 识别的最小物理执行单元。 例如，一个核心上可能有多个 PU（例如，硬件线程）。
当多台机器形成整体单一系统映像 (SSI) 时，可以使用附加系统类型，例如 Kerrighed。
最后，请注意，一个对象可以由其在拓扑图中的数字“深度”表示。
水平类型的宽度: 

深度
查看硬件信息 hwloc-info 
depth 0:	1 Machine (type #1)
depth 1:	2 NUMANode (type #2)
depth 2:	2 Package (type #3) 
depth 3:	2 L3Cache (type #4)
depth 4:	56 L2Cache (type #4)
depth 5:	56 L1dCache (type #4)
depth 6:	56 L1iCache (type #4)
depth 7:	56 Core (type #5)
depth 8:	56 PU (type #6)
Special depth -3:	9 Bridge (type #9)
Special depth -4:	7 PCI Device (type #10)
Special depth -5:	10 OS Device (type #11)



学习路线:
  初始化  *
  格式化
  创建池
  创建容器
  挂载
  IO路径
  故障后的重建,rebuild


register_dbtree_classes
  dbtree_class_register(DBTREE_CLASS_KV dbtree_kv_ops
    if (!(tree_feats & (BTR_FEAT_UINT_KEY | BTR_FEAT_DIRECT_KEY))) 利用位运算判断, 如果没有设置唯一键或直接键
    btr_class_registered[tree_class].tc_ops = ops 注册操作(利用数组index索引树的类型)
    btr_class_registered[tree_class].tc_feats = tree_feats 注册树类型特征位


dss_module_init_one
  rc = smod->sm_init() 初始化单个模块 -> vos_mod_init
  dss_register_key(smod->sm_key)
  daos_rpc_register(smod->sm_proto_fmt[i], smod->sm_cli_count[i]... 注册RPC CRT_PROTO_OPC
  drpc_hdlr_register_all(smod->sm_drpc_handlers) 注册所有控制器
    drpc_hdlr_register
      registry_table[module_id] = handler 注册drpc控制器, 模块id也就是对应的drpc枚举中的索引号


模块列表, 模块初始化, vos,rdb,rsvc,security,mgmt,dtx,pool,cont,obj,rebuild
关键字: .sm_name 模块名
enum daos_module_id 模块ID
vos_srv -> vos_srv_module
vos_mod_init vos模块初始化
  vos_pool_settings_init vos池初始化设置
    pmemobj_ctl_set(NULL, "heap.arenas_assignment_type", &atype) 线程使用1个全局的arena POBJ_ARENAS_ASSIGNMENT_GLOBAL 全局模式
  vos_cont_tab_register 注册容器 VOS_BTR_CONT_TABLE class: 14
  vos_dtx_table_register
    dbtree_class_register(VOS_BTR_DTX_ACT_TABLE 注册DTX活动表
    dbtree_class_register(VOS_BTR_DTX_CMT_TABLE 注册提交表
  vos_obj_tab_register
    dbtree_class_register(VOS_BTR_OBJ_TABLE 注册对象索引表(OI) 
  obj_tree_register
    dbtree_class_register(ta->ta_class, ta->ta_feats 注册db树 vos_btr_attrs: VOS_BTR_DKEY, VOS_BTR_AKEY, VOS_BTR_SINGV, vos_dkey, vos_akey, singv
  vos_ilog_init 初始化全局化身日志
    ilog_init
  d_getenv_int DAOS_VOS_AGG_THRESH 触发vos聚合的块数=256块, 总大小256*4K=1MB


rdb初始化
rdb_module_init
  rdb_hash_init
    d_hash_table_create_inplace rdb_hash 加锁创建rdb_hash表


rsvc初始化
rsvc_module_init
  d_hash_table_create_inplace rsvc_hash

安全模块security默认加载
init
  ds_sec_server_socket_path = /var/run/daos_server/daos_server.sock

mgmt 管理模块初始化
ds_mgmt_init
  ds_mgmt_system_module_init
    crt_group_lookup
    d_iov_set
    ds_rsvc_class_register(DS_RSVC_CLASS_MGMT, &mgmt_svc_rsvc_class)
      rsvc_classes[id] = class 将类添加到rsvc类的数组中


dtx 初始化
dtx_init
  设置阈值
  dbtree_class_register(DBTREE_CLASS_DTX_CF
  dbtree_class_register(DBTREE_CLASS_DTX_COS


池初始化
pool
init(void) .sm_init	= init
  ds_pool_cache_init 缓存初始化
    daos_lru_cache_create 创建lru缓存,内存中
      d_hash_table_create_inplace D_HASH_FT_LRU dlc_htable 创建lru_hash表
      D_INIT_LIST_HEAD 初始化链表头 驱逐LRU缓存条目时避免全扫描
  ds_pool_hdl_hash_init
    d_hash_table_create pool_hdl_hash 创建池控制器哈希表
  ds_pool_iv_init 添加服务器 iv 树以在服务器之间共享值。 通过iv tree共享pool map，只先做同步更新
    ds_iv_class_register IV_POOL_MAP 通过iv传播属性
  ds_pool_prop_default_init DAOS-7254 聚合：将 EC 聚合与 VOS 聚合分开 (#5667), 将 EC agg 和 VOS aggregate 分离成两个 ULT，避免 iv fetch，dsc_pool/container open 每次回调，同时保证 EC aggregation 可以在删除快照时触发。添加 DAOS_EC_AGG 环境以禁用 EC 聚合以进行测试
    entry = daos_prop_entry_get(&pool_prop_default, DAOS_PROP_PO_ACL)
    &prop->dpp_entries[i]
    entry->dpe_val_ptr = ds_sec_alloc_default_daos_pool_acl()
      alloc_default_daos_acl_with_perms
        alloc_ace_with_access
          daos_ace_create
        daos_acl_create
          flatten_aces 扁平化
  ds_pool_rsvc_class_register DS_RSVC_CLASS_POOL 注册副本服务
  bio_register_ract_ops 设置nvme操作表, 故障回调, 重新加入的回调, io错误的回调


cont 初始化容器服务
init(void)
  ds_oid_iv_init
    ds_iv_class_register 注册iv类
  ds_cont_iv_init
    ds_iv_class_register 注册各种类
  ds_cont_prop_default_init

obj 对象初始化
obj_mod_init(void)
  obj_utils_init
  obj_class_init
    D_ALLOC_ARRAY(oc_ident_array, OC_NR)
  obj_ec_codec_init


rebuild 重建初始化
init(void)
  D_INIT_LIST_HEAD rg_tgt_tracker_list 所有tagert
  rg_global_tracker_list 主
  ABT_mutex_create(&rebuild_gst.rg_lock) 重建全局锁
  rebuild_iv_init
    ds_iv_class_register

启动drpc监听用户级线程
drpc_listener_start_ult
  setup_listener_ctx(&ctx)
    unlink(sockpath)
    listener = drpc_listen(sockpath, drpc_hdlr_process_msg)  unix domain socket 域套接字, 创建了一个新系统 XS (1)，绑定到与 XS 0 相同的 CPU 核心。dRPC 侦听器循环专门在这个 XS 上运行。 当 dRPC 消息到达时，drpc_progress 在 XS 0 上启动一个新的 ULT 来处理消息。 它必须在 XS 0 上才能与只能在 XS 0 上运行的服务进行交互。为 dRPC 上下文结构添加了引用计数。 drpc_close 现在要么减少引用计数，要么如果这是最后一个引用，则关闭/释放套接字。 删除了现在不再使用的 drpc_recv
    drpc_hdlr_process_msg drpc处理消息的控制器
      unixcomm_listen(sockaddr, O_NONBLOCK, &comm)
        new_unixcomm_socket(flags, &comm)
          comm->fd = socket(AF_UNIX, SOCK_SEQPACKET, 0) 创建socket , SOCK_SEQPACKET为固定最大长度的数据报提供有序的、可靠的、基于双向连接的数据传输路径； 消费者需要在每次输入系统调用时读取整个数据包, 协议参数, 该协议指定要与套接字一起使用的特定协议。 通常只存在一个协议来支持给定协议族中的特定套接字类型，在这种情况下 protocol 可以指定为 0。但是，可能存在许多协议，在这种情况下必须在此指定一个特定的协议 方式。要使用的协议号特定于要进行通信的“通信域”； 请参阅协议 (5)。 有关如何将协议名称字符串映射到协议编号的信息，请参阅 getprotoent(3)
          fcntl(comm->fd, F_SETFL, flags)
        fill_socket_address(sockaddr, &address)
        bind(comm->fd 在 SOCK_STREAM 套接字可以接收连接之前，通常需要使用 bind() 分配本地地址（请参阅 accept(2)）
        listen(comm->fd, SOMAXCONN) listen() 将 sockfd 引用的套接字标记为被动套接字，即，作为将用于使用 accept(2) 接受传入连接请求的套接字。
sockfd 参数是一个文件描述符，它引用 SOCK_STREAM 或 SOCK_SEQPACKET 类型的套接字。
backlog 参数定义了 sockfd 的挂起连接队列可能增长到的最大长度。 如果连接请求在队列已满时到达，则客户端可能会收到带有 ECONNREFUSED 指示的错误，或者，如果底层协议支持重传，则可能会忽略该请求，以便稍后重新尝试连接成功
      init_drpc_ctx(ctx, comm, handler)
        ctx->handler = handler 赋值 = drpc_hdlr_process_msg
    *new_ctx = drpc_progress_context_create(listener)
  dss_ult_create(drpc_listener_run
    ult_create_internal(func
      stream_id = sched_ult2xs(xs_type, tgt_idx) 获取执行流id, 根据映射关系
      dx = dss_get_xstream(stream_id) 通过数组角标
      ABT_thread_attr_set_stacksize(attr, stack_size)
      sched_create_thread(dx, func, arg, attr, ult, flags)
        sched_xstream_stopping?
         daos_abt_thread_create(cur_dx->dx_sp, dss_free_stack_cb, abt_pool, func, arg, t_attr, thread)  max_map_count文件包含限制一个进程可以拥有的VMA(虚拟内存区域)的数量: sysctl vm.max_map_count 默认 64K
         ABT_thread_create -> drpc_listener_run
          set_listener_running(true)
          while (is_listener_running()) 
          rc = drpc_progress(ctx, 1000)
            drpc_progress_context_to_unixcomms(ctx, &comms)
              num_comms = get_open_drpc_session_count(ctx)
            unixcomm_poll(comms, num_comms, timeout_ms)
              fds[i].events = POLLIN | POLLPRI
              poll_rc = poll(fds, num_comms, timeout_ms) 轮训
              comms[i].activity = poll_events_to_unixcomm_activity 获取激活原因
            process_activity(ctx, comms, num_comms) 处理激活的事件
              process_all_session_activities(ctx, comms, num_comms)
                process_session_activity
                  handle_incoming_call
                    drpc_recv_call
                      get_incoming_call
                        unixcomm_recv
                        drpc__call__unpack
                    drpc_response_create
                    create_call_ctx
                    dss_ult_create(drpc_handler_ult
              process_listener_activity(ctx, comms, num_comms)
                drpc_accept
                d_list_add(&session_node->link
          ABT_thread_yield()
          drpc_progress_context_close(ctx)


drpc_handler_ult 执行控制器
  ctx->session->handler(ctx->call, ctx->resp) -> drpc_hdlr_process_msg
  drpc_hdlr_process_msg
    handler = drpc_hdlr_get_handler(request->module) 根据请求中的模块id, 获取请求对应的模块
      handler = registry_table[module_id] 从注册表中获取控制器
    handler(request, resp) 处理请求
  rc = drpc_send_response(ctx->session, ctx->resp)
free_call_ctx(ctx)


static struct vos_sys_db	vos_db
dss_srv_init
vos_db_init vos初始化 元数据初始化 -> vos_db_init_ex(db_path, NULL, false, false)
  memset(&vos_db, 0, sizeof(vos_db)) 系统db置零 /mnt/daos/daos_sys/sys_db 系统数据库sys_db -> struct vos_sys_db
  ABT_mutex_create(&vos_db.db_lock)
  vos_db.db_poh = DAOS_HDL_INVAL
  vos_db.db_pub.sd_fetch	  = db_fetch ...
  uuid_parse(SYS_DB_POOL, vos_db.db_pool) 保留系统池(00000000-DA05-C001-CAFE-000020200101), 容器的uuid(00000000-DA05-C001-CAFE-000020191231)
  db_open_create(&vos_db.db_pub, !!create) 创建或打开 vos_pool_open -> 在持久文件中创建或打开系统数据库(系统数据库是一个简单的本地KV存储) -> db_open_create(struct sys_db *db, bool try_create)
    access 先检查文件是否存在 第二轮创建(try_create为真)
    mkdir(vdb->db_path, 0777)
    vos_pool_create(vdb->db_file, vdb->db_pool, SYS_DB_SIZE, 0, VOS_POF_SYSDB, &vdb->db_poh) -> 创建vosp 系统scm池(128MB) jump, 0个blob, 
    vos_pool_create_ex(const char *path, uuid_t uuid, daos_size_t scm_sz, daos_size_t nvme_sz, daos_size_t wal_sz, unsigned int flags, daos_handle_t *poh) -> wal_sz=0
    
    vos_cont_create(vdb->db_poh, vdb->db_cont) 在vos池中(vpool)创建一个容器
      cont_df_lookup(vpool, &ukey, &args)
        dbtree_lookup(vpool->vp_cont_th, &key, &value) b树搜索
          dbtree_fetch(toh, BTR_PROBE_EQ, DAOS_INTENT_DEFAULT, key, NULL 探测键等于提供键的记录
            btr_probe_key(tcx, opc, intent, key)
              btr_hkey_gen(tcx, key, hkey)
              btr_probe(tcx, probe_opc, intent, key, hkey) 查找b树
                btr_context_set_depth
                btr_cmp 二分查找
                btr_node_is_leaf
                btr_trace_set
                btr_node_child_at
                ... b树
      umem_tx_begin(vos_pool2umm(vpool), NULL)
      dbtree_update(vpool->vp_cont_th, &key, &value)
        btr_upsert(tcx, BTR_PROBE_EQ, DAOS_INTENT_UPDATE, key, val) 更新或插入
      umem_tx_end(vos_pool2umm(vpool), rc)
    vos_cont_open(vdb->db_poh, vdb->db_cont, &vdb->db_coh)
      cont_lookup(&ukey, &pkey, &cont)
        d_uhash_link_lookup(vos_cont_hhash_get(), key, pkey)
      cont_df_lookup(pool, &ukey, &args)
      uuid_copy(cont->vc_id, co_uuid)
      cont->vc_pool	 = pool 设置容器属性
      D_INIT_LIST_HEAD(&cont->vc_dtx_act_list)
      gc_check_cont(cont) 检查是否需要加入gc列表
        D_INIT_LIST_HEAD(&cont->vc_gc_link)
        d_list_add_tail(&cont->vc_gc_link, &cont->vc_pool->vp_gc_cont)
      dbtree_open_inplace_ex(&cont->vc_cont_df->cd_obj_root
      lrua_array_alloc(&cont->vc_dtx_array 分配LRU数组, 活动的dtx记录
      dbtree_create_inplace_ex(VOS_BTR_DTX_ACT_TABLE
      dbtree_create_inplace_ex(VOS_BTR_DTX_CMT_TABLE
      vea_hint_load(&cont->vc_cont_df->cd_hint_df[i]
      vos_dtx_act_reindex(cont)
        lrua_allocx_inplace(cont->vc_dtx_array
        dbtree_upsert(cont->vc_dtx_active_hdl
        dae->dae_start_time = crt_hlc_get()
        d_list_add_tail(&dae->dae_link, &cont->vc_dtx_act_list) dtx可能在其他线程中遍历此链表
      cont_insert(cont, &ukey, &pkey, coh)
        d_uhash_ulink_init(&cont->vc_uhlink, &co_hdl_uh_ops)
        d_uhash_link_insert(vos_cont_hhash_get(), key 插入uuid hash表
    rc = db_upsert(db, SYS_DB_MD, &key, &val) metadata元数据,做容器dkey
      db_io_init(&io, table, key, val)
        io->io_iod.iod_type = DAOS_IOD_SINGLE 一个不可分割的值以原子方式更新
      vos_obj_update(vdb->db_coh 更新指定对象的记录。 如果sql中没有提供输入缓冲区，则该函数返回新分配的地址来存放记录，上层可以直接向这些地址写入数据（rdma模式）
        vos_obj_update_ex
          vos_update_begin
            vos_check_akeys(iod_nr, iods)
            vos_ioc_create
              vos_ilog_fetch_init(&ioc->ic_dkey_info)
                ilog_fetch_init(&info->ii_entries)
              vos_ilog_fetch_init(&ioc->ic_akey_info)
              vos_ioc_reserve_init(ioc, dth)
              vos_ts_set_allocate
                vos_kh_clear()
                vos_ts_set_append_cflags(*ts_set, cflags)
              bio_iod_alloc
              bsgl = bio_iod_sgl(ioc->ic_biod, i)
              rc = bio_sgl_init(bsgl, iov_nr)
            vos_space_hold(vos_cont2pool
              vos_space_query(pool, &vps, false)
                pmemobj_ctl_get(pool->vp_umm.umm_pool stats.heap.curr_allocated
                vea_query(pool->vp_vea_info, attr, stat) 查询nvme
                  dbtree_iterate(vsi->vsi_md_free_btr count_free_persistent
                    verify_free_entry
                  dbtree_iterate count_free_transient
                  d_binheap_size
                  d_binheap_root
                    d_binheap_find
                      d_binheap_find_locked
                        d_binheap_pointer
              estimate_space 估计更新请求将消耗多少空间。 这个保守的估计总是假设新对象、dkey、akey 将被创建用于更新
                umem_slab_usize
                scm += estimate_space_key(umm, dkey)
                vos_media_select
            dkey_update_begin(ioc)
              iod_set_cursor
              akey_update_begin
                vos_media_select(vos_cont2pool(ioc->ic_cont)
                vos_reserve_single(ioc, media, size)
                  reserve_space(ioc, DAOS_MEDIA_SCM, scm_size, &off)
                    vos_reserve_scm
                      umem_reserve
                    vos_reserve_blocks
                      vea_reserve
                  vos_irec_init_csum(irec, value_csum)
                  bio_addr_set_hole
                vos_reserve_recx(ioc, media, size, recx_csum
                  bio_addr_set_hole
                  vos_dedup_lookup 去重
                    d_hash_rec_find(pool->vp_dedup_hash
                  iod_reserve
                    bio_iod_sgl 获取 io 描述符的指定 SG 列表的辅助函数
          vos_obj_copy
            bio_iod_prep
              iterate_biov(biod, arg ? bulk_map_one : dma_map_one, arg)
              dma_rw(biod)
            bio_iod_copy
              iterate_biov(biod, copy_one, &arg)
            bio_iod_post
              dma_rw(biod)
              bdb = iod_dma_buf(biod)
              dma_drop_iod(bdb)
                ABT_cond_broadcast(bdb->bdb_wait_iods)
          vos_update_end
            vos_dedup_verify_fini
            vos_ts_set_add
            vos_tx_begin
            vos_dtx_commit_internal
            vos_obj_hold(vos_obj_cache_current()
            dkey_update
            vos_ts_set_check_conflict
            vos_space_unhold

dss_srv_tls_init


dsm_tls_init
  ds_cont_child_cache_create
  daos_lru_cache_create cont_child_cache_ops
  ds_cont_hdl_hash_create

obj_tls_init
  每个IO指标



---------------------------------------- DL ----------------------------------------
daos_pool, 创建池, 
[root@c1f82c52f85d daos]# dmg pool create sxb -z 4g
Creating DAOS pool with automatic storage allocation: 4.0 GB total, 6,94 tier ratio
{
  "properties": [
    {
      "number": 1,
      "Value": {
        "Strval": "sxb"
      }
    }
  ],
  "Sys": "",
  "HostList": null,
  "User": "",
  "UserGroup": "",
  "ACL": null,
  "NumSvcReps": 0,
  "TotalBytes": 4000000000,
  "TierRatio": [
    0.06,
    0.94
  ],
  "NumRanks": 0,
  "Ranks": [],
  "TierBytes": null
}

{"uuid":"d8854013-e811-498a-b121-e653e289e7c1","sys":"daos_server","user":"root@","usergroup":"root@","properties":[{"number":1,"Value":{"Strval":"sxb"}}],"faultDomains":[2,1,3,1,4,1,1,6,1,1,2,1,1,2,0],"numsvcreps":3,"tierratio":[0,0],"ranks":[0,1,2],"tierbytes":[80000000,1253333333]}

type PoolCreateCmd struct 参数 打印参数: data, _ := json.Marshal(req)
src/control/cmd/dmg/pool.go
PoolCreateCmd
control.PoolCreate PoolCreateReq
    mgmtpb.NewMgmtSvcClient(conn).PoolCreate(ctx, pbReq)
        func (svc *mgmtSvc) PoolCreate(ctx context.Context, req *mgmtpb.PoolCreateReq)
            checkLeaderRequest 在leader上创建池
                CheckLeader
                checkSystemRequest(req)
            svc.harness.CallDrpc(ctx, drpc.MethodPoolCreate, req)
            C.DRPC_METHOD_MGMT_POOL_CREATE  src/control/drpc/modules.go
            process_drpc_request
                case DRPC_METHOD_MGMT_POOL_CREATE
                ds_mgmt_drpc_pool_create(drpc_req, drpc_resp)  src/mgmt/srv_drpc.c
                  conv_req_props 解析请求中的池属性
                  targets = uint32_array_to_rank_list(req->ranks, req->n_ranks)
                  uuid_parse(req->uuid, pool_uuid)
                  create_pool_props
                    ds_pool_svc_rf_from_nreplicas
                    daos_acl_from_strs
                    new_prop = daos_prop_alloc(entries) 分配entries个属性
                  daos_prop_merge 合并属性
                  ds_mgmt_create_pool 管理面创建池
ds_mgmt_create_pool 创建池
    crt_group_ranks_get(NULL, &pg_ranks)
    crt_group_size(NULL, &pg_size)
    pg_ranks = d_rank_list_alloc(pg_size)
    d_rank_list_dup(&filtered_targets, targets)
    d_rank_list_filter
    d_rank_list_identical 比较两个排名列表是否相同
      qsort(rank_list->rl_ranks, rank_list->rl_nr 先排序,再比较
      pg_str = d_rank_list_to_str(pg_ranks) rank列表转字符串
    ds_mgmt_tgt_pool_create_ranks
        topo = crt_tree_topo(CRT_TREE_KNOMIAL, 4) 创建k项树拓扑, 创建集合RPC, 向所有目标发送创池RPC
        opc = MGMT_TGT_CREATE -> ds_mgmt_hdlr_tgt_create 操作码编码 DAOS_RPC_OPCODE, 操作码,模块id,版本
        crt_corpc_req_create
        pool_create_rpc_timeout(tc_req, scm_size) -> 针对创池使用较大的RPC超时
          max(timeout, default_timeout) -> 计算超时后比较, 取更大的超时
        crt_req_set_timeout(tc_req, timeout)
        dss_rpc_send 发送rpc到所有目标rank上, 等待回复, 不消耗引用计数
          ABT_eventual_create(sizeof(*status), &eventual)
          crt_req_send(rpc, rpc_cb, &eventual)
          ABT_eventual_wait(eventual, (void **)&status) 等待返回, 挂起或立即返回
        tc_out = crt_reply_get(tc_req) -> return rpc->cr_output -> typedef struct crt_rpc -> 获取回复 reply
        --------------------------------------------------------
        ds_mgmt_hdlr_tgt_create(crt_rpc_t *tc_req) -> 服务端收到RPC,执行创池RPC控制器, 抽象成在目标上执行创建操作
        ...
    d_rank_list_alloc(svc_nr)
      ds_mgmt_pool_svc_create 创建svc
        ds_pool_svc_create
            select_svc_ranks
            d_iov_set
            ds_rsvc_dist_start
            rsvc_client_init
            d_backoff_seq_init -> 退避
            rsvc_client_choose
            pool_req_create(info->dmi_ctx, &ep, POOL_CREATE, &rpc) 操作码 = POOL_CREATE -> ds_pool_create_handler
            dss_rpc_send
            rsvc_client_complete_rpc
            ds_pool_create_handler
            ds_pool_start

其他rank收到rpc请求
ds_mgmt_hdlr_tgt_create(crt_rpc_t *tc_req)
  tc_out = crt_reply_get(tc_req) -> 先拿到回复字段, 在创建完成后填充该字段, 如: tc_out->tc_ranks.ca_arrays = rank;
  cleanup_leftover_pools 清理滞留池以释放空间, 删除spdk blobs和pmemobj
    zombie_pool_iterate(cleanup_leftover_cb, &dead_list)
      common_pool_iterate 遍历僵尸目录 调用 cleanup_leftover_cb /ZOMBIES 在ds_mgmt_tgt_setup中设置僵尸目录
        storage = opendir(path) 打开目录
        cb(uuid, arg) -> cleanup_leftover_cb
          dss_thread_collective(tgt_kill_pool, &id, 0)  在所有服务器 xstreams 上共同执行 func(arg)。 只能由 ULT 调用。 只能执行 tasklet 兼容的函数
            coll_ops.co_func = tgt_kill_pool = carg.ca_future.dfa_func	= ops->co_func
            dss_collective_internal
            vos_pool_kill(id->uuid) 执行回调
              pool_lookup(&ukey, &pool)
              gc_have_pool(pool)
                gc_del_pool(pool)
                vos_pool_decref(pool)
              vos_delete_blob(uuid)
                spdk_thread_send_msg(owner_thread(bbs), blob_msg_delete, &bma)
          d_list_add(&dp->dp_link, dead_list)
  d_hash_rec_insert(&pooltgts->dpt_creates_ht 将pool uuid 插入hash表, 用于销毁目标: d_hash_rec_find(&pooltgts->dpt_creates_ht
  pthread_create tgt_create_preallocate 创建预分配线程
      (void)dss_xstream_set_affinity(tca->tca_dx) 设置cpu亲和性
      ds_mgmt_tgt_file
        path_gen(pool_uuid, dss_storage_path, fname, idx, fpath)
      access(tca->tca_path, F_OK)
      dir_fsync(tca->tca_path) 已存在
        fd = open(path, O_RDONLY|O_DIRECTORY) 打开
        rc = fsync(fd) 同步
        (void)close(fd) 关闭
      path_gen 不存在
      mkdir(tca->tca_newborn, 0700)
      tgt_vos_preallocate 针对target预分配vos, 每个执行流创建一个vos文件, 每个scm分区(pm对象)最小16MB
        path_gen
        open
        scm_size = D_ALIGNUP(scm_size, 1ULL << 12) 向上对齐到4KB
        fallocate(fd, 0, 0, scm_size) 为 vos 文件预分配块以提供一致的性能并避免通过页面错误(缺页异常)进入后端文件系统分配器, 使用 fallocate(2) 而不是 posix_fallocate(3) 因为后者与 tmpfs 是伪造的, fallocate() 允许调用者直接操作为 fd 引用的文件分配的磁盘空间，其字节范围从 offset 开始并持续 len 个字节
        fallocate -l 50M /data/web/www/html/b.zip 文件预留
        fsync(fd) 除了刷新文件数据，fsync() 还刷新与文件关联的元数据信息
  for(;;) 死循环
  pthread_cancel(thread) 销毁状态,则取消线程
  pthread_tryjoin_np(thread, &res) 等线程
  ABT_thread_yield()
  dss_thread_collective(tgt_vos_create_one, &vpa, 0) 在所有服务流上集体执行 coll_ops.co_func	= tgt_vos_create_one
    dss_collective_internal
      dss_thread_collective_reduce(&coll_ops, &coll_args, flags) coll_ops.co_func	= func 集合聚合缩减函数
        dss_collective_reduce_internal
  rc = rename(tca.tca_newborn, tca.tca_path)
  (void)dir_fsync(tca.tca_path) 确认目录已持久化
  rc = ds_pool_start(tc_in->tc_pool_uuid);
    daos_lru_ref_hold(pool_cache 查找缓存,获取其引用, 最近最少使用, 查找或插入
    pool = pool_obj(llink)
    daos_lru_ref_release(pool_cache, &pool->sp_entry) 已经存在,释放引用
    daos_lru_ref_hold(pool_cache &arg 带参数创建
    dss_ult_create(pool_fetch_hdls_ult, pool, DSS_XS_SYS
    ds_pool_start_ec_eph_query_ult(pool)
    ds_iv_ns_start(pool->sp_iv_ns)
  (void)tgt_destroy(tca.tca_ptrec->dptr_uuid

----------------------
ds_pool_create_handler
    pool_svc_lookup
    rdb_tx_begin
    rdb_tx_lookup
    daos_prop_dup
    pool_prop_default_copy
    rdb_tx_create_root
    init_pool_metadata
      d_iov_set
      rdb_tx_update
      rdb_tx_update
      write_map_buf
      write_map_buf
      rdb_tx_create_kvs
    ds_cont_init_metadata
    rdb_tx_commit



daos_pool_connect


tgt_vos_create_one
  path_gen /mnt/daos/93c634a2-d122-4914-b4b3-5ba76c696f68/vos-0 创建vos目录
  vos_pool_create 创建vos池
    *xs_ctxt = vos_xsctxt_get() 上下文是怎样初始化的?
    uma.uma_id = UMEM_CLASS_PMEM 设置内存类型
    if (flags & VOS_POF_SMALL) 小池, 置上独立标记位(VOS_POF_EXCL)
    pool_lookup(&ukey, &pool) 根据key查询池
      hlink = d_uhash_link_lookup(vos_pool_hhash_get(), ukey, NULL)
        vos_tls_get()->vtl_pool_hhash
      *pool = pool_hlink2ptr(hlink)
    vos_pmemobj_create 0600 -rw- (600) 只有拥有者有读写权限 在创建/连接/关闭池时的blob对应的操作, /mnt/daos/daos_sys/sys_db
    vos_pmemobj_create(path, uuid, VOS_POOL_LAYOUT, scm_sz, nvme_sz, wal_sz, flags, &ph)
      struct bio_xs_context	*xs_ctxt = vos_xsctxt_get() -> 每个xs一个nvme上下文
      store.store_type = umempobj_get_backend_type() -> 默认pm作后端, daos_md_backend = DAOS_MD_PMEM
      bio_mc_create(xs_ctxt, pool_id, meta_sz, wal_sz, nvme_sz, mc_flags)
      bio_mc_open(xs_ctxt, pool_id, mc_flags, &mc)
      bio_meta_get_attr(mc, &store.stor_size, &store.stor_blk_size, &store.stor_hdr_blks)
      pop = umempobj_create(path, layout, UMEMPOBJ_ENABLE_STATS, scm_sz, 0600, &store)
        pop = pmemobj_create(path, layout_name, poolsize, mode) -> 创建持久内存对象,取scm大小 -> pmdk笔记: https://blog.csdn.net/Sylvia_Wu51/article/details/117442789, 创建pm内存池: https://manpages.debian.org/experimental/libpmemobj-dev/pmemobj_create.3.en.html, 参考:持久内存编程开发人员综合指南
      pmemobj_ctl_set(pop, "stats.enabled", &enabled) -> 启用使用率统计, scm和nvme
      register_slabs(umm_pool) -> register_slabs(struct umem_pool *ph_p) -> slab_map[] -> 不同大小的slab
        __builtin_ctz(used) -> 返回输入数二进制表示从最高位开始(左起)的连续的0的个数
        set_slab_desc(ph_p, slab) -> 创建slab
          pmemobj_ctl_set(pop, "heap.alloc_class.new.desc", &pmemslab) -> 创建pool_set, 参考: https://cloud.tencent.com/developer/article/1747314, 官方API: http://wlemkows.github.io/pmdk/manpages/linux/v1.6/libpmemobj/pmemobj_ctl_get.3
    ...
    pool_df = vos_pool_pop2df(ph) pm转持久格式 -> umempobj_get_rootptr
      TOID(struct vos_pool_df) pool_df
      pool_df = POBJ_ROOT(pop, struct vos_pool_df) 获取根
        pmemobj_root(PMEMobjpool *pop, size_t size)
    uma.uma_id = umempobj_backend_type2class_id(ph->up_store.store_type)
    umem_class_init(&uma, &umem) 通过uma中的属性实例化一个内存类umm -> static umem_ops_t	vmem_ops
      set_offsets(umm) 为池计算出必要的偏移量和基地址, 如果是vmem则置零
        pmemobj_root
        pmemobj_direct
    umem_tx_begin(&umem, NULL) 开始事务
      umm->umm_ops->mo_tx_begin(umm, txd)  .mo_tx_begin		= pmem_tx_begin, 开始pm事务
        pmemobj_tx_begin(umm->umm_pool  可变参数
        pmdk事务 libpmemobj就提供了存储的事务特性
        https://www.cnblogs.com/Kimbing-Ng/p/12738735.html 
        https://pmem.io/pmdk/manpages/linux/v1.4/libpmemobj/libpmemobj.7/  
        https://manpages.debian.org/experimental/libpmemobj-dev/pmemobj_tx_begin.3.en.html
        https://my.oschina.net/fileoptions/blog/1629405
    umem_tx_add_ptr(&umem, pool_df, sizeof(*pool_df))
      mo_tx_add_ptr -> pmem_tx_add_ptr
        pmemobj_tx_add_range_direct 添加内存范围
    dbtree_create_inplace(VOS_BTR_CONT_TABLE 创建b树, 头文件: btree.h
      dbtree_create_inplace_ex
        btr_context_create
          btr_class_init
            umem_class_init(uma, &tins->ti_umm)
            tins->ti_ops = tc->tc_ops 从注册的b树中初始化,设置标记位,函数操作表
          btr_context_set_depth
        btr_tx_tree_init
          btr_tx_begin(tcx)
          btr_tree_init(tcx, root)
            btr_root_init(tcx, root, true)
              btr_root_tx_add(tcx)
          btr_tx_end(tcx, rc)
        btr_tcx2hdl
          hdl.cookie = (uint64_t)tcx
    dbtree_close(hdl) 减引用
    gc_init_pool(&umem, pool_df)
      GC_MAX = 4
      struct vos_gc_bin_df *bin = &pd->pd_gc_bins[i]
      bin->bin_bag_size  = gc_bag_size 包大小=250
      bag_id = umem_zalloc(umm, size)
        mo_tx_alloc -> pmem_tx_alloc(pmemobj_tx_xalloc分配一个新对象) | vmem_alloc(calloc|malloc)
    umem_tx_commit(&umem) | umem_tx_abort(&umem, rc)
      mo_tx_commit 提交事务 -> pmem_tx_commit
        pmemobj_tx_commit
        pmemobj_tx_end
    bio_nvme_configured // 以下只针对配置了Nvme大小的池, 纯SCM(vos池)不走下面的逻辑
    bio_blob_create(uuid, xs_ctxt, nvme_sz)
    // Format SPDK blob header 格式化spdk
      spdk_bs_get_cluster_size(bbs->bb_bs)
      spdk_blob_opts_init
      smd_pool_get_blob
      blob_cp_arg_init(ba)
      bio_bs_hold(bbs)
      spdk_thread_send_msg(owner_thread(bbs), blob_msg_create, &bma)
      blob_wait_completion(xs_ctxt, ba)
      smd_pool_add_tgt
      spdk_bs_create_blob_ext
    blob_wait_completion
      xs_poll_completion
        spdk_thread_poll(ctxt->bxc_thread, 0, 0)
        在线程上执行一次迭代处理。 这包括过期的和连续的轮询器以及消息。 如果线程已经退出，则立即返回。
    vea_format(&umem, vos_txd_get(), &pool_df->pd_vea_df, VOS_BLK_SZ  vos_blob_format_cb
      rc = cb(cb_data, umem) 先执行回调 vos_blob_format_cb
      umem_tx_begin
      umem_tx_add_ptr
      dbtree_open_inplace free_btr  vec_btr
      dbtree_update
      dbtree_create_inplace(DBTREE_CLASS_IV
      umem_tx_abort(umem, rc) : umem_tx_commit 提交或终止
    pool_open(ph, pool_df, uuid, flags, poh) // 打开池
      pool_alloc(uuid, &pool)
        d_uhash_ulink_init(&pool->vp_hlink, &pool_uuid_hops) uuid hash tab
        D_INIT_LIST_HEAD(&pool->vp_gc_link)
        D_INIT_LIST_HEAD(&pool->vp_gc_cont)
      vos_register_slabs(uma) 注册7个slab
        set_slab_prop(i, slab) slab->unit_size: 单个分配单元中的字节数。 单个分配最多可跨越 64 个单元（或在没有标头的情况下为 1 个）。 如果创建一个具有特定单元大小的分配类并强制它处理更大的大小，那么将使用多个单元。 例如，具有紧凑标头和 128 字节单元大小的分配类，对于 200 字节的请求，将创建一个包含 256 字节且跨越两个单元的内存块。 该分配的可用大小为 240 字节：2 * 128 - 16（标头）
          vos_tree_get_overhead(0, tclass, 0, &ovhd) 返回常量，可用于估计持久内存磁盘格式的元数据开销
            evt_overhead_get
            dbtree_overhead_get
              hkey_size = btr_hkey_size_const(ops, ofeat) 从树类中找: enum vos_tree_class 
                size = ops->to_hkey_size() oi_hkey_size
              ovhd->to_record_msize = ops->to_rec_msize(alloc_overhead) oi_rec_msize
          pmemobj_ctl_set heap.alloc_class.new.desc 以编程方式执行写入 ctl 查询
      umem_class_init
      dbtree_open_inplace_ex
        btr_context_create
        btr_tcx2hdl
      vos_xsctxt_get
      bio_ioctxt_open 打开每个vos的io上下文
        D_INIT_LIST_HEAD(&ctxt->bic_link)
        bio_bs_hold 有nvme才执行
        bio_blob_open
      vea_load 从 SCM 加载空间跟踪信息以初始化内存中的复合索引
      vos_dedup_init
        d_hash_table_create vp_dedup_hash
      pool_link
        d_uhash_link_insert vos_pool_hhash_get
        vos_pool2hdl
      vos_space_sys_init
        POOL_SCM_SYS
        get_frag_overhead
        gc_reserve_space(&pool->vp_space_sys[0]) gc保留
        agg_reserve_space(&pool->vp_space_sys[0]); 聚合保留
        tiny pool 小池不预留
      gc_add_pool 附加一个用于 GC 的池，此函数还将池固定在打开的哈希表中。如果 GC 没有留下任何内容，GC 将从开放哈希中删除该池，并且用户已经关闭它
        d_list_add_tail(&pool->vp_gc_link, &tls->vtl_gc_pools) 加入链表, 取链表中的数据 pool = d_list_entry(tls->vtl_gc_pools.next
        vos_gc_run 其他线程执行gc
          pool = d_list_entry(pools->next, struct vos_pool, vp_gc_link)
          gc_reclaim_pool 为池开启垃圾回收
      lock_pool_memory
        getrlimit(RLIMIT_MEMLOCK, &rlim) 获取软限制和硬限制, 这是可以锁定到 RAM 中的最大内存字节数, 值 RLIM_INFINITY 表示对资源没有限制（在 getrlimit() 返回的结构和传递给 setrlimit() 的结构中）,一般都有限制, 上面使用 PRIuMAX 宏的解决方案很好，但是从 C99 开始还有 j 类型修饰符，用于打印 intmax_t 和 uintmax_t 类型的值：
        printf("Soft limit: %" PRIuMAX " bytes\n", (uintmax_t)cur_bytes);
        printf("Soft limit: %ju bytes\n", (uintmax_t)cur_bytes);
        mlock((void *)pool->vp_umm.umm_base, pool->vp_pool_df->pd_scm_sz) 锁内存, 保证范围 [ADDR,ADDR+LEN) 映射的所有整页都驻留在内存中(no swap)

vos_blob_format_cb 写blob头
  bio_ioctxt_open 创建一个上下文来获取blob
  bio_ioctxt_close

rsvc 复制服务
架构, daos vos, daos元数据, 核心, daos基石
[ pool_svc, cont_svc, ... ]
[ ds_rsvc ]
[                rdb                ]
[ raft ]
[                vos                ]
umm
pmdk/mem/bio/vea

---------------------------------------- DL ----------------------------------------

vos_cont_destroy
  gc_wait()


查池
umg pool list -v
func (cmd *PoolListCmd) Execute
func ListPools
  PoolQuery(ctx, rpcClient, &PoolQueryReq{ID: p.UUID})
  func (svc *mgmtSvc) PoolQuery
  svc.makePoolServiceCall(ctx, drpc.MethodPoolQuery, req)
  MethodPoolQuery MgmtMethod = C.DRPC_METHOD_MGMT_POOL_QUERY
  DRPC_METHOD_MGMT_POOL_QUERY
  ds_mgmt_drpc_pool_query
    ds_mgmt_pool_query
      ds_pool_svc_query
        rsvc_client_init
        rsvc_client_choose
        pool_req_create POOL_QUERY 创建查池RPC请求
        in = crt_req_get(rpc)
        pool_query_bits
        map_bulk_create 创建池map bulk
          *buf = pool_buf_alloc(nr) 分配一个缓冲
          d_iov_set(&iov, *buf, pool_buf_size((*buf)->pb_nr))
          sgl.sg_iovs = &iov;
          crt_bulk_create(ctx, &sgl, CRT_BULK_RW, bulk)
        dss_rpc_send(rpc) -> ds_pool_query_handler
        out = crt_reply_get(rpc)
        pool_rsvc_client_complete_rpc
        process_query_result 处理结果
        rsvc_client_fini(&client)
  pool_query_free_tier_stats


POOL_QUERY opc=2040003, 关键词:"pool", DAOS_POOL_MODULE
ds_pool_query_handler
  pool_svc_lookup_leader
    ds_pool_failed_lookup
    ds_rsvc_lookup_leader(DS_RSVC_CLASS_POOL, &id, &rsvc, hint) -> 为了方便一般复制服务 RPC 处理程序，此函数通过 id 查找复制服务，检查其是否已启动，并引用领导者字段。 仅当返回零时才填充 svcp。 如果复制的服务未启动，则提示已填充
      ds_rsvc_lookup(class, id, &svc) -> 查找副本服务
        entry = d_hash_rec_find(&rsvc_hash, id->iov_buf, id->iov_len) <- ds_rsvc_start -> 启动复制服务。 如果 create 为 false，则忽略所有剩余的输入参数； 否则，先创建副本。 如果replicas为NULL，则忽略所有剩余的输入参数； 否则，引导复制的服务
      get_leader(svc)
    *svcp = pool_svc_obj(rsvc)  
  rdb_tx_begin
  d_iov_set key, value
  rdb_tx_lookup
  pool_prop_read
  read_map_buf 池服务负责人使用存储在 AEP 中的池映射缓冲区用于 RDMA，同时传播到其他服务器或传输回客户端。此补丁更改了这一点，并在 RDMA 之前将池映射缓冲区从 AEP 复制到 DRAM，因为某些提供商无法支持来自/到 AEP 的 RDMA, AEP简介 AEP是Intel推出的一种新型的非易失Optane Memory设备，又被称作Apache Pass，所以一般习惯称作AEP。在这之前也有类似的设备称作NVDIMM或PMEM，目前Linux创建的AEP设备节点也是叫做pmem（如/dev/pmem0），所以本文中NVDIMM或PMEM都指AEP, NVDIMM (Non-Volatile Dual In-line Memory Module) 是一种可以随机访问的, 非易失性内存
    locate_map_buf 将持久内存中的池映射缓冲区地址和池映射版本分别检索到“map_buf”和“map_version”
      d_iov_set
      rdb_tx_lookup
  ds_pool_transfer_map_buf 传输大块数据, 将池映射缓冲区传输到“remote_bulk”。 如果远程批量缓冲区太小，则返回 -DER_TRUNC 并将“required_buf_size”设置为本地池映射缓冲区大小。
    pool_buf_size
    crt_bulk_get_len
    crt_bulk_create(rpc->cr_ctx, &map_sgl, CRT_BULK_RO, &bulk)
    ABT_eventual_create(sizeof(*status), &eventual)
    crt_bulk_transfer(&map_desc, bulk_cb, &eventual, &map_opid)
    ABT_eventual_wait(eventual, (void **)&status)
  crt_reply_send

傲腾产品分两类：，分别是傲腾非易失性内存和傲腾SSD。这里要讲的是傲腾内存。习惯上大家叫 aep，实际上这个称呼并不十分正确，正确的叫法应该是：DCPM或傲腾DCPM，全称是：optane DC（data center）persistent memory，而aep(Apache pass)是指基于3D xpoint的持久化内存，并不完全等同于DCPM

server_init
  dss_module_setup_all
    ds_mgmt_tgt_setup
      NEWBORNS 新生儿
      ZOMBIES 僵尸


dss_module_setup_all      
  rc = m->sm_setup() 服务端动态模块初始化 -> ds_pool_start_all 创建ULT, 在SYS执行流上启动
  ds_mgmt_tgt_pool_iterate(start_one, NULL /* arg */)
    common_pool_iterate(dss_storage_path, cb, arg)
      start_one uuid, arg
        ds_pool_start(uuid)
        path = pool_svc_rdb_path(uuid)
        ds_rsvc_start(DS_RSVC_CLASS_POOL





recv_installsnapshot
rdb_raft_cb_recv_installsnapshot
  rdb_raft_destroy_lc
    vos_cont_destroy 销毁容器
      VOS 独立模式的功能，它回收所有已删除的项目。
      gc_wait
        vos_gc_run


umem事务示例:
{
  /* Start transaction */
  err = umem_tx_begin(umem);
  if (err) return err;

  /* Proceeds with works inside of transaction */
  err = foo();
  if (err) goto out;
  err = pmemobj_tx_xxx(umem);
  if (err) { error cleanup for foo(); goto out; }

  out:
  /* Commit/Abort transaction on success/error */
  if (err)
  err = umem_tx_abort(umem, err);
  else
  err = umem_tx_commit(umem);
}


dss_collective_reduce_internal 服务端集合请求内部收敛
  ABT_future_create(xs_nr + 1, collective_reduce, &future) 创建未来, 聚集集合, 其他线程 ABT_future_set dfa_future
  carg.ca_future.dfa_future = future
  carg.ca_future.dfa_func	= ops->co_func
  co_reduce_arg_alloc
  ABT_future_set(future, (void *)&aggregator)
  sched_create_thread(dx, collective_func, stream, attr, NULL, flags)
  ABT_future_wait(future) 等信号
  co_reduce_arg_free

collective_reduce
  aggregator->at_reduce 
  pool_query_xs_reduce
    aggregate_pool_space 聚合池空间



dss_acc_offload
dss_ult_create(ult_execute_cb 创建一个ult, 调度执行
  ult_execute_cb
    arg->dfa_func(arg->dfa_arg) 先执行回调 -> collective_reduce
    ABT_future_set(arg->dfa_future, (void *)(intptr_t)rc) 同步
    arg->dfa_comp_cb(arg->dfa_comp_arg) 异步执行完成回调



pool_fetch_hdls_ult
  ABT_cond_wait(pool->sp_fetch_hdls_cond, pool->sp_mutex)
  ds_pool_iv_conn_hdl_fetch(pool)
    ds_iv_fetch(pool->sp_iv_ns, &key, &sgl, false /* retry */) 获取或新建iv
      iv_op(ns, key, value, NULL, 0, retry, IV_FETCH)
        _iv_op(ns, key, value, sync, shortcut, retry, opc)
          iv_op_internal(ns, key, value, sync, shortcut, opc)
            class = iv_class_lookup(key_iv->class_id)
            iv_key_pack(&key_iov, key_iv)
            ds_iv_ns_get(ns)
            case IV_FETCH
              crt_iv_fetch ds_iv_done
            ABT_future_wait(future)
            ds_iv_ns_put(ns)
  ABT_cond_signal(pool->sp_fetch_hdls_done_cond) ABT_cond_wait(pool->sp_fetch_hdls_done_cond, pool->sp_mutex) <- pool_fetch_hdls_ult_abort



pool架构
pool
  iv_ns(ds_iv_ns)

iv_user
  key

register ds_iv_class/crt_iv_class

--------------------------------------
       POOL
--------------------------------------
LRU | DS_IV -|-> CRT_IV | HANDLE
--------------------------------------


iv 内部广播变量
static struct daos_llink_ops pool_cache_ops
pool_alloc_ref
  ds_iv_ns_create
    iv_ns_create_internal
      ns->iv_master_rank = master_rank




----------------------------------
ULT
----------------------------------





tse_sched_init(&dx->dx_sched_dsc, NULL, dmi->dmi_ctx)
    D_INIT_LIST_HEAD(&dsp->dsp_init_list); 双向链表初始化
    ...
    tse_sched_register_comp_cb
        d_list_add(&dsc->dsc_list,&dsp->dsp_comp_cb_list) 插入链表头部



初始化 tse_task。 该任务会被添加到调度器任务列表中，稍后被调度，如果提供了依赖任务，则该任务将被添加到依赖任务的dep列表中，一旦依赖任务完成，则添加该任务 到调度程序列表。
attached to scheduler 


dc_task_create
    tse_task_create(func, sched, NULL, &task)
        D_ALLOC_PTR(task) // coredump
        D_INIT_LIST_HEAD(&dtp->dtp_list)   初始化列表，任务列表，完成列表，预完成列表
        ...
        *taskp = task





D_ALLOC_PTR(task)
    D_ALLOC(ptr, sizeof(*ptr))
        D_ALLOC_CORE(ptr, size, 1)
            (ptr) = (__typeof__(ptr))d_calloc((count), (size)) 分配
            D_CHECK_ALLOC 检查分配


内存分配检查宏展开
#define D_CHECK_ALLOC(func,cond,ptr,name,size,count,cname,on_error) 
do { 
    if (D_SHOULD_FAIL(d_fault_attr_mem)) { 
        d_free(ptr); 
        (ptr) = NULL; 
    } 
    if ((cond) && (ptr) != NULL) { 
        if ((count) <= 1) 
            D_DEBUG(DB_MEM, "alloc(" #func ") '" name "':%i at %p.\n", (int)(size), (ptr)); 
        else 
            D_DEBUG(DB_MEM, "alloc(" #func ") '" name "':%i * '" cname "':%i at %p.\n", (int)(size), (int)(count), (ptr)); 
        break; 
    } 
    (void)(on_error); 
    if ((count) >= 1) 
        D_ERROR("out of memory (tried to " #func " '" name "':%i)\n", (int)((size) * (count))); 
    else 
        D_ERROR("out of memory (tried to " #func " '" name "':%i)\n", (int)(size)); 
} while (0)



vos_blob_format_cb
  bio_write_blob_hdr
    bio_write
      bio_rw
        bio_rwv
          bio_iod_post 提交io描述
            dma_rw
              nvme_rw
                spdk_blob_io_write rw_completion
                  blob_request_submit_op
                    blob_request_submit_op_single
                      bs_batch_write_dev
                        blob_bdev->bs_dev.write
                        bdev_blob_write
                          spdk_bdev_write_blocks


rw_completion
  spdk_thread_send_msg


客户端mount:
dfuse -m /mnt/sxb --pool sxb --cont sxb
dfuse_main.c -> main
  daos_debug_init
    d_log_init_adv 高级日志初始化, 客户端日志文件
      log_file = getenv(D_LOG_FILE_ENV) export D_LOG_FILE=/tmp/daos_client.log
      debug_prio_err_load_env
      d_log_open
        freopen(mst.log_file 重新关联标准输出或错误输出
        setlinebuf(stderr) 设置错误输出为行缓冲
    d_log_sync_mask
  daos_init
    daos_eq_lib_init 事件队列
      crt_init_opt
  dfuse_fs_init
    d_hash_table_create_inplace dpi_pool_table 打开的池表， 创建hash表 大小=power2(n次方)， 操作方法
    dpi_iet open inodes
    daos_eq_create 一个事件队列关联一个网络上下文， 跟踪池的多个事件
      daos_eq_alloc
      crt_context_create(&eqx->eqx_ctx)
        crt_contpext_provider_create
          crt_context_init
      daos_eq_insert(eqx)
      daos_eq_handle(eqx, eqh)
      tse_sched_init(&eqx->eqx_sched, NULL, eqx->eqx_ctx)
    sem_init
  duns_resolve_path
  dfuse_pool_connect
  dfuse_cont_open
  dfuse_fs_start 启动文件系统
    d_hash_rec_insert(&fs_handle->dpi_iet 将根插入hash表, 在 dfuse_reply_entry 中也会插入: d_hash_rec_find_insert(&fs_handle->dpi_iet
    d_slab_init
    d_slab_register
    dfuse_progress_thread pthread_create(&fs_handle->dpi_thread, NULL, dfuse_progress_thread, fs_handle) 异步进度线程，该线程在启动时使用事件队列启动，并阻塞在信号量上，直到创建异步事件，此时线程唤醒并在 daos_eq_poll() 中忙于轮询直到完成
      sem_wait -> 等信号量
      daos_eq_poll  从 EQ 中检索完成事件
        daos_eq_lookup 查找私有事件队列
          daos_hhash_link_lookup
        crt_progress_cond(epa.eqx->eqx_ctx, timeout, eq_progress_cb, &epa)
          eq_progress_cb
        dfuse_launch_fuse(fs_handle, &args) 创建fuse文件系统
          fuse_session_new(args, &dfuse_ops, sizeof(dfuse_ops), fs_handle)
          fuse_session_mount
          dfuse_send_to_fg
          dfuse_loop
            rc = sem_init(&dtm->tm_finish
            sem_wait(&dtm->tm_finish) -> 等信号量 sem_post
  dfuse_fs_fini


dfuse_progress_thread
  rc = sem_wait(&fs_handle->dpi_sem) 等dpi_sem信号
  daos_eq_poll
  daos_event_fini 完成一个事件。 如果事件已被传递到任何 DAOS API，则它只能在从 EQ 中轮询出来时才能完成，即使通过调用 daos_event_abort() 中止也是如此。 如果事件是用父事件初始化的，那么该事件将从父事件的子列表中删除。 如果ev本身是一个父事件，那么这个函数会finalize所有的子事件和ev。
  ev->de_complete_cb(ev) -> dfuse_cb_write_complete 写完成回调

struct fuse_lowlevel_ops dfuse_ops dfuse低层操作对象
  .create		= df_ll_create
	.open		= dfuse_cb_open,
	.release	= dfuse_cb_release,
	.write_buf	= dfuse_cb_write,
	.read		= dfuse_cb_read,
	.readlink	= dfuse_cb_readlink,
	.ioctl		= dfuse_cb_ioctl,

struct fuse_file_info libfuse


客户端写数据：write.c
write -> dfuse_cb_write 回调写 src/client/dfuse/fuse3
  fuse_req_userdata
  fuse_req_ctx
  fuse_buf_size(bufv)
  ibuf = FUSE_BUFVEC_INIT(len) 分配本地缓冲区
  DFUSE_TRA_DEBUG 调试
  fuse_buf_copy(&ibuf, bufv, 0)
  dfuse_cache_evict
  d_slab_acquire 以高效的方式分配数据
  fuse_buf_copy libfuse
  daos_event_init 线程事件初始化
    evx->evx_status	= DAOS_EVS_READY
    D_INIT_LIST_HEAD(&evx->evx_child) 初始化链表
    daos_eq_putref 从事件队列继承传输上下文
  ev->de_complete_cb = dfuse_cb_write_complete 设置回调
  d_iov_set(&ev->de_iov, ibuf.buf[0].mem, len)  # 设置io向量， 将第二参数的地址和长度赋值给第一个参数
  ev->de_sgl.sg_iovs = &ev->de_iov   sgl分散聚集列表
  readahead ie_truncated 预读和截断
  dfs_write (文件系统， 对象，sgl列表，文件(对象)偏移，事件) 将数据写到文件对象
    事件为空
      daos_event_launch -> 将事件移动到事件队列的运行队列
        daos_event_launch_locked(eqx, evx)
          d_list_add_tail(&evx->evx_link, &eq->eq_running)
          eq->eq_n_running++
      daos_event_complete
    daos_event_errno_rc(ev) 将错误码转正
      daos_ev2evx(ev)
    daos_array_write 写数组对象: daos_array_write(obj->oh, DAOS_TX_NONE, &iod, sgl, ev)
      dc_task_create(dc_array_write, NULL, ev, &task) 创建任务
      dc_task_schedule(task, true)  task与args做转换: dc_task_get_args 调度任务
    return daos_der2errno(rc)
  sem_post(&fs_handle->dpi_sem)   解锁信号量(+1,如果大于0,其他线程将被唤醒执行),唤醒线程（线程同步）: dfuse_progress_thread sem_wait(&fs_handle->dpi_sem)


dc_array_write
  daos_task_get_args task和args可互转
  dc_array_io opc = DAOS_OPC_ARRAY_WRITE 操作码是写数组  读:DAOS_OPC_ARRAY_READ
    array_hdl2ptr
    io_extent_same
    D_INIT_LIST_HEAD(&io_task_list)
    daos_task_create(DAOS_OPC_ARRAY_GET_SIZE 短读任务 DAOS_OPC_ARRAY_READ
    while (u < rg_iod->arr_nr) 遍历每个范围，但同时组合属于同一 dkey 的连续范围。 如果用户给出的范围不增加偏移量，则它们可能不会合并，除非分隔范围也属于同一个 dkey
      compute_dkey 计算分布式key 在给定此范围的数组索引的情况下计算 dkey。 还计算从我们开始的索引开始，dkey 可以保存的记录数写作。 相对于 dkey 的记录索引
      struct io_params *prev, *current 如果有多个dkey io, 则通过链表连接起来
      num_ios++
      d_iov_set(dkey, &params->dkey_val, sizeof(uint64_t));
      d_iov_set(&iod->iod_name, &params->akey_val, 1);
      compute_dkey 再次计算dkey
      create_sgl 创建分散聚集列表
      daos_task_create(DAOS_OPC_OBJ_FETCH 读: DAOS_OPC_ARRAY_READ 按索引号 -> dc_obj_fetch_task
      daos_task_create(DAOS_OPC_OBJ_UPDATE 写 或 DAOS_OPC_ARRAY_PUNCH truncate dc_funcs[opc].task_func 客户端方法数组
      daos_task_get_args
      tse_task_register_deps 注册在计划任务之前需要完成的依赖任务。 依赖任务无法进行, 如果一个任务依赖于其他任务，只有依赖的任务完成了，才可以将任务添加到调度器列表中
      tse_task_list_add(io_task, &io_task_list)  d_list_add_tail(&dtp->dtp_task_list, head); 添加任务到链表
    tse_task_register_comp_cb(task, free_io_params_cb, &head, sizeof(head))  为任务注册完成回调
    if (op_type == DAOS_OPC_ARRAY_READ && array->byte_array) 短读
      tse_task_register_deps(task, 1, &stask) 注册依赖任务, 最终是子减父引用
      tse_task_list_add(stask, &io_task_list) 加到io任务列表
    tse_task_list_sched(&io_task_list, false); 批量任务调度执行 -> dc_obj_update_task

    array_decref(array)
    tse_task_register_cbs(stask, check_short_read_cb 读回调
    tse_sched_progress(tse_task2sched(task)) 推进/处理, 在调度程序上取得进展。 运行它们所依赖的任务完成后准备执行的任务。 请注意，使用 tse_task_complete() 完成任务必须由引擎用户完成，以推动引擎的进度。 在 DAOS 中，tse_task_complete 由发送到服务器的 RPC 请求的完成 CB 调用


check_short_read_cb

通过以下对象连接
.cpf_name =
daos_opc_t
dc_funcs[opc].task_func 客户端方法数组

DAOS_OPC_OBJ_UPDATE 写
  dc_obj_update_task DAOS_OBJ_RPC_UPDATE
    obj_req_valid(task, args, DAOS_OBJ_RPC_UPDATE
      obj_auxi = tse_task_stack_push(task, sizeof(*obj_auxi))
      tse_task_stack_pop
    dc_tx_attach(args->th, obj, DAOS_OBJ_RPC_UPDATE, task) 如果事务有效(hdl.cookie == 1), 则走dtx
  dc_obj_update 否则
      obj_task_init_common(task, DAOS_OBJ_RPC_UPDATE
        tse_task_stack_push
        shard_task_list_init(obj_auxi)
      obj_rw_req_reassemb 重新组装
      dkey_hash = obj_dkey2hash
      obj_req_get_tgts 获取对象对应的目标
        obj_dkey2grpmemb
          obj_dkey2grpidx
            pool_map_ver = pool_map_get_version(pool->dp_map)
            grp_size = obj_get_grp_size(obj)
            grp_idx = d_hash_jump(hash, obj->cob_shards_nr / grp_size) how hash generate? obj with pool
        obj_shards_2_fwtgts
          obj_shard_tgts_query 分片目标查询
            obj_shard_open
              dc_obj_shard_open
                pool_map_find_target 二分查找
                  comp_sorter_find_target(sorter, id)
                    daos_array_find
                      array_bin_search
          obj_grp_leader_get
            pl_select_leader obj_get_shard
              array_bin_search 二分查找 daos_obj_classes
      tse_task_register_comp_cb(task, obj_comp_cb, NULL, 0)
      obj_csum_update(obj, args, obj_auxi)
      obj_rw_bulk_prep
      obj_req_fanout(obj, obj_auxi, dkey_hash, map_ver, epoch, shard_rw_prep, dc_obj_shard_rw, task)  扇出 shard_io_cb = io_cb = dc_obj_shard_rw


ds_obj_rw_handler 接收端的回调
  obj_ioc_begin 访问VOS前的各种检查
  obj_rpc_is_fetch
  process_epoch
  obj_rpc_is_fetch
  rc = dtx_begin 返回超时?
    dtx_handle_init
      dtx_shares_init(dth) 初始化以下链表, 提交,中断,活动,检查
      dtx_epoch_bound
      vos_dtx_rsrvd_init(dth)
  obj_local_rw 本地读写


创建任务, daos_task_create
创建一个异步任务并将其与 daos 客户端操作相关联。 对于同步操作，请为该操作使用特定的 API。 通常，此 API 用于需要将一系列 daos 操作排队到 DAOS 异步引擎中的用例，这些任务之间的执行顺序具有特定的依赖性。 例如，用户可以创建任务来打开一个对象，然后使用插入到打开任务更新中的依赖项来更新该对象。 对于更简单的工作流程，用户可以使用基于事件的 API 而不是任务。


dfuse_start
  dfuse_progress_thread


重要结构：

tse_task_t	*io_task = NULL; io任务






ubix:

pool_map
dfuse_cb_write position=0
  dfuse_cb_write_complete 回调
  dfs_write
    dc_array_write
    daos_task_create DAOS_OPC_OBJ_UPDATE
    dc_obj_update_task
    dc_obj_update
      obj_update_shards_get
      obj_rw_bulk_prep
      obj_req_fanout shard_rw_prep dc_obj_shard_rw = shard_io_cb 请求扇出
        io_prep_cb shard_rw_prep 发送io前执行的回调
        shard_io 分片IO
          obj_shard_open
            map_ver
            dc_obj_shard_open 打开分片
            dc_cont_tgt_idx2ptr 根据容器handler和taget索引，获取池的目标pool_target
              pool
              dc_hdl2pool
                daos_hhash_link_lookup(poh.cookie)
              pool_map_find_target
              dc_pool_put
                daos_hhash_link_putref
              dc_cont_put
          shard_auxi->shard_io_cb(obj_shard,...) -> dc_obj_shard_rw 对象分片读写回调
          obj_shard_close



import struct
pool_target


main
dfuse_start
  dfuse_lanuch_fuse
    ll_loop_fn
      dfuse_loop
        start_one
          dfuse_do_work
            fuse_session_process_buf_int libfuse3.so.3
              do_write_buf(req, in->nodeid, inarg, buf)
                se->op.write_buf(req, nodeid, &bufv, arg->offset, &fi)
                dfuse_cb_write step?




dc_obj_shard_rw 客户端对象分片读写(读写对象分片)
  obj_shard_ptr2pool(shard) 根据分片获取池
  obj_req_create opc = DAOS_OBJ_RPC_UPDATE -> ds_obj_rw_handler
  uuid_copy
  daos_dti_copy 拷贝dtx_id
  跳过ec逻辑
  tse_task_register_comp_cb
  daos_rpc_send
    crt_req_send daos_rpc_cb -> tse_task_complete 发送完成回调流程:hg -> crt_hg_req_send_cb -> crp_complete_cb -> -> daos_rpc_cb -> dc_rw_cb

超时检测和业务回调是在不同的线程中并发执行

ds_obj_rw_handler
  obj_rw_reply
    obj_reply_set_status 与 obj_reply_get_status 成对使用
      ((struct obj_rw_out *)reply)->orw_ret = status 设置回复状态


怎么处理超时

crt_swim_cli_cb
ca_arrays


tse 调度
dc_task_create
  sched = daos_ev2sched(ev) 事件转调度器
  tse_task_create 初始化 tse_task。 该任务会被添加到调度器任务列表中，稍后被调度，如果提供了依赖任务，则该任务将被添加到依赖任务的dep列表中，一旦依赖任务完成，则添加该任务 到调度程序列表。
  task_ptr2args 指针转参数
    tse_task_buf_embedded 获取任务的嵌入式缓冲区，用户可以使用它来携带功能参数。 任务的嵌入式缓冲区有大小限制，如果 buf_size 大于限制，此函数将返回 NULL。 用户应通过 tse_task_set_priv() 使用私有数据来传递大参数。 MSC - 我将其更改为只是一个缓冲区，而不是像以前那样, 不断给一个额外的指针指向大的预涂层缓冲区。 以前的方式不适用于公共用途。我们现在应该使它更简单，更通用，如下面的评论
      tse_task_buf_size
        return (size + 7) & ~0x7
  tse_task_register_comp_cb(task, task_comp_event, NULL, 0) dtc->dtc_cb = cb task_comp_event -> 注册任务完成回调
    register_cb(task, true, comp_cb, arg, arg_size)
      d_list_add(&dtc->dtc_list, &dtp->dtp_comp_cb_list) 插入到列表开始处




执行任务的回调，如果所有的CB都执行完则返回true, 并且不重新启动任务。 如果任务被用户重新初始化，则意味着它又在飞行中，所以我们在重新初始化它的当前 CB 处中断，并返回 false，表示任务未完成。 所有剩余的 CB未执行的仍然附加，但已执行的此时已经从列表中删除
tse_task_complete_callback
  ret = dtc->dtc_cb(task, dtc->dtc_arg); task_comp_event




crt_rpc_completed call 2？ 重复完成，duplicated completions

daos_rpc_db
daos_rpc_complete | daos_rpc_cb
  tse_task_complete
    tse_sched_process_complete(dsp)
      post_procee
        tse_task_complete_callback -> 返回true/false
          dtc_cb 执行注册时的回调 register_cb -> dc_rw_cb
          task_comp_event
            event_complete
              complete_locked
                daos_event_complete_cb
                  d_list_for_each_entry_safe
                  d_list_del_init
                  __gurt_list_del

dc_rw_cb 
  opc_get
  DAOS_FAIL_CHECK
dc_obj_shard_rw
  tse_task_register_comp_cb(task, dc_rw_cb...)  dtc->dtc_cb = cb   <- tse_task_complete_callback
    dc_rw_cb
      rc = obj_reply_get_status  DER_SHUTDOWN(-2017) | DER_IO(-2001) 2008 NOTLEADER
        ((struct obj_rw_out *)reply)->orw_ret

dc_task_schedule
  task_is_valid
  daos_event_launch
  tse_task_schedule
    tse_task_schedule_with_delay 与 tse_task_schedule 相同，如果 instant 为 false，则期望任务不会在 delay 微秒内执行。

daos_event_comp_list 事件完成列表


dc_rw_cb DER_NO_HDL 'Invalid handle' 1002 追rc
  opc = OBJ_RPC_UPDATE = 0
  rc = obj_reply_get_status orw_ret




crt_ctx_epi_abort
  crt_rpc_complete


---------------------------------------- DL ----------------------------------------

(dlv) bt
0  0x0000000000e1638f in main.(*storageFormatCmd).Execute
   at ./src/control/cmd/dmg/storage.go:117
1  0x0000000000e09542 in main.parseOpts.func1
   at ./src/control/cmd/dmg/main.go:285
2  0x0000000000de9ea3 in github.com/jessevdk/go-flags.(*Parser).ParseArgs
   at ./src/control/vendor/github.com/jessevdk/go-flags/parser.go:333
3  0x0000000000e079c9 in main.parseOpts
   at ./src/control/cmd/dmg/main.go:292
4  0x0000000000e0998c in main.main
   at ./src/control/cmd/dmg/main.go:307
5  0x0000000000452b92 in runtime.main
   at /usr/lib/golang/src/runtime/proc.go:250
6  0x0000000000485d81 in runtime.goexit
   at /usr/lib/golang/src/runtime/asm_amd64.s:1594
dmg system stop; dmg sys erase; dmg storage format
b main.main -> src/control/cmd/dmg/main.go -> if err := cmd.Execute(args) -> func (cmd *storageFormatCmd) Execute -> b main.(*storageFormatCmd).Execute
StorageFormat 在请求的主机列表中提供的所有主机或所有已配置的主机（如果未明确指定）并发执行存储准备步骤。 该函数会阻塞，直到收到所有结果（成功或失败），并返回一个包含所有主机存储准备操作结果的响应结构
dlv exec /opt/daos/bin/dmg -- storage format  # Format SCM and NVMe storage attached to remote servers.
type storageFormatCmd struct
func (cmd *storageFormatCmd) Execute
  req := &control.StorageFormatReq{Reformat: cmd.Force}
  req.SetHostList(cmd.getHostList())
  func StorageFormat(ctx context.Context, rpcClient UnaryInvoker, req *StorageFormatReq) (*StorageFormatResp, error) -> StorageFormat 在请求的主机列表中提供的所有主机上同时执行存储准备步骤，如果未明确指定，则在所有已配置的主机上执行存储准备步骤。 该函数将阻塞，直到收到所有结果（成功或失败），并返回包含所有主机存储准备操作结果的单个响应结构 -> b StorageFormat
    checkFormatReq(ctx, rpcClient, req) -> 执行一些验证以确定在允许请求中的主机的格式请求之前是否应擦除系统。 目标是防止重新格式化正在运行的系统，同时允许（重新）格式化不作为 MS 副本参与的主机
      SystemQuery(ctx, rpcClient, sysReq) -> func SystemQuery(ctx context.Context, rpcClient UnaryInvoker, req *SystemQueryReq) -> c.config.SystemName daos_server -> func SystemQuery -> func (svc *mgmtSvc) SystemQuery -> 实现为管理服务定义的方法。 通过返回系统成员资格中存储的详细信息来检索系统中 DAOS 排名的状态。 请求列表中提供的排名的详细信息（如果请求排名列表为空，则请求所有成员）。 该控制服务方法由lib/control/system.go中同名的控制API方法触发，并返回所有选定等级的结果
        svc.checkReplicaRequest -> checkReplicaRequest 对必须在 MS 副本上运行的请求执行健全性检查
          checkSystemRequest
          svc.sysdb.CheckReplica() -> raft数据库接口: src/control/system/raft/database.go -> 单元测试 -> src/control/system/raft/database_test.go
        resolveRanks -> raft -> src/control/vendor/github.com/hashicorp/raft/membership.md -> https://github.com/hashicorp/raft/blob/main/membership.md
        svc.membership.Members
    return ctlpb.NewCtlSvcClient(conn).StorageFormat(ctx, pbReq) -> func (c *ControlService) StorageFormat
    ur, err := rpcClient.InvokeUnaryRPC(ctx, req)


(dlv) bt
 0  0x000000000110c952 in github.com/daos-stack/daos/src/control/server.(*ControlService).StorageFormat
    at ./src/control/server/ctl_storage_rpc.go:743
 1  0x0000000000c81689 in github.com/daos-stack/daos/src/control/common/proto/ctl._CtlSvc_StorageFormat_Handler.func1 -> FullMethod: "/ctl.CtlSvc/StorageFormat"
    at ./src/control/common/proto/ctl/ctl_grpc.pb.go:330
 2  0x0000000001125dff in github.com/daos-stack/daos/src/control/server.unaryAccessInterceptor
    at ./src/control/server/interceptors.go:73
 3  0x0000000000c2ea46 in google.golang.org/grpc.chainUnaryInterceptors.func1.1
    at ./src/control/vendor/google.golang.org/grpc/server.go:1129
 4  0x00000000011272d6 in github.com/daos-stack/daos/src/control/server.unaryVersionInterceptor
    at ./src/control/server/interceptors.go:177
 5  0x0000000000c2eb4b in google.golang.org/grpc.chainUnaryInterceptors.func1.1
    at ./src/control/vendor/google.golang.org/grpc/server.go:1132
 6  0x00000000011275ac in github.com/daos-stack/daos/src/control/server.unaryStatusInterceptor
    at ./src/control/server/interceptors.go:206
 7  0x0000000000c2eb4b in google.golang.org/grpc.chainUnaryInterceptors.func1.1
    at ./src/control/vendor/google.golang.org/grpc/server.go:1132
 8  0x00000000011273bc in github.com/daos-stack/daos/src/control/server.unaryErrorInterceptor
    at ./src/control/server/interceptors.go:181
 9  0x0000000000c2eb4b in google.golang.org/grpc.chainUnaryInterceptors.func1.1
    at ./src/control/vendor/google.golang.org/grpc/server.go:1132
10  0x0000000001127b73 in github.com/daos-stack/daos/src/control/server.unaryLoggingInterceptor.func1
    at ./src/control/server/interceptors.go:243
11  0x0000000000c2eb4b in google.golang.org/grpc.chainUnaryInterceptors.func1.1
    at ./src/control/vendor/google.golang.org/grpc/server.go:1132
12  0x0000000000c2e899 in google.golang.org/grpc.chainUnaryInterceptors.func1
    at ./src/control/vendor/google.golang.org/grpc/server.go:1134
13  0x0000000000c814be in github.com/daos-stack/daos/src/control/common/proto/ctl._CtlSvc_StorageFormat_Handler
    at ./src/control/common/proto/ctl/ctl_grpc.pb.go:332
14  0x0000000000c2fdaa in google.golang.org/grpc.(*Server).processUnaryRPC
    at ./src/control/vendor/google.golang.org/grpc/server.go:1295
15  0x0000000000c376f9 in google.golang.org/grpc.(*Server).handleStream
    at ./src/control/vendor/google.golang.org/grpc/server.go:1636
16  0x0000000000c2c5cd in google.golang.org/grpc.(*Server).serveStreams.func1.2
    at ./src/control/vendor/google.golang.org/grpc/server.go:932
17  0x000000000048a501 in runtime.goexit
    at /usr/lib/golang/src/runtime/asm_amd64.s:1594 -> daos_server启动流程-grpc流程

dlv attach `ps aux|grep 'daos_server start'|grep -v grep|awk '{print$2}'`
b github.com/daos-stack/daos/src/control/server.(*ControlService).StorageFormat
func (c *ControlService) StorageFormat
  c.formatMetadata(instances, req.Reformat) -> func (c *ControlService) formatMetadata
    ControlMetadataNeedsFormat -> func (p *Provider) ControlMetadataNeedsFormat()
      HasPath
      NeedsFormat
  instanceErrors, instanceSkips, err := formatScm(ctx, fsr, resp) -> func formatScm
    ei.GetStorage().ScmNeedsFormat() -> func (p *Provider) ScmNeedsFormat()
      createScmFormatRequest -> 区分类型, 比如: case ClassRam:, case ClassDcpm:
      func (f *ScmAdminForwarder) CheckFormat -> f.SendReq("ScmCheckFormat" -> fRes, err = h.scmProvider.CheckFormat(fReq) -> CheckFormat 尝试确定请求中指定的 SCM 是否已格式化。 如果已安装，则假定已格式化。 对于 DCPM，直接检查设备是否存在文件系统
        validateFormatRequest
        p.IsMounted(req.Mountpoint) -> /proc/self/mountinfo
        p.sys.Getfs(req.Dcpm.Device)
    ei.GetStorage().GetScmConfig()
    checkTmpfsMem
    StorageFormatSCM
  formatNvme(ctx, fnr, resp) -> func (sb *spdkBackend) Format -> switch req.Properties.Class -> 根据类型格式化
    return sb.formatAioFile(&req) -> createAioFile
    return sb.formatKdev(&req)
    return sb.formatNvme(&req)
      dl, err := substituteVMDAddresses(sb.log, needDevs, req.BdevCache)
      sb.binding.init(sb.log, spdkOpts) -> func (w *spdkWrapper) init(log logging.Logger, spdkOpts *spdk.EnvOptions) 
        w.suppressOutput -> 抑制输出是一种可怕的黑客攻击，因为 SPDK 对标准输出胡言乱语，导致控制台垃圾邮件并扰乱服务器和特权助手之间的安全通信通道
         w.InitSPDKEnv
          C.daos_spdk_init -> daos_spdk_init(int mem_sz, char *env_ctx, size_t nr_pcil, char **pcil)
            spdk_env_init(&opts) -> 初始化spdk环境(dpdk)
          C.spdk_vmd_init()
      sb.binding.Format(sb.log) -> func (n *NvmeImpl) Format -> 通过SPDK可格式化设备，破坏性操作！ 尝试擦除每个控制器命名空间的 LBA-0。 然后删除每个格式化设备的锁定文件
        collectFormatResults(C.nvme_wipe_namespaces() -> nvme_wipe_namespaces(void)
          spdk_nvme_probe(NULL, NULL, probe_cb, attach_cb, NULL)
          ret->wipe_results = wipe_ctrlrs() -> wipe_ctrlr(centry)
            spdk_pci_addr_fmt(res->ctrlr_pci_addr, sizeof(res->ctrlr_pci_addr), &centry->pci_addr)
            qpair = spdk_nvme_ctrlr_alloc_io_qpair(centry->ctrlr, NULL, 0)
            buf =  spdk_dma_zmalloc(4096, 4096, NULL)
            rc = spdk_nvme_ns_cmd_write(nentry->ns, qpair,
					    buf, 0 /** LBA start */,
					    4096 / sector_size /** #LBAS */,
					    write_complete, &data, 0)  -> 将LBA0写0
            spdk_nvme_ctrlr_free_io_qpair(qpair)
        resultPCIAddresses(results)
        cleanLockfiles(log, realRemove, pciAddrs...) -> const lockfilePathPrefix = "/var/tmp/spdk_pci_lock_" -> 删除锁文件
  ei.NotifyStorageReady()

格式化后, 在scm目录下增加以下文件集目录: 
NEWBORNS  ZOMBIES  control_raft  daos_nvme.conf  daos_sys  superblock

CreateDatabaseConfig -> control_raft
vos_db_init_ex -> daos_sys
superblockPath() -> superblock
---------------------------------------- DL ----------------------------------------


dc_pool_query


daos_event_init
  D_CASSERT
  D_INIT_LIST_HEAD(&evx->evx_child);
	D_INIT_LIST_HEAD(&evx->evx_link);
	D_INIT_LIST_HEAD(&evx->evx_callback.evx_comp_list);
  eqx = daos_eq_lookup(eqh)
    hlink = daos_hhash_link_lookup(eqh.cookie)
      d_hhash_link_lookup(daos_ht.dht_hhash, key)
  daos_eq_putref(eqx)



daos_hhash_init_feats
  d_hhash_create dht_hhash


dfuse_fs_init
  daos_eq_create
    daos_eq_insert



读
dc_array_read
  dc_array_io DAOS_OPC_ARRAY_READ



读
dc_obj_fetch_task
  obj_req_valid 校验请求
  obj_task_init
  obj_req_with_cond_flags
    obj_cond_fetch_prep 预处理, 将 obj 任务拆分为多个子任务
  obj_fetch_shards_get
  obj_shards_2_fwtgts
  obj_csum_fetch
  obj_rw_bulk_prep
  obj_req_fanout shard_rw_prep dc_obj_shard_rw 请求扇出


重复完成 coredump
dfuse_ops
.lookup		= df_ll_lookup 按名称查找目录条目并获取其属性
df_ll_lookup
  // 查inodes表
  d_hash_rec_find(&fs_handle->dpi_iet, &parent, sizeof(parent))  d_hash_rec_insert(&fs_handle->dpi_iet 插入hash表
  parent_inode->ie_dfs->dfs_ops->lookup(req, parent_inode, name) 调用父节点的查找方法
    dfuse_cb_lookup
  d_hash_rec_decref(&fs_handle->dpi_iet, rlink)

dfuse_cb_getattr 先获取属性
dfuse_dfs_ops
.lookup		= dfuse_cb_lookup
分配ie
dfs_lookupx 查询条目,获取属性, 生成?
  dfs_lookup_rel_int
    check_name 检查文件名和文件名长度
    get_daos_obj_mode 返回对象模式, 只读|读写
    fetch_entry 获取条目/查条目
      d_iov_set(&dkey, (void *)name, len)
      d_iov_set(&iod->iod_name, INODE_AKEY_NAME
      DAOS_IOD_ARRAY
      sgl->sg_iovs	= sg_iovs
      daos_obj_fetch 从数组中查询对象 ioms: 存储缓冲层(接收缓冲区)
        dc_obj_fetch_task_create 创建获取任务
          DAOS_API_ARG_ASSERT(*args, OBJ_FETCH) 通过断言检查参数预定义大小和传入的参数大小
          dc_task_create(dc_obj_fetch_task, tse, ev, task) dc_obj_fetch_task -> func -> task_func -> dtp_func daos任务私有回调
            daos_event_priv_get(&ev) 获取基于线程的私有事件, static __thread daos_event_t	ev_thpriv; 线程私有数据
            sched = daos_ev2sched(ev)
            tse_task_create
        dc_task_schedule
    switch (entry.mode & S_IFMT) 判断条目类型 文件,符号链接, 
      daos_array_open_with_attr 普通文件 S_IFREG
        dc_array_open
          daos_task_create(DAOS_OPC_OBJ_OPEN 创建对象打开任务, 创建一个异步任务并将其与 daos 客户端操作相关联。 对于同步操作，请为该操作使用特定的 API。 通常，此 API 用于需要将一系列 daos 操作排队到 DAOS 异步引擎中的用例，这些任务之间的执行顺序具有特定的依赖性。 例如，用户可以创建任务来打开一个对象，然后使用插入到打开任务更新中的依赖项来更新该对象。 对于更简单的工作流程，用户可以使用基于事件的 API 而不是任务
          tse_task_register_deps 注册依赖任务
            for num_deps 遍历依赖任务数量
              tse_task_add_dependent(task, dep_tasks[i]) -> 主任务和依赖任务可以是不同的调度器, 跨调度器时需要锁依赖任务调度器上的锁
                Add dependent 主任务 -> 依赖任务
                tlink->tl_task = task -> 依赖任务链接到主任务
                  d_list_add_tail(&tlink->tl_link, &dep_dtp->dtp_dep_list) 添加到依赖链表
          tse_task_register_comp_cb(task, open_handle_cb 注册打开完成回调
          tse_task_schedule(open_task, false) 调度任务, 仅添加到初始队列
          tse_sched_progress(tse_task2sched(task)) 推进任务
          daos_task_create(DAOS_OPC_OBJ_FETCH 查看元数据
atomic_store_relaxed(&ie->ie_ref, 1); 初始化引用计数 原子操作
dfs_obj2id 文件系统转daos对象 128位(32+96)
dfuse_compute_inode
S_ISDIR 目录
  check_for_uns_ep 统一命名空间
dfuse_reply_entry

io故障

打开回调
open_handle_cb
  open_with_attr = 1
  array_hdl_link(array) 
  *args->oh = array_ptr2hdl(array)
    daos_hhash_link_lookup(oh.cookie) 根据key查找hash表


dc_array_set_size


0x00001 ----> oh



dc_array_get_size
dc_array_stat
DAOS_OPC_OBJ_QUERY_KEY DAOS_OBJ_RPC_QUERY_KEY
dc_obj_query_key
  obj_task_init(api_task, DAOS_OBJ_RPC_QUERY_KEY
  queue_shard_query_key_task
    shard_query_key_task // 客户端pool map err
    dc_obj_shard_query_key
      obj_shard_query_key_cb


opc=0x4090009 DAOS_OBJ_RPC_QUERY_KEY 查键
ds_obj_query_key_handler_1

全局rpc操作码: cg_opc_map


open shard
obj_shard_open 18


shard_query_key_task

daos_task_create(DAOS_OPC_OBJ_QUERY_KEY


dc_obj_open_task_create
  dc_task_create(dc_obj_open, "obj open")
  


obj_rw_req_reassemb 重新组装对象读写请求


终结/销毁流程
daos_eq_lib_fini / daos_eq_destroy / 
tse_sched_complete(&sched, 0, true) -> 销毁调度器, 等待调度程序中的所有任务完成并终结。 如果另一个线程正在完成调度程序，则立即返回, 终止或销毁中需要取消调度器中所有的任务
  if (dsp->dsp_cancelling)
    tse_sched_complete_inflight
      d_list_for_each_entry_safe(dtp, tmp, &dsp->dsp_running_list -> 将任务从运行队列移除
  tse_sched_complete_cb(sched) -> MSC - 我们可能只需要 1 个完成 cb 而不是列表




#0  tse_sched_run (sched=sched@entry=0x7f91aedd6ba0 <daos_sched_g>) at src/common/tse.c:755
#1  0x00007f91ae789065 in tse_sched_progress (sched=0x7f91aedd6ba0 <daos_sched_g>) at src/common/tse.c:791
#2  0x00007f91aea1033f in ev_progress_cb (arg=arg@entry=0x7f919fffe180) at src/client/api/event.c:516
#3  0x00007f91acca2eb2 in crt_progress_cond (crt_ctx=0x5602ff224b00, timeout=timeout@entry=0, cond_cb=cond_cb@entry=0x7f91aea1032b <ev_progress_cb>, arg=arg@entry=0x7f919fffe180) at src/cart/crt_context.c:1648
#4  0x00007f91aea1a2f3 in daos_event_priv_wait () at src/client/api/event.c:1276
#5  0x00007f91aea2793c in dc_task_schedule (task=<optimized out>, instant=instant@entry=true) at src/client/api/task.c:124
#6  0x00007f91aea209c1 in daos_obj_open (coh=..., oid=..., mode=mode@entry=2, oh=oh@entry=0x7f919fffe200, ev=ev@entry=0x0) at src/client/api/object.c:49
#7  0x00007f91ae2faeb5 in dfs_ostat (dfs=0x5602ff6f09e0, obj=0x5602ff6efb80, stbuf=stbuf@entry=0x7f919fffe250) at src/client/dfs/dfs.c:4928
#8  0x00005602fd14ed99 in dfuse_cb_getattr (req=0x7f9198000c10, ie=0x5602ff6ef960) at src/client/dfuse/ops/fgetattr.c:22
#9  0x00005602fd1429fc in df_ll_getattr (req=0x7f9198000c10, ino=<optimized out>, fi=<optimized out>) at src/client/dfuse/dfuse_fuseops.c:204
#10 0x00007f91adc48009 in do_getattr () from /lib64/libfuse3.so.3
#11 0x00007f91adc49dba in fuse_session_process_buf_int () from /lib64/libfuse3.so.3
#12 0x00005602fd14599a in dfuse_do_work (arg=0x5602ff6f0090) at src/client/dfuse/dfuse_thread.c:60
#13 0x00007f91aeddf1ca in start_thread () from /lib64/libpthread.so.0
#14 0x00007f91ad646e73 in clone () from /lib64/libc.so.6



dc_rw_cb
  ...
  dc_shard_update_size
  daos_sgls_copy_data_out
  rw_cb_csum_verify
  crt_req_decref(rw_args->rpc)
  return ret



dmg pool list -v
DEBUG 02:45:41.071975 interceptors.go:239: gRPC request: *mgmt.ListPoolsReq (sys:"daos_server-2.5.100")
DEBUG 02:45:41.072511 interceptors.go:262: gRPC response for *mgmt.ListPoolsReq: *mgmt.ListPoolsResp6 1 pools: sxb:Ready (elapsed: 33.46µs)
DEBUG 02:45:41.094068 interceptors.go:239: gRPC request: *mgmt.PoolQueryReq (sys:"daos_server-2.5.100"  id:"e65bab05-f895-4f28-aad1-4967908509bd")

#0  crt_reply_send (req=req@entry=0x7f4126118a90) at src/cart/crt_rpc.c:1472
#1  0x00007f41356676b5 in ds_pool_tgt_query_handler (rpc=0x7f4126118a90) at src/pool/srv_target.c:1550 -> X(POOL_TGT_QUERY
#2  0x00007f4141c0b437 in crt_handle_rpc (arg=0x7f4126118a90) at src/cart/crt_rpc.c:1686 -> rpc_priv->crp_opc_info->coi_rpc_cb(rpc_pub)
#3  0x00007f4140d3e77a in ABTD_ythread_func_wrapper (p_arg=0x7f411d265260) at arch/abtd_ythread.c:21
#4  0x00007f4140d3e911 in make_fcontext () at arch/fcontext/make_x86_64_sysv_elf_gas.S:64

查池
ds_pool_tgt_query_handler
  crt_reply_send




ds_pool_query_handler_v5 <- POOL_QUERY


obj_req_fanout
  tse_task_create(shard_io_task





(gdb) bt
#0  task_comp_event (task=0x7f11b40081e0, data=0x7f11b4004190) at src/client/api/task.c:37
#1  0x00007f11c920210c in tse_task_complete_callback (task=task@entry=0x7f11b40081e0) at src/common/tse.c:520
#2  0x00007f11c9207be9 in tse_task_complete (task=0x7f11b40081e0, ret=<optimized out>) at src/common/tse.c:870
#3  0x00007f11c94a3f48 in daos_rpc_cb (cb_info=<optimized out>) at src/client/api/rpc.c:24
#4  0x00007f11c7762d8a in crt_hg_req_send_cb (hg_cbinfo=<optimized out>) at src/cart/crt_hg.c:1364
#5  0x00007f11c690a4ce in hg_core_forward_cb (callback_info=<optimized out>) at /home/xb/project/stor/daos/main/daos/build/external/debug/mercury/src/mercury.c:990
#6  0x00007f11c691a034 in hg_core_trigger_entry (hg_core_handle=0x563d4ca28340) at /home/xb/project/stor/daos/main/daos/build/external/debug/mercury/src/mercury_core.c:5406
#7  hg_core_trigger (context=0x563d4c88d500, timeout_ms=timeout_ms@entry=0, max_count=max_count@entry=256, actual_count_p=actual_count_p@entry=0x7f11bb1f9a54) at /home/xb/project/stor/daos/main/daos/build/external/debug/mercury/src/mercury_core.c:5267
#8  0x00007f11c6922dbb in HG_Core_trigger (context=<optimized out>, timeout=timeout@entry=0, max_count=max_count@entry=256, actual_count_p=actual_count_p@entry=0x7f11bb1f9a54)
    at /home/xb/project/stor/daos/main/daos/build/external/debug/mercury/src/mercury_core.c:6419
#9  0x00007f11c690e48e in HG_Trigger (context=context@entry=0x563d4c82ab70, timeout=timeout@entry=0, max_count=max_count@entry=256, actual_count_p=actual_count_p@entry=0x7f11bb1f9a54)
    at /home/xb/project/stor/daos/main/daos/build/external/debug/mercury/src/mercury.c:2197
#10 0x00007f11c77700e9 in crt_hg_progress (hg_ctx=hg_ctx@entry=0x563d4c817b18, timeout=timeout@entry=0) at src/cart/crt_hg.c:1540
#11 0x00007f11c7720eea in crt_progress_cond (crt_ctx=0x563d4c817b00, timeout=timeout@entry=0, cond_cb=cond_cb@entry=0x7f11c948e32b <ev_progress_cb>, arg=arg@entry=0x7f11bb1f9b00) at src/cart/crt_context.c:1668
#12 0x00007f11c94982f3 in daos_event_priv_wait () at src/client/api/event.c:1276
#13 0x00007f11c94a593c in dc_task_schedule (task=<optimized out>, instant=instant@entry=true) at src/client/api/task.c:124
#14 0x00007f11c949ee72 in daos_obj_fetch (oh=..., oh@entry=..., th=..., th@entry=..., flags=flags@entry=8, dkey=dkey@entry=0x7f11bb1f9c30, nr=nr@entry=1, iods=<optimized out>, sgls=0x7f11bb1f9c10, maps=0x0, ev=0x0) at src/client/api/object.c:170
#15 0x00007f11c8d5e7b7 in fetch_entry (ver=<optimized out>, oh=..., th=..., th@entry=..., name=<optimized out>, name@entry=0x7f1190001f30 "file", len=len@entry=4, fetch_sym=fetch_sym@entry=false, exists=0x7f11bb1f9e47, entry=0x7f11bb1f9e60, xnr=0, 
    xnames=0x0, xvals=0x0, xsizes=0x0) at src/client/dfs/dfs.c:658
#16 0x00007f11c8d645a2 in entry_stat (dfs=dfs@entry=0x563d4cce39e0, th=th@entry=..., oh=..., name=name@entry=0x7f1190001f30 "file", len=len@entry=4, obj=obj@entry=0x7f1190001f00, get_size=<optimized out>, stbuf=<optimized out>, obj_hlc=<optimized out>)
    at src/client/dfs/dfs.c:1000
#17 0x00007f11c8d7b40e in dfs_osetattr (dfs=0x563d4cce39e0, obj=0x7f1190001f00, stbuf=stbuf@entry=0x7f11bb1fa2f0, flags=flags@entry=4) at src/client/dfs/dfs.c:5344
#18 0x0000563d4bc31967 in dfuse_cb_setattr (req=0x7f11b4007f50, ie=0x7f1190000bf0, attr=0x7f11bb1fa2f0, to_set=0) at src/client/dfuse/ops/setattr.c:108
#19 0x0000563d4bbfeba7 in df_ll_setattr (req=0x7f11b4007f50, ino=<optimized out>, attr=0x7f11bb1fa2f0, to_set=1056, fi=<optimized out>) at src/client/dfuse/dfuse_fuseops.c:245
#20 0x00007f11c86c5f6a in do_setattr () from /lib64/libfuse3.so.3
#21 0x00007f11c86c7dba in fuse_session_process_buf_int () from /lib64/libfuse3.so.3
#22 0x0000563d4bc0299a in dfuse_do_work (arg=0x563d4cce3090) at src/client/dfuse/dfuse_thread.c:60
#23 0x00007f11c985d1ca in start_thread () from /lib64/libpthread.so.0
#24 0x00007f11c80c4e73 in clone () from /lib64/libc.so.6
(gdb) c
Continuing.

Thread 6 "dfuse_worker" hit Breakpoint 3, task_comp_event (task=0x7f11b40081e0, data=0x7f11b4004190) at src/client/api/task.c:37
(gdb) bt
#0  task_comp_event (task=0x7f11b40081e0, data=0x7f11b4004190) at src/client/api/task.c:37
#1  0x00007f11c920210c in tse_task_complete_callback (task=task@entry=0x7f11b40081e0) at src/common/tse.c:520
#2  0x00007f11c9202f54 in tse_task_post_process (task=task@entry=0x7f11b40086e0) at src/common/tse.c:675
#3  0x00007f11c9206833 in tse_sched_process_complete (dsp=dsp@entry=0x7f11c9854bb0 <daos_sched_g+16>) at src/common/tse.c:727
#4  0x00007f11c9206fe5 in tse_sched_run (sched=sched@entry=0x7f11c9854ba0 <daos_sched_g>) at src/common/tse.c:763
#5  0x00007f11c9207065 in tse_sched_progress (sched=0x7f11c9854ba0 <daos_sched_g>) at src/common/tse.c:791
#6  0x00007f11c948e33f in ev_progress_cb (arg=<optimized out>) at src/client/api/event.c:516
#7  0x00007f11c7721346 in crt_progress_cond (crt_ctx=0x563d4c817b00, timeout=timeout@entry=0, cond_cb=cond_cb@entry=0x7f11c948e32b <ev_progress_cb>, arg=arg@entry=0x7f11bb1f9da0) at src/cart/crt_context.c:1675
#8  0x00007f11c94982f3 in daos_event_priv_wait () at src/client/api/event.c:1276
#9  0x00007f11c94a593c in dc_task_schedule (task=<optimized out>, instant=instant@entry=true) at src/client/api/task.c:124
#10 0x00007f11c9487aec in daos_array_stat (oh=..., th=th@entry=..., stbuf=stbuf@entry=0x7f11bb1f9e50, ev=ev@entry=0x0) at src/client/api/array.c:261
#11 0x00007f11c8d648d8 in entry_stat (dfs=dfs@entry=0x563d4cce39e0, th=th@entry=..., oh=..., name=name@entry=0x7f1190001f30 "file", len=len@entry=4, obj=obj@entry=0x7f1190001f00, get_size=<optimized out>, stbuf=<optimized out>, obj_hlc=<optimized out>)
    at src/client/dfs/dfs.c:1060
#12 0x00007f11c8d7b40e in dfs_osetattr (dfs=0x563d4cce39e0, obj=0x7f1190001f00, stbuf=stbuf@entry=0x7f11bb1fa2f0, flags=flags@entry=4) at src/client/dfs/dfs.c:5344 -> 设置文件的状态属性并获取新值。 如果对象是符号链接，则链接本身会被修改。 请参阅 dfs_stat() 填写的条目
#13 0x0000563d4bc31967 in dfuse_cb_setattr (req=0x7f11b4007f50, ie=0x7f1190000bf0, attr=0x7f11bb1fa2f0, to_set=0) at src/client/dfuse/ops/setattr.c:108
#14 0x0000563d4bbfeba7 in df_ll_setattr (req=0x7f11b4007f50, ino=<optimized out>, attr=0x7f11bb1fa2f0, to_set=1056, fi=<optimized out>) at src/client/dfuse/dfuse_fuseops.c:245
#15 0x00007f11c86c5f6a in do_setattr () from /lib64/libfuse3.so.3
#16 0x00007f11c86c7dba in fuse_session_process_buf_int () from /lib64/libfuse3.so.3
#17 0x0000563d4bc0299a in dfuse_do_work (arg=0x563d4cce3090) at src/client/dfuse/dfuse_thread.c:60
#18 0x00007f11c985d1ca in start_thread () from /lib64/libpthread.so.0
#19 0x00007f11c80c4e73 in clone () from /lib64/libc.so.6
(gdb) c
Continuing.

Thread 6 "dfuse_worker" hit Breakpoint 3, task_comp_event (task=0x7f11b40086e0, data=0x7f11b4004190) at src/client/api/task.c:37
(gdb) bt
#0  task_comp_event (task=0x7f11b40086e0, data=0x7f11b4004190) at src/client/api/task.c:37
#1  0x00007f11c920210c in tse_task_complete_callback (task=task@entry=0x7f11b40086e0) at src/common/tse.c:520
#2  0x00007f11c9207be9 in tse_task_complete (task=0x7f11b40086e0, ret=<optimized out>) at src/common/tse.c:870
#3  0x00007f11c94a3f48 in daos_rpc_cb (cb_info=<optimized out>) at src/client/api/rpc.c:24
#4  0x00007f11c7762d8a in crt_hg_req_send_cb (hg_cbinfo=<optimized out>) at src/cart/crt_hg.c:1364
#5  0x00007f11c690a4ce in hg_core_forward_cb (callback_info=<optimized out>) at /home/xb/project/stor/daos/main/daos/build/external/debug/mercury/src/mercury.c:990
#6  0x00007f11c691a034 in hg_core_trigger_entry (hg_core_handle=0x563d4ca1d640) at /home/xb/project/stor/daos/main/daos/build/external/debug/mercury/src/mercury_core.c:5406
#7  hg_core_trigger (context=0x563d4c88d500, timeout_ms=timeout_ms@entry=0, max_count=max_count@entry=256, actual_count_p=actual_count_p@entry=0x7f11bb1f9e04) at /home/xb/project/stor/daos/main/daos/build/external/debug/mercury/src/mercury_core.c:5267
#8  0x00007f11c6922dbb in HG_Core_trigger (context=<optimized out>, timeout=timeout@entry=0, max_count=max_count@entry=256, actual_count_p=actual_count_p@entry=0x7f11bb1f9e04)
    at /home/xb/project/stor/daos/main/daos/build/external/debug/mercury/src/mercury_core.c:6419
#9  0x00007f11c690e48e in HG_Trigger (context=context@entry=0x563d4c82ab70, timeout=timeout@entry=0, max_count=max_count@entry=256, actual_count_p=actual_count_p@entry=0x7f11bb1f9e04)
    at /home/xb/project/stor/daos/main/daos/build/external/debug/mercury/src/mercury.c:2197
#10 0x00007f11c77700e9 in crt_hg_progress (hg_ctx=hg_ctx@entry=0x563d4c817b18, timeout=timeout@entry=0) at src/cart/crt_hg.c:1540
#11 0x00007f11c7720eea in crt_progress_cond (crt_ctx=0x563d4c817b00, timeout=timeout@entry=0, cond_cb=cond_cb@entry=0x7f11c948e32b <ev_progress_cb>, arg=arg@entry=0x7f11bb1f9eb0) at src/cart/crt_context.c:1668
#12 0x00007f11c94982f3 in daos_event_priv_wait () at src/client/api/event.c:1276
#13 0x00007f11c94a593c in dc_task_schedule (task=<optimized out>, instant=instant@entry=true) at src/client/api/task.c:124
#14 0x00007f11c949eecb in daos_obj_update (oh=..., th=..., th@entry=..., flags=flags@entry=4, dkey=dkey@entry=0x7f11bb1f9f70, nr=nr@entry=1, iods=iods@entry=0x7f11bb1f9f90, sgls=0x7f11bb1f9f60, ev=0x0) at src/client/api/object.c:186
#15 0x00007f11c8d7b99b in dfs_osetattr (dfs=<optimized out>, obj=<optimized out>, stbuf=stbuf@entry=0x7f11bb1fa2f0, flags=0, flags@entry=4) at src/client/dfs/dfs.c:5490
#16 0x0000563d4bc31967 in dfuse_cb_setattr (req=0x7f11b4007f50, ie=0x7f1190000bf0, attr=0x7f11bb1fa2f0, to_set=0) at src/client/dfuse/ops/setattr.c:108
#17 0x0000563d4bbfeba7 in df_ll_setattr (req=0x7f11b4007f50, ino=<optimized out>, attr=0x7f11bb1fa2f0, to_set=1056, fi=<optimized out>) at src/client/dfuse/dfuse_fuseops.c:245
#18 0x00007f11c86c5f6a in do_setattr () from /lib64/libfuse3.so.3
#19 0x00007f11c86c7dba in fuse_session_process_buf_int () from /lib64/libfuse3.so.3
#20 0x0000563d4bc0299a in dfuse_do_work (arg=0x563d4cce3090) at src/client/dfuse/dfuse_thread.c:60
#21 0x00007f11c985d1ca in start_thread () from /lib64/libpthread.so.0
#22 0x00007f11c80c4e73 in clone () from /lib64/libc.so.6
(gdb) c
Continuing.
[Switching to Thread 0x7f11bb9fc700 (LWP 374)]

Thread 5 "dfuse_progress" hit Breakpoint 3, task_comp_event (task=0x7f11ac004cf0, data=0x7f11ac002760) at src/client/api/task.c:37
(gdb) bt
#0  task_comp_event (task=0x7f11ac004cf0, data=0x7f11ac002760) at src/client/api/task.c:37
#1  0x00007f11c920210c in tse_task_complete_callback (task=task@entry=0x7f11ac004cf0) at src/common/tse.c:520
#2  0x00007f11c9202f54 in tse_task_post_process (task=task@entry=0x7f11ac005a50) at src/common/tse.c:675
#3  0x00007f11c9206833 in tse_sched_process_complete (dsp=dsp@entry=0x563d4ca41270) at src/common/tse.c:727
#4  0x00007f11c9207fd0 in tse_task_complete (task=task@entry=0x7f11ac005a50, ret=ret@entry=0) at src/common/tse.c:885
#5  0x00007f11c95e65fb in check_short_read_cb (task=0x7f11ac005a50, data=<optimized out>) at src/client/array/dc_array.c:1407
#6  0x00007f11c91fe2cc in tse_task_prep_callback (task=task@entry=0x7f11ac005a50) at src/common/tse.c:478
#7  0x00007f11c920437c in tse_sched_process_init (dsp=dsp@entry=0x563d4ca41270) at src/common/tse.c:595
#8  0x00007f11c9206fdb in tse_sched_run (sched=sched@entry=0x563d4ca41260) at src/common/tse.c:762
#9  0x00007f11c9207065 in tse_sched_progress (sched=sched@entry=0x563d4ca41260) at src/common/tse.c:791
#10 0x00007f11c948f93a in eq_progress_cb (arg=0x7f11bb9fb0a0) at src/client/api/event.c:669
#11 0x00007f11c7721346 in crt_progress_cond (crt_ctx=0x563d4ca41450, timeout=timeout@entry=-1, cond_cb=cond_cb@entry=0x7f11c948f922 <eq_progress_cb>, arg=arg@entry=0x7f11bb9fb0a0) at src/cart/crt_context.c:1675
#12 0x00007f11c9493c74 in daos_eq_poll (eqh=..., wait_running=wait_running@entry=1, timeout=timeout@entry=-1, n_events=n_events@entry=128, events=events@entry=0x7f11bb9fb100) at src/client/api/event.c:745
#13 0x0000563d4bbe6d6e in dfuse_progress_thread (arg=0x563d4ca2f130) at src/client/dfuse/dfuse_core.c:56
#14 0x00007f11c985d1ca in start_thread () from /lib64/libpthread.so.0
#15 0x00007f11c80c4e73 in clone () from /lib64/libc.so.6




#0  crt_req_send (req=0x7f11ac006310, complete_cb=complete_cb@entry=0x7f11c94a3f35 <daos_rpc_cb>, arg=arg@entry=0x7f11ac004cf0) at src/cart/crt_rpc.c:1377
#1  0x00007f11c94a4586 in daos_rpc_send (rpc=<optimized out>, task=task@entry=0x7f11ac004cf0) at src/client/api/rpc.c:44
#2  0x00007f11c955cfc5 in dc_obj_shard_rw (shard=0x7f11ac003508, opc=DAOS_OBJ_RPC_FETCH, shard_args=<optimized out>, fw_shard_tgts=0x0, fw_cnt=<optimized out>, task=0x7f11ac004cf0) at src/object/cli_shard.c:1161
#3  0x00007f11c952f779 in shard_io (task=task@entry=0x7f11ac004cf0, shard_auxi=shard_auxi@entry=0x7f11ac005060) at src/object/cli_obj.c:2890
#4  0x00007f11c952fe03 in obj_req_fanout (obj=<optimized out>, obj_auxi=0x7f11ac004e30, map_ver=<optimized out>, epoch=epoch@entry=0x7f11ba9f90e0, io_prep_cb=io_prep_cb@entry=0x7f11c9521698 <shard_rw_prep>, io_cb=<optimized out>, obj_task=<optimized out>)
    at src/object/cli_obj.c:3090
#5  0x00007f11c954a0d7 in dc_obj_fetch_task (task=0x7f11ac004cf0) at src/object/cli_obj.c:5522
#6  0x00007f11c92088ad in tse_task_schedule_with_delay (task=task@entry=0x7f11ac004cf0, instant=instant@entry=true, delay=delay@entry=0) at src/common/tse.c:1056
#7  0x00007f11c9208cba in tse_task_schedule (task=task@entry=0x7f11ac004cf0, instant=instant@entry=true) at src/common/tse.c:1070
#8  0x00007f11c94a5780 in dc_task_schedule (task=0x7f11ac004cf0, instant=instant@entry=true) at src/client/api/task.c:114
#9  0x00007f11c949ee72 in daos_obj_fetch (oh=..., th=..., th@entry=..., flags=flags@entry=64, dkey=dkey@entry=0x7f11ba9f9250, nr=nr@entry=1, iods=iods@entry=0x7f11ba9f9270, sgls=0x0, maps=0x0, ev=0x0) at src/client/api/object.c:170
#10 0x00007f11c8d7ee3e in dfs_getxattr (dfs=0x563d4cce39e0, obj=<optimized out>, name=name@entry=0x7f11b97f5040 "security.capability", value=value@entry=0x0, size=size@entry=0x7f11ba9f9308) at src/client/dfs/dfs.c:6228
#11 0x0000563d4bc0d57d in dfuse_cb_getxattr (req=0x7f11ac0036b0, inode=0x563d4cce2960, name=<optimized out>, size=24) at src/client/dfuse/ops/getxattr.c:62
#12 0x0000563d4bbfc8b9 in df_ll_getxattr (req=0x7f11ac0036b0, ino=<optimized out>, name=0x7f11b97f5040 "security.capability", size=24) at src/client/dfuse/dfuse_fuseops.c:522
#13 0x00007f11c86c7dba in fuse_session_process_buf_int () from /lib64/libfuse3.so.3
#14 0x0000563d4bc0299a in dfuse_do_work (arg=0x563d4cccad20) at src/client/dfuse/dfuse_thread.c:60
#15 0x00007f11c985d1ca in start_thread () from /lib64/libpthread.so.0
#16 0x00007f11c80c4e73 in clone () from /lib64/libc.so.6




(gdb) bt
#0  task_comp_event (task=0x7f0210006660, data=0x7f0210004510) at src/client/api/task.c:37
#1  0x00007f0225b4210c in tse_task_complete_callback (task=task@entry=0x7f0210006660) at src/common/tse.c:520
#2  0x00007f0225b47be9 in tse_task_complete (task=task@entry=0x7f0210006660, ret=ret@entry=0) at src/common/tse.c:870
#3  0x00007f0225e857fe in dc_obj_open (task=0x7f0210006660) at src/object/cli_obj.c:1521
#4  0x00007f0225b488ad in tse_task_schedule_with_delay (task=task@entry=0x7f0210006660, instant=instant@entry=true, delay=delay@entry=0) at src/common/tse.c:1056
#5  0x00007f0225b48cba in tse_task_schedule (task=task@entry=0x7f0210006660, instant=instant@entry=true) at src/common/tse.c:1070
#6  0x00007f0225de5780 in dc_task_schedule (task=0x7f0210006660, instant=instant@entry=true) at src/client/api/task.c:114
#7  0x00007f0225dde9c1 in daos_obj_open (coh=..., coh@entry=..., oid=..., mode=mode@entry=4, oh=oh@entry=0x7f0210006b88, ev=ev@entry=0x0) at src/client/api/object.c:49
#8  0x00007f0225f204fb in dc_array_g2l (coh=..., array_glob=array_glob@entry=0x7f02177fce80, mode=mode@entry=4, oh=oh@entry=0x7f0210006c80) at src/client/array/dc_array.c:400
#9  0x00007f0225f2a479 in dc_array_global2local (coh=..., glob=..., mode=mode@entry=4, oh=oh@entry=0x7f0210006c80) at src/client/array/dc_array.c:463
#10 0x00007f0225dc7758 in daos_array_global2local (coh=..., glob=..., mode=mode@entry=4, oh=oh@entry=0x7f0210006c80) at src/client/api/array.c:108
#11 0x00007f02256b6037 in dfs_dup (dfs=0x557cdf4a79e0, obj=0x7f01f0001440, flags=49666, _new_obj=_new_obj@entry=0x7f0210005658) at src/client/dfs/dfs.c:4363
#12 0x0000557cdec05748 in dfuse_cb_open (req=0x7f0210005c30, ino=<optimized out>, fi=0x7f02177fd360) at src/client/dfuse/ops/open.c:46
#13 0x00007f0225007248 in do_open () from /lib64/libfuse3.so.3
#14 0x00007f0225007dba in fuse_session_process_buf_int () from /lib64/libfuse3.so.3
#15 0x0000557cdebe999a in dfuse_do_work (arg=0x557cdf4a7090) at src/client/dfuse/dfuse_thread.c:60
#16 0x00007f022619d1ca in start_thread () from /lib64/libpthread.so.0
#17 0x00007f0224a04e73 in clone () from /lib64/libc.so.6


依赖任务:
tse_task_register_deps(task, 1, &io_task)



spdk_daos_bdev:
daos_bdev:
_bdev_daos_submit_request
  case SPDK_BDEV_IO_TYPE_WRITE
    bdev_daos_writev
      daos_event_init
      d_iov_set
      dfs_write(ch->dfs, ch->obj, &task->sgl, offset, &task->ev)




dfs test:
src/tests/simple_dfs.c -> main


export POOL_SCM_SIZE=1
export POOL_NVME_SIZE=4
cd /opt/daos/bin; ./dfs_test -s     # unit_test -> gdb --args ./dfs_test -u
src/tests/suite/dfs_test.c -> main
par_init(int *argc, char ***argv) -> DAOS-7734 版本：将 MPI 代码包装在统一接口中 (#8127)，将 daos 测试代码与序列化存根库链接，以避免与 MPI 直接链接。 使用 dlopen，此存根库可以尝试加载启用 MPI 的版本，但否则可以在没有 MPI 的情况下执行相同的应用程序。 这简化了我们的包，并允许我们在包中提供更多实用程序，而无需任何 MPI 依赖。 事实上，只有一个库依赖 MPI 进行这一更改。 我们可以选择只提供这个支持 MPI 的库的源代码，并附带一个 Makefile，以便用户可以在我们的实用程序中使用 MPI 时插入自己的版本。
  load_stubs()
    pthread_once(&init_control, init_routine)
      stubs_handle = dlopen("libdpar_mpi.so", RTLD_NOW)
  stubs.ps_init(argc, argv) 
    MPI_Init(argc, argv) -> MPI_Init() 是MPI程序的第一个调用，它完成MPI程序所有的初始化工作，所有MPI程序的第一条可执行语句都是这条语句。MPI系统通过argc和argv得到命令行参数，并且将MPI系统专用的参数删除，留下用户的解释参数
  MPI_Comm_rank(comm, rank)
  MPI_Comm_size
  MPI_Barrier(comm) -> MPI_Barrier函数 用于一个通信子中所有进程的同步，调用函数时进程将处于等待状态，直到通信子中所有进程 都调用了该函数后才继续执行
  daos_init()
run_specified_tests(tests, rank, size, NULL, 0) -> pus
  run_dfs_par_test
  run_dfs_unit_test
    MPI_Barrier
    cmocka_run_group_tests_name("DAOS_FileSystem_DFS_Unit", dfs_unit_tests, dfs_setup, dfs_teardown) -> cmocka_run_group_tests_name (const char *group_name, const struct CMUnitTest group_tests[], CMFixtureFunction group_setup, CMFixtureFunction group_teardown)
      dfs_setup
        test_setup(state, SETUP_POOL_CONNECT
          test_setup_next_step
            daos_eq_create
        test_setup_pool_create
          dmg_pool_create
            daos_dmg_json_pipe
              cmd_str = cmd_string(cmd_base, args, argcount) ->    dmg -j -i -d --log-file=/tmp/suite_dmg.log pool create --user=root --group=root --scm-size=4294967296b --nvme-size=17179869184b test_OH1F19 --nsvc=1
              run_cmd
                pipe(stdoutfd)
                system(command)
      dfs_unit_tests -> static const struct CMUnitTest dfs_unit_tests[] 
      dfs_teardown
    setenv("DFS_USE_DTX", "1", 1) -> 使用dtx -> DAOS-8952 vos：写入时检查负条目时间戳 (#7821) 如果不重新组织 VOS 代码，因此我们将树加载与存在检查分开，我们会遇到一个问题，即有条件或其他方式的提取可能会提前退出，并且只更新负时间戳 。 因此，后续更新可能看不到这些读取时间戳并错误地执行写入。 而不是承诺立即进行重组
    cmocka_run_group_tests_name("DAOS_FileSystem_DFS_Unit_DTX", dfs_unit_tests, dfs_setup, dfs_teardown)
  run_dfs_sys_unit_test
    dfs_sys_test_read_write





docker rocky8
cat /etc/*-release
Rocky Linux release 8.8 (Green Obsidian)
NAME="Rocky Linux"



[self.c:281] ABT_self_get_type: 4, argobots未开调式 -> ABTI_SETUP_LOCAL_XSTREAM(&p_local_xstream) -> #define ABT_ERR_INV_XSTREAM         4 -> 有函数调用: ABT_self_get_type


crt_reply_send
...
.respond = hg_core_respond_na
  NA_Msg_send_expected


---------------------------------------- DL ----------------------------------------
单元测试, test, cmocka, src/tests, 定义单元测试: cmocka_unit_test, 执行测试: run_all_tests, static const struct CMUnitTest pool_tests
执行测试: cmocka_run_group_tests_name, run_pool_test
*_ut.c, *_tests.c, 

struct CMUnitTest // cmock测试结构体, https://api.cmocka.org/structCMUnitTest.html
const char * 	name
CMUnitTestFunction 	test_func
CMFixtureFunction 	setup_func
CMFixtureFunction 	teardown_func
void * 	initial_state

dfs文件系统测试: dfs_unit_test.c

调度器单元测试: static const struct CMUnitTest sched_uts
事件队列EQ测试: static const struct CMUnitTest eq_uts[]

wal: wal_ut.c -> bio_ut



每服务元数据单元测试: smd_ut -> main
smd_ut_setup
  daos_debug_init
  db_init()
  smd_init(&ut_db.ud_db)
smd_uts
  ut_device
    smd_dev_add_tgt(dev_id1, 0, SMD_DEV_TYPE_DATA)
    dev.sd_state	= SMD_DEV_NORMAL
    smd_db_tx_begin() -> null
    smd_db_upsert(TABLE_DEV, &id, sizeof(id), &dev, sizeof(dev)) -> db_upsert(struct sys_db *db, char *table, d_iov_t *key, d_iov_t *val)
      chain = db_chain_alloc(key->iov_len, val->iov_len) -> db_chain_alloc(int key_size, int val_size) -> calloc
      d_list_add_tail(&chain->uc_link, head)
    smd_db_upsert(TABLE_TGTS[st], &tgt_id, sizeof(tgt_id), &id, sizeof(id))
    smd_dev_set_state(dev_id2, SMD_DEV_FAULTY) -> dev.sd_state = state
    smd_dev_get_by_id(id3, &dev_info) -> smd_dev_get_info
    verify_dev(dev_info, dev_id1, 1)
    smd_dev_get_by_tgt
  -----------------------
  ut_pool
    smd_pool_add_tgt(id1, i, i << 10, st, 100)
    smd_pool_get_info(id1, &pool_info)
    verify_pool(pool_info, id1, 10)
    smd_pool_get_blob(id1, i, st, &blob_id)
    smd_pool_list(&pool_list, &pool_cnt)
  -----------------------
  ut_dev_replace
    smd_dev_replace
      smd_db_delete
      smd_db_upsert
      smd_pool_replace_blobs_locked
smd_ut_teardown

---------------------------------------- DL ----------------------------------------




scons默认环境: DefaultEnvironment
设置参数: CCFLAGS

自定义构建目标, all, server, client, test, scons [args] client install


./eq_tests -> eq_tests.c -> main, 消费者模式, 
cmocka_run_group_tests_name -> cmocka_run_one_test_or_fixture
eq_ut_setup
  daos_debug_init
  daos_hhash_init
  daos_eq_lib_init
  daos_eq_create
eq_uts
eq_test_1


eq_ut_teardown


daos_event_register_comp_cb 注册事件回调


性能测试: daos_perf, https://daosio.atlassian.net/wiki/spaces/DC/pages/2145650559/daos+perf+in+VOS+mode+sm
daos_perf -T vos -P 120G -d 1 -a 200 -r 500 -s 1M -C 0 -z (-t)
D_LOG_MASK=ERR
CRT_PHY_ADDR_STR=sm
CRT_CTX_SHARE_ADDR=0
CRT_CTX_NUM=36
VOS_MEM_CLASS=PMEM



fio --name=global --bs=1M --direct=1 --directory=/tmp/daos_dfuse//907E96B2-18B8-47F2-9D66-5519B049CA04 --group_reporting=1 --iodepth=16 --ioengine=libaio --rw=rw --size=10M --thread=1 --verify=crc64 --name=test --numjobs=1

fio --name=global --bs=1M --direct=1 --directory=/tmp/sxb --group_reporting=1 --iodepth=16 --ioengine=libaio --rw=rw --size=10M --thread=1 --verify=crc64 --name=test --numjobs=1


dbench

Dbench 是一个文件系统基准测试，它生成类似于商业 Netbench 基准测试的负载模式，但不需要 Windows 负载生成器实验室来运行。 它现在被认为是在 Linux VFS 上生成负载的事实上的标准


Avocado(牛油果) 是一组有助于自动化测试的工具和库。人们可以将其称为一种具有优势的测试框架。 本机测试是用 Python 编写的，它们遵循单元测试模式，但任何可执行文件都可以作为测试
https://avocado-framework.readthedocs.io/en/102.0/


Running DAOS Functional Tests: 功能测试
pip3 install --user avocado-framework; avocado run /bin/true
运行 DAOS 功能测试
本文档介绍如何使用 Ansible 设置一个或多个节点来运行 DAOS 鳄梨功能测试。 ansible playbook 还生成两个 bash 脚本，允许轻松构建 DAOS 二进制文件，并能够运行一组 DAOS 鳄梨功能测试。
先决条件
子集群节点
应保留使用受支持的 Linux 发行版的 Wolf[^1] 集群的一个或多个节点。 目前，唯一支持的 Linux 发行版是 Rocky 8.5，可以使用以下命令将其安装在节点 Wolf-666 上：
nodemgr -n Wolf-666 -p daos_ci-rocky8.5 安装
应根据要启动的测试的硬件要求（例如 pmem、vmd 等）选择要使用的节点。 根据所选的节点，最终应调整要启动的测试的 yaml 配置文件。
：警告：强烈建议不要在最终用户用于其他任务（例如日常开发）的节点上部署（即应用 playbook 任务）DAOS 功能测试平台。 事实上，Ansible playbook 最终会更改节点的配置，因此可能会破坏最终用户之前定义的自定义设置：rpm 包版本、大页面设置等。
[^1]：boro集群的节点应该也支持，但尚未测试。
安装 Ansible
通过从包含此 README.md 文件的目录运行以下命令可以轻松安装 Ansible：
python3 -m pip install --user --requirement 需求.txt
SSH授权
ssh 授权密钥的配置方式应允许使用 Ansible playbook 的用户无需密码即可访问子集群所有节点的 root 帐户。 例如，可以使用以下命令向 Wolf-666 节点授予无密码的 root 访问权限。
ssh-copy-id root@wolf-666
功能测试平台的部署
Ansible 库存
ansible清单列出了子集群的不同节点，并根据它们在功能测试平台中的角色将它们聚集在不同的组中（可以自由重叠）。
daos_dev：该组应仅包含一个节点。 最后一个将用于安装 DAOS 二进制文件并通过生成的 bash 脚本启动功能测试。 有关如何使用这些脚本的详细信息将在以下部分中详细介绍。
daos_servers：该组包含将运行 DAOS 服务器的节点。
daos_clients：该组包含将运行最终用户应用程序（例如使用 DAOS 文件系统的 ior 或 fio）的节点。
清单还应包含一组强制性和可选变量。
daos_runtime_dir：定义用于安装 DAOS 二进制文件的共享目录的强制变量。 应可从子集群的所有节点使用相同的路径访问此目录。
daos_source_dir：强制变量，仅由 daos_dev 组的节点使用，定义包含 DAOS 源代码的目录路径。
daos_ofi_interface：可选变量，仅由定义要使用的网络接口的 daos_dev 组的节点使用。 当未定义该变量时，网络接口由 DAOS 任意选择。
daos_hugepages_nb：可选变量（默认值：4096）仅由 daos_servers 组的节点使用，定义 Linux 内核分配的大页数。
daos_avocado_version：可选变量（默认值：“2.4.3”）仅由定义要安装的avocado版本的daos_dev组的节点使用。
daos_avocado_framework_version：可选变量（默认值：“82.1”）仅由定义要安装的 avocado_framework 版本的 daos_dev 组的节点使用。
支持不同的文件格式（例如 YAML、INI 等）和文件树结构来定义 ansible 清单。 以下简单的 ansible 清单在一个 YAML 文件中描述了一个简单的 DAOS 功能平台，该平台由两个承担多个角色的节点组成。




DAOS-10146 构建：ftest 帮助程序脚本 (daos-stack#8754)，引入 Ansile 脚本，允许在 WOLF 的子集群上部署 DAOS 功能平台。 这些 ansible 脚本还生成 bash 脚本，允许安装 DAOS 二进制文件并运行功能测试。 这些用法 ansible 脚本的详细信息在 utils/ansible/ftest/README.md 文件中, https://github.com/ssbandjl/daos/commit/ee951496fb5153c2c5e251c2f4ef9ac7ee193efa
daos自动测试, autotest: https://daosio.atlassian.net/wiki/spaces/DC/pages/2183561867/DAOS-6820+-1.2+Verbs+Automated+testing+status
src/tests/ftest/daos_test/suite.yaml
ansible-playbook -i my-inventory.yml ftest.yml
daos_base_deps

编译: daos-make.sh.j2, daos-make.sh, usage, install(install)), update
daos-launch.sh.j2

for cmd in $@
hash $cmd > "/dev/null" 2>&1

scons重新编译: --implicit-deps-changed, 强制 SCons 忽略缓存的隐式依赖项。 这会导致重新扫描和重新缓存隐式依赖项。 这意味着 --implicit-cache

测试:
test_nvme_io
def write_objects




bio, bdh_write_cmds, 指标metric: d_tm_set_counter(bdh->bdh_du_written, page->data_units_written[0]), 


goroutine, 协程, 
dss_dump_ABT_state -> DAOS-1452 iosrv：转储 ULT 堆栈功能 (#3371), 起初，Argobot API 只允许获取/打印 XStreams/Scheds/Pools/ULTs 信息，但现在这已根据我们的要求提供，让我们也使用它！ 有两种方式，有或没有 xstream 的内部同步/停放。 值得注意的是，目前 ULT 堆栈是原始打印的，直到 Argobots 中添加展开功能之前，都需要手动完成


sched_run(ABT_sched sched)
  ABT_sched_get_data(sched, (void **)&data)
  ABT_sched_get_pools
  sched_pop_net_poll
  sched_pop_nvme_poll
  ABT_xstream_run_unit(unit, pool)

创建ULT: ABT_thread_create(pools[pool_id], hello_world, &thread_args[i], ABT_THREAD_ATTR_NULL, &threads[i])
sched_create_thread -> daos_abt_thread_create -> ABT_thread_create
daos_abt_thread_create_on_xstream -> ABT_thread_create_on_xstream -> 创建与执行流关联的新 ULT。 ABT_thread_create_on_xstream() 创建一个新的 ULT，由属性 attr 给定，将其与执行流 xstream 的主调度程序的第一个池关联，并通过 newthread 返回其句柄。 此例程将创建的 ULT 推送到池 pool。 创建的 ULT 在调度时使用 arg 调用 thread_func()。 attr 可以通过 ABT_thread_attr_create() 创建。 如果用户为 attr 传递 ABT_THREAD_ATTR_NULL，则使用默认的 ULT 属性


hello_world_ws.c -> 此提交添加了新的模板示例，同时删除了一些斐波那契示例，因为它们是重叠的。 斐波那契代码也被细化

ABT_sched_create_basic -> 创建调度器
stencil 模板, 



flush_ult
gc_ult
gc_rate_ctl
yield_fn <- wait_cb (vp_wait_cb) 等待正在进行的事务提交，或者让步以取得进展 | sc_yield_fn <- sc_sleep
dss_ult_yield
sched_req_sleep -> sleep场景使用
  sched_req_yield
    ABT_thread_yield



daos_eq_destroy    
  crt_context_flush

daos_eq_query -> 查询EQ中有多少个未完成的事件，如果events不为NULL，则这些事件将被存储到其中。 查询返回的事件仍然属于 DAOS，不允许完成或释放该函数返回的事件，但允许调用 daos_event_abort() 来中止已启动的操作。 此外，返回的事件的状态仍然可以改变，例如，返回的“已启动”事件可以在访问之前变为“已完成”。 用户有责任保证轮询过程释放返回的事件


daos_event_parent_barrier -> 将父事件标记为已启动的屏障，这意味着在所有其他子事件完成并且父事件从 EQ 中轮询或测试完成（如果它不在 EQ 中）之前，无法添加更多子事件。 在所有子级完成之前，不会从 EQ 中轮询父级，也不会使用 daos_event_test 返回完成。 请注意，如果父事件作为另一个 daos 操作的一部分启动，则不应再调用此函数，并且启动事件的函数将成为屏障操作。 在这种情况下，操作本身可以在子级完成之前完成，但在所有子级完成之前该事件不会被标记为就绪, 将此事件标记为障碍事件，由最后一个完成的子事件后触发完成


daos_event_abort -> 尝试中止与此事件相关的操作。 在此调用之后，用户仍然需要等待或轮询该事件。 目前，这不会中止任何内部 DAOS 操作，并且实际上是无操作, commit: daos_event_abort() 将事件标记为已中止，但 DAOS 内部任务并未中止，目前无法中止该操作。 因此，稍后如果用户继续（通过测试或轮询）中止事件，事件 API 会释放该事件并将其标记为准备初始化，因为它之前已中止，但 DAOS 内部任务仍未完成。 当 DAOS 任务最终进行时，可能会导致损坏，因为用户可能重新使用了该事件，甚至重新使用了传递给该事件的缓冲区。 由于我们不支持取消内部 DAOS 任务和 RPC，因此暂时将中止实现更改为无操作，这将导致对事件的测试或轮询等待内部任务完成，以避免此类损坏问题


---------------------------------------- DL ----------------------------------------
创建容器流程, cont_create, daos container create sxb --type POSIX sxb -> src/control/cmd/daos/main.go -> type cliOptions struct -> func main()
parseOpts -> func (cmd *containerCreateCmd) Execute(_ []string) (err error)
  disconnectPool, err := cmd.connectPool(C.DAOS_PC_RW, ap)
  contID, err = cmd.contCreate() -> func (cmd *containerCreateCmd) contCreate() -> 容器布局类型, container layout type, DAOS_PROP_CO_LAYOUT_POSIX
    C.daos_cont_create(cmd.cPoolHandle, &contUUID, props, nil) -> #define daos_cont_create daos_cont_create2
  cmd.openContainer(C.DAOS_COO_RO)
  ci, err = cmd.queryContainer()
...

container不开辟空间，仅设置数据存储策略

enum daos_obj_redun 对象冗余

daos container create --pool sxb --type POSIX --label sxb
daos cont create --label Cont1 --type POSIX --oclass RP_3G1 --properties rf:2 Pool1 
daos cont create --oclass=RP_3GX --properties=rf:1 --type POSIX --pool sxb --label sxb

创建容器
func (cmd *containerCreateCmd) Execute
  ap, deallocCmdArgs, err := allocCmdArgs(cmd.log)
    ap = &C.struct_cmd_args_s{} struct cmd_args_s 解析 pool、cont、obj 命令的命令行参数的综合结果，其中大部分是常见的
  defer deallocCmdArgs()
  disconnectPool, err := cmd.connectPool(C.DAOS_PC_RW, ap)
    cmd.poolBaseCmd.connectPool(flags)
      C.daos_pool_connect2
        dc_task_create(dc_pool_connect, NULL, ev, &task)
  ap.c_op = C.CONT_CREATE  
  cmd.contUUID, err = uuidFromC(ap.c_uuid)
  C.cont_create_hdlr(ap) -> cont_create_hdlr(struct cmd_args_s *ap) src/utils/daos_hdlr.c
    daos_cont_create
      daos_cont_create2
  cmd.openContainer
  ci, err = cmd.queryContainer()


dfs_cont_create2
  dfs_cont_create_int
    daos_cont_prop2redunfac
    daos_obj_generate_oid_by_rf 超级块
    daos_obj_generate_oid_by_rf root
    daos_prop_entry_set_ptr
    daos_cont_create(poh, cuuid, prop, NULL) 未设置uuid
    daos_cont_open
      ...
      cont_open(struct rdb_tx *tx
      ...
    open_sb
    insert_entry
    daos_obj_close
    dfs_mount


dc_pool_connect
  init_pool
    dc_pool_alloc(DC_POOL_DEFAULT_COMPONENTS_NR) 128
      daos_hhash_hlink_init(&pool->dp_hlink, &pool_h_ops) 初始化hash表
      D_INIT_LIST_HEAD(&pool->dp_co_list) 初始化容器链表
      pool->dp_map_sz = pool_buf_size(nr) offsetof
    dc_mgmt_sys_attach(grp, &pool->dp_sys) //TODO
    rsvc_client_init(&pool->dp_client, NULL)
    client->sc_ranks = d_rank_list_alloc(0)
    rsvc_client_reset_leader(client)
  daos_task_set_priv(task, pool)
  pool = dc_task_get_priv(task)
  dc_pool_connect_internal
    dc_pool_choose_svc_rank
    pool_req_create(daos_task2ctx(task), &ep, POOL_CONNECT, &rpc)
    dc_sec_request_creds(&pci->pci_cred)
    map_bulk_create
    tse_task_register_comp_cb(task, pool_connect_cp, &con_args
    daos_rpc_send(rpc, task)


pool_connect_cp
  pool_rsvc_client_complete_rpc
  process_query_reply
  dc_mgmt_notify_pool_connect
  dc_pool_hdl_link(pool)
  dc_pool2hdl(pool, arg->hdlp)


真正最新最好的容器创建实现, 使用者需要包含 daos_cont.h 头文件
daos_cont_create2 -> daos_cont_create
  dc_task_create(dc_cont_create, NULL, ev, &task) -> 创建容器任务
    dc_cont_create(tse_task_t *task)
      entry = daos_prop_entry_get(args->prop, DAOS_PROP_CO_STATUS)
      pool = dc_hdl2pool(args->poh)
      rc = dup_cont_create_props(args->poh, &rpc_prop, args->prop)
      dc_pool_choose_svc_rank -> 按标签或 UUID 选择池服务副本排名(pool service replica rank)。 如果 rsvc 模块指示 DER_NOTREPLICA，（仅限客户端）尝试通过查询 MS 来刷新列表
        rsvc_client_choose
        dc_mgmt_pool_find
      cont_req_create(daos_task2ctx(task), &ep, CONT_CREATE, &rpc) -> 创建容器操作 -> ds_cont_op_handler
      tse_task_register_comp_cb(task, cont_create_complete 完成回调
        cont_rsvc_client_complete_rpc
          tse_task_reinit(task) 重新初始化任务并将其移至调度程序的初始化列表中。 该任务必须具有要重新插入调度程序的主体函数。 如果任务在其完成 CB 之一中重新初始化，则该回调和已执行的回调将从 cb 列表中删除，并且需要在重新插入后由用户重新注册 -> tse_task_reinit_with_delay(task, 0 /* delay */)
            dtp_generation_inc(dtp) -> 任务的生成，每次任务重新初始化或添加依赖任务时+1
            d_list_move_tail(&dtp->dtp_list, &dsp->dsp_init_list) -> 重新加入初始队列



服务端创建容器
ds_cont_op_handler(crt_rpc_t *rpc, int cont_proto_ver)
  pool_hdl = ds_pool_hdl_lookup(in->ci_pool_hdl)
  cont_svc_lookup_leader
    ds_pool_cont_svc_lookup_leader
  cont_op_with_svc(pool_hdl, svc, rpc)
    rdb_tx_begin
    case CONT_CREATE
      cont_create -> cont_create(struct rdb_tx *tx, struct ds_pool_hdl *pool_hdl, struct cont_svc *svc, crt_rpc_t *rpc)
        ds_sec_pool_can_create_cont
        daos_prop_dup
        cont_create_prop_prepare
        daos_prop_entry_get
        cont_existence_check
        rdb_tx_create_kvs -> rdb_tx_append RDB_TX_CREATE
          rdb_tx_leader_check
          rdb_tx_op_encode
        rdb_path_clone
        rdb_path_push
          rdb_encode_iov
        rdb_tx_update -> rdb_tx_append
        cont_prop_write
    get_metadata_times
    rdb_tx_commit(&tx) -> 提交事务, 如果成功，则 tx 中的所有更新都会向查询显示。 如果发生错误，则 tx 被中止, struct rdb_tx rdb事务（TX）（不透明），所有字段都是私有的。 这些向调用者公开，以便他们可以分配 rdb_tx 对象，可能在其堆栈上
      rdb_raft_append_apply(tx->dt_db, tx->dt_entry, tx->dt_entry_len, &result) -> RAFT_LOGTYPE_NORMAL -> rdb_raft_append_apply_internal(db, &mentry, result) -> 追加并等待\a 条目被应用。 调用者必须持有 d_raft_mutex
        rdb_raft_register_result -> d_hash_rec_insert
        rdb_raft_save_state
        raft_recv_entry
        rdb_raft_check_state
        rdb_raft_wait_applied(db, mresponse.idx, mresponse.term) -> 等待索引在期限内应用。 仅供领导者使用。 调用者最初持有 d_raft_mutex
          ABT_cond_wait(db->d_applied_cv, db->d_raft_mutex)
        raft_apply_all(db->d_raft) -> 应用所有条目直到提交索引
          raft_snapshot_is_in_progress(me_)
          raft_apply_entry(me_) -> 在lastApplied + 1处应用条目。条目变为“已提交”
            me->cb.applylog(me_, me->udata, ety, me->last_applied_idx)
  ds_rsvc_set_hint
  cont_svc_put_leader
  ds_pool_hdl_put(pool_hdl)

---------------------------------------- DL ----------------------------------------


---------------------------------------- DL ----------------------------------------
daos_agent:
dlv attach `ps aux|grep 'daos_agent'|grep -v grep|awk '{print$2}'`



- 持久内存和易失性内存的内存抽象
- 通用 btree（更像是 b+tree 的变体）实现。
。 树创建/销毁/打开/关闭
。 记录插入（树分裂/增长）
。 记录查找
。 迭代器
。 仍然不支持删除



bio_ut.c -> main -> bio单元测试
run_wal_tests
wal_ut_setup
wal_uts
  wal_ut_single
  wal_ut_many_acts
  wal_ut_checkpoint
  ...
wal_ut_teardown



vea_ut.c -> main
vea_uts

vea_tx_publish -> 使预订持久化。 它应该是调用者操纵的事务的一部分


daos_task_reset -> 使用另一个操作码重置 DAOS 任务。 该任务必须已经完成或尚未处于运行状态，并且尚未被释放（使用时必须对该任务进行引用计数，以防止在 DAOS 操作完成后它被释放）


参考配置: utils/config/examples/daos_server_verbs.yml


指标: engine_io_latency_tgt_update_{min|mean|max} and engine_io_latency_update_{min|mean|max}
更新副本对象。 客户端需要先将更新RPC发送给一个引擎（称为领导者），然后该引擎将更新RPC（tgt_update）转发给其他引擎，一旦所有引擎完成更新，领导者就会回复客户端。 所以更新 RPC 是从客户端到领导者的 RPC。 （engine_io_latency_update用于描述整个更新的延迟）。 tgt_update RPC是从leader到其他引擎的RPC，（engine_io_latency_tgt_update用于描述一个引擎的更新延迟，即其他引擎处理来自leader的tgt_update RPC



bio_media_error | bio_bs_monitor <- bio_nvme_poll
auto_faulty_detect(struct bio_blobstore *bbs)
  is_bbs_faulty(bbs)



centos7:
scl enable devtoolset-9 bash

import pdb; pdb.set_trace()



parse_device_info


func (sb *spdkBackend) Scan
  groomDiscoveredBdevs(needDevs, foundDevs, req.VMDEnabled)

dlv exec ./daos_server nvme scan -> func (cmd *scanNVMeCmd) Execute -> func (cmd *scanNVMeCmd) scanNVMe
  scanBackend -> func (scs *StorageControlService) NvmeScan -> 
  f.SendReq("BdevScan" ->  pbinPath, err := common.FindBinary(f.pbinName) -> daos_server_helper -> child := exec.CommandContext(ctx, binPath) -> /opt/daos/bin/daos_server_helper
src/control/cmd/daos_server_helper/main.go
h.bdevProvider.Scan(sReq)

